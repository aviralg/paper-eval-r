\documentclass[review,screen,acmsmall,anonymous=true]{acmart}
\settopmatter{printfolios=false,printccs=false,printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}


\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}   %% For author/year citations
\usepackage{listings,hyperref,multirow,paralist,xspace,url,wrapfig}
\usepackage{tabularx}
\input{macro}
\newcommand{\mypara}[1]{\medskip\noindent\emph{#1}\xspace}
\newcommand{\NOTE}[1]{{\it Note: #1}\xspace}
\newcommand{\authorcomment}[3]{\xspace\textcolor{#1}{{\bf #2} #3}\xspace}
\newcommand{\todo}[1]{\authorcomment{red}{TODO}{#1}}

% cf. https://tex.stackexchange.com/a/144640
\makeatletter\let\expandableinput\@@input\makeatother

\begin{document}
\title{What We Eval in the Shadows}
\subtitle{A large-scale study of {\sf eval} usage in R programs}

\author{Author }
\authornote{Both authors contributed equally to this research.}
\email{a@b.com}\orcid{}\author{Author}\authornotemark[1]
\email{r@s.com}
\affiliation{\institution{Institute} \country{Country}}

\begin{abstract}
  \noindent Most dynamic languages allow users to turn text into code using
  various functions, often named \eval, with language-dependent semantics. The
  widespread use of these reflective functions hinders static analysis and
  prevents compilers from performing optimizations. This paper aims to provide a
  better sense of why programmers use \eval. Understanding why \eval is used in
  practice is key to finding ways to mitigate its negative impact. We have
  reasons to believe that reflective feature usage is language and application
  domain specific; we focus on data science code written in R, and compare our
  results to previous work that analyzed web programming in JavaScript. This
  paper studied \CranRunnableScripts scripts extracted from \CranPackages R
  packages, for a total of \packageAllcalls calls to \eval. We find that \eval
  is indeed in widespread use; R's \eval is more pervasive and arguably
  dangerous than what was previously reported for JavaScript.
\end{abstract}

\maketitle

\section{Introduction}

Most dynamic languages provide their users with a facility to
transform unstructured text into executable code and evaluate that
code. We refer to this reflective facility as \eval bowing to its
origins in LISP, all the way back in 1956~\cite{lisp}. \Eval has been
much maligned over the years. In computing lore, it is as close to a
boogeyman as it gets. Yet, for McCarthy, \eval was simply the way to
write down the definition of LISP, he was surprised that someone coded
it up and offered it to end users. Since then, reflective facilities
have been used to parameterize programs over code patterns that can be
provided after the program is written. The presence of such a feature
in a language is a hallmark of dynamism; it is a form of delayed
binding as the behavior of any particular call to \eval will only be
known when the program is run and that particular call site is
evaluated.

\vspace{2mm}\noindent\emph{Trouble in Paradise.} Reflective facilities
hinder most attempts to reason about, or apply meaning-preserving
transformation to, the code using them. In practice, \eval causes
static analysis techniques to loose so much precision as to become
pointless. For compilers, anything but the most trivial, local,
optimizations are unsound after a use of \eval. Furthermore, the
addition of arbitrary code --- code that could have been obtained from
a network connection --- as a program is running is a security
vulnerability waiting to happen. To illustrate these challenges,
consider the interaction of a static analysis tool with a dynamic
language. An abstract interpretation-based program analyzer computes
an over-approximation of the set of possible behaviors exhibited by
the program under study~\cite{cc77}. A reflective may have \emph{any}
behavior that can be expressed in the target language; i.e. \eval can
be replaced by any legal sequence of instructions. As dynamic
languages tend to be permissive, the analysis has to, for example,
assume that many (or all) functions in scope may have been redefined,
e.g. that \texttt{`+`} now opens a network connection or something
equally surprising. A single occurrence of \eval causes the static
analyzer to loose all information about program state and
meaning of identifiers. This loss of precision can sometimes be
mitigated by analyzing the string argument~\cite{moller03} to bound its
possible behavior but when the string comes from outside the program
not much can be done. A frustrated group researchers argued giving up
on soundness and, instead, under-approximating dynamic features
(soundiness)~\cite{soundy}. In their words ``a practical analysis,
therefore, may pretend that \eval does nothing, unless it can
precisely resolve its string argument at compile time.'' Alas,
assuming that \eval does not have side-effects, or that side-effects
will not affect the results of the analysis, may be unduly optimistic.

\vspace{2mm}\noindent\emph{Is Past Prologue?} Previous work investigated how
\eval is used in web programming, specifically in websites that use
JavaScript~\cite{pldi10a}. In 2010, 17 of the largest websites used the feature.
In 2011, 82\% of the 10,000 most accessed sites used \eval~\cite{ecoop11}. Yet,
the strings passed to \eval, and their behaviors when executed, are far from
random; it was shown that when one can observe several calls to \eval, the
``shape'' of future calls can be predicted with 97\% accuracy~\cite{oopsla12b}.
Overall, practical usage suggested that most reflective calls were relatively
harmless. While this backs up the soundiness squad's approach, does it
generalize to other application domains than web programming and to other
languages?

\vspace{2mm}\noindent\emph{The Here and Now.} In this study, we investigate the
usage of \eval in programs written in the R programming language. R is language
designed by statisticians for applications in data science~\cite{r,R96}. What
makes looking at R after JavaScript interesting is that, while both languages
are dynamic, they are quite different. While one can program in an
object-oriented style in R, like in JavaScript, R is primarily a lazy, untyped,
functional language. JavaScript was designed to run untrusted code in browser,
while R is used for statistical computing on desktops. JavaScript is a general
purpose language used by a wide community of programmers; while R is used for
scientific computing by data scientists and domain experts with, often, limited
programming experience. One can distinguish between library implementers,
developers with some programming experience and a working knowledge of R, and
end-users, who are typically not expert programmers and often have only a
cursory knowledge of the language.\footnote{Consider that R is lazy like
Haskell. We informally surveyed end-users and did not find a single user aware
of this fact. Library developers, on the other hand, know and program
defensively around laziness.} Our goal is thus to highlight the differences in
usage between JavaScript and R, and try to explain those differences in terms of
language features, application domain and programmer experience. Hopefully some
of our observations will generalize to other languages.

\vspace{2mm}\noindent\emph{The What and How.} One significant benefit of
choosing R is that every package in the CRAN repository is curated and comes
with examples of typical usage. This gives us a large code base that we can
analyze dynamically. To observe \eval we built a two-level monitoring
infrastructure:\footnote{Our infrastructure is open source and publicly
available and  will be submitted to the artifact evaluation committee. } we can
monitor R programs by instrumentation --- this gives us access to many
user-visible properties of R programs --- but we can also monitor the
inner-workings of the R interpreter --- this allows us to capture details not
exposed at the source level. Dynamic analysis is limited, it can only observe
behaviors triggered by the particular inputs passed to a program. Luckily, CRAN
libraries come with many tests and use-cases. The choice of corpus is crucial.
Our corpus has been constructed to reflect the levels of sophistication of the R
community. We distinguish between \emph{CRAN packages} (\CranPackages curated
packages that pass stringent quality checks and are equipped with tests and
sample data) and \emph{Kaggle scripts} (\KaggleUnique end-user written programs
that performs a particular data analysis task). It is reasonable to expect that
\eval usage differ between these datasets: the libraries represent a lively
ecosystem with new libraries added each day, while end user code is often thrown
together, run once, and never revisited.


\vspace{2mm}\noindent\emph{Why do we Eval?} {\bf A short summary of the
results should go here.}


\newpage
\section{Background and Previous work}

This section provides a short introduction to R and the reflective features of
the language; then looks at the semantics of \eval in R and discusses design
choices; lastly, this work is put in context.

\subsection{R, Briefly}

\citet{ecoop12} gave a programming language-centric overview of the R language.
They characterized it as a lazy, vectorized, functional language with a rich
complement of dynamic features expressive enough to layer several object systems
on top of the core language. Most data types are sequences of primitive values
constructed by calling the combination function \c{c}, for instance
\c{c("Ha","bye")} evaluates to a vector of two strings, constants such as \c{42}
are vectors of length one. To enable equational reasoning, values accessible
through multiple aliases are copied when written to. Furthermore, values can be
tagged by attributes, these are key-value pairs. For instance, the attribute
\c{dim}-\c{c(2,2)} can be attached to the value at \c{x} by the call
\c{attr(x,"dim")$\leftarrow$c(2,2)}. In this case, the addition of this
attribute turns \c x into a matrix. The \c{class} attribute gives a 'class', in
the object-oriented sense, to a value. So, \c{class(x)<-"human"} sets the class
of \c{x} to \c{human}; classes are used for method dispatch. Every linguistic
construct is desugared to a function call, even control flow statements,
assignments, and bracketing. All functions can be shadowed or redefined, making
the language at the same time remarkably flexible and exceedingly challenging to
compile statically as vividly detailed by~\citet{dls19}. R uses a relaxed
call-by-need convention for passing arguments to functions. Each argument is a
thunk composed of an expression, its environment, and a slot for the result,
these are called \emph{promises}. To get the value of an argument, the
corresponding promise must be forced, once forced the promise's result is cached
for future use.


\subsection{On the Expressive Power of Eval}

While a data-to-code facility is available in many languages, some design
choices affect the expressive power of \eval. The key choices are the input
format, the environment in which generated code is evaluated and the reflective
operations available to that code. Fig.~\ref{comp} summarizes designs.

\begin{figure}[!h]\center\small\begin{tabular}{r@{~}l|l|l|l}\hline
\tiny\sc Language&&\sc\tiny Input&\sc\tiny Scope&\tiny\sc Reflective operations\\\hline
\bf Julia&\cite{julia}     & expression& toplevel         & data\\
\bf Java&\cite{cl}  & bytecode  & classloader       & data\\
\bf JavaScript&\cite{ecoop11}& text      & current, toplevel& data\\
\bf R&\cite{R96}  & expression& programmatic      & data, stack, environment\\\hline
\end{tabular}\caption{Design space of \eval}\label{comp}
\end{figure}

The input to \eval can be in any format convertible to code. JavaScript allows
arbitrary strings to be used. Both Julia and R are more restrictive as they
require expressions (or abstract syntax trees). Finally, Java is the most
restrictive as its classloader only accepts complete classes in bytecode form.

The choice of the environment of \eval is important as it determines how much of
a program \eval can observe as well as the reach of potential side-effects
performed by that operation. The most restrictive semantics is that of Java
where newly loaded code evaluates in the environment defined by the classes
visible from the current classloader. Julia and JavaScript's strict mode, limit
\eval to the global environment. Finally, R is the most flexible as any
accessible environment can be selected and passed to \eval.

The last degree of freedom is the expressive power of the code executed by
\eval. The main difference between languages lies in how much of the state of a
program is accessible through reflective operations. Julia, Java and JavaScript
all allow some form of introspection on the data. R is more flexible as it is
possible to inspect the program's call stack. Thus any environment in the
program can be inspected and modified.

Given the above, the claim that R is amongst the languages with the most
powerful \eval seems plausible. The rationale for R's design seems to have been
to expose as much of the language and its internals as possible in order to
maximize expressivity. In R, \eval is a key tool to extend the language and
implement DSLs, it is also a replacement for macros. By contrast, the designers
of Julia chose to limit \eval. In Julia, only global variables can be
side-effected and environments cannot be readily manipulated. This is designed
to shield optimized code from some of the most pernicious uses of the
facility~\cite{oopsla18a}. Furthermore, Julia provides a versioning
mechanism, called world age, to ensure that any methods defined within an \eval
only become visible at well-defined program points and thus that recompilation
does not have to occur when optimized code is running~\cite{oopsla20a}.

\subsection{Eval in R}\label{sec:eval-in-r}

R exposes a rich reflective interface with functions, \c{eval},
\c{evalq}, \c{eval.parent}, and \c{local}. The following simplifies details not
relevant to this paper, the interested reader should consult \citet{hadley}.

\begin{lstlisting}
 eval <- function(e, envir = parent.frame(),
                   encl = if(is.list(envir)) parent.frame() else baseenv()) ...
\end{lstlisting}
The most general is \eval. Parameter \c e is the \c{expression} to evaluate,
\c{envir} is the evaluation environment, and \c{encl} is used to look up
variables not found in \c{envir}. The default value of the latter is either the
caller's environment or the base environment \Eval can take several types for \c e, for
our purposes on the \c{expression}. These can be thought of as the abstract
syntax trees returned by the parser. An \c{expression} can be obtained by
calling \c{quote} or \c{parse} with a string:
\begin{lstlisting}
 > quote(a + b)
 # a + b
 > parse(text="a+b")
 # expression(a+b)
\end{lstlisting}
Usually, expressions are obtained from promises using \c{substitute}. Consider a
function called with \c{x = a+b}, then \c f returns an expression object.
\begin{lstlisting}
 > f <- function(x) substitute(x)
 > f(a+b)
 # expression(a+b)
 > substitute(a+b,list(a=1,b=2))
 # 1+2
\end{lstlisting}
The call to \c{substitute(x)} extracts the unevaluated expression from the
promise \c x. Function \c{substitute(e,envir)} takes two arguments, the second
is used to substitute free variables in \c e.
\begin{lstlisting}
 > environment()
 # <environment: R_GlobalEnv>
\end{lstlisting}
The \c{environment} function returns the current environment. Environments nest,
each has a parent. When creating a new environment with \c{new.env}, the parent
is the current environment. Environment chains can be traversed with
\c{parent.env}, until \c{emptyenv} is reached. The top-level environment is
\c{.GlobalEnv}, it has parents that represent the packages that have been
loaded. One can also directly read, modify or create new bindings, given any
environment:
\begin{itemize}
\item \c{envir\$v} and \c{get("v",envir=envir)}: read  \c{v} from \c{envir};
\item \c{envir\$v<-2} and \c{assign("v",2,envir=envir)}: store 2 in \c{v}, if
  not found, \c{v} is created in \c{envir}.
\end{itemize}
\noindent Environments are used as hash maps as they have reference semantics
and a built-in string look up. Functions \c{parent.frame} and \c{sys.frame}
return environments further up the call stack.
 \begin{lstlisting}
  evalq <- function(e,envir,encl) eval(quote(e),envir,encl)
  eval.parent <- function(e,n) eval(e,parent.frame(n))
  local <- function(e,envir,encl) evalq(e,new.env())
\end{lstlisting}
The three variants can be expressed with \eval: \c{evalq} quotes its
argument to prevent evaluation in the current environment,
\c{eval.parent(e,n)} evaluates \c{e} in the environment of the {\tt n}-th
caller of this function; finally, \c{local} evaluates \c{e} in a new environment
to avoid polluting the current one.

\subsection{Previous Work}

\citet{ecoop11} provided the first large-scale study of the runtime behavior of
\eval in JavaScript. They dynamically analyzed a corpus of the 10,000 most
popular websites with an instrumented web browser to gather execution traces.
They show that \eval is pervasive with 82\% of the most popular websites using
it. The reasons for its use include the desire to load code on demand,
deserialization of JSON dataq and lightweight meta-programming to customize web
pages. While many uses were legitimate, just as many were unnecessary and could
be replaced with equivalent and safer code. They categorized inputs to \eval so
as to cover the vast majority of input strings. Restricting themselves to \eval
in which all named variables refer to the global scope, many patterns could be
replaced by more disciplined code~\cite{oopsla12b, moller12}. The work did not
measure code coverage, so the numbers presented are a lower bound on possible
behaviors. Furthermore, JavaScript usage in 2011 is likely different from today,
e.g. Node.js was not covered. More details about dynamic analysis of JavaScript
can be found in~\cite{liang}.

\citet{wang} analyzed use of dynamic features in 18 Python programs to find if
they affect file change-proneness. Files with dynamic features are significantly
more likely to be the subject of changes, than other files. Chen et al. looked
at the correlation between code changes and dynamic features, including \eval,
in 17 Python programs~\cite{chen}. They did not observe many uses of \eval.
\citet{oscar} performed an empirical study of the usage of dynamic features in
1,000 Smalltalk projects. While \eval itself is not present, Smalltalk has a
rich reflective interface. The authors found that reflective methods are used in less
than 2\% of methods. The most common reflective method is \c{perform:}; it send
a message that is specified by a string. These features are mostly used in the
core libraries.

\citet{bodden} looked at usage of reflection in the Java DaCapo benchmark suite.
They found that dynamic loading was triggered by the benchmark harness. The
harness then executes methods via reflection, this caused static analysis tools
to generate an incorrect call graph for the programs in DaCapo.

\citet{Arceri21} study \eval in JavaScript from a software security point of
view. The authors report that 53\% of the malware they studied used \eval as a
means to obfuscate attack code or to mount attacks. They propose an abstract
interpretation-based approach to analyzing dynamic languages. One must construct
a static approximation of the argument to \eval and then analyze possible
behaviors of the interpreter when evaluating the generated code.

\citet{ecoop12} had a short section on the usage of \eval in R. They found the
it widely used in R code with 8500 call sites in CRAN and 2.2 million dynamic
calls. The 15 most frequent call sites account for 88\% of those. The
\c{match.arg} function is the highest used one with 54\% of all calls. In the
other call sites, they saw two uses cases. The most common is the evaluation of
the source code of a promise retrieved by \c{substitute} in a new environment;
e.g. as done in the \c{with} function. The other use case is the invocation of a
function whose name or arguments are determined dynamically. For this purpose, R
provides \c{do.call} and thus \eval is overkill.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

This section explains how we selected our corpus and how we obtained the
reported results.

\subsection{Corpus}

Our corpus is assembled from three sources: the \emph{Base} libraries that are
bundled with the language implementation, packages hosted on \emph{CRAN} and
scripts from \emph{Kaggle}.

\mypara{Base.} The dataset contains \BasePackages libraries performing basic
arithmetics, statistics, and operating system functionalities. These libraries
are bundled with R and executed pervasively. Together they contain
\BaseEvalCallSites call sites to \eval in \BaseFunsWithEvals functions. Some of these
functions provide basic functionalities such as package loading and thus there
is hardly any code which does not invoke one of these.

\mypara{CRAN.} The Comprehensive R Archive Network
({\small \url{cran.r-project.org}}) is the largest curated repository of R
packages. It hosts \CranAvailablePackagesRnd packages with 6 new ones submitted
daily~\cite{Ligges2017}. Packages are authored by experienced developers and
abide by well-formedness rules automatically checked on each commit.
Each package comes with sample data. There are three sources of
runnable code in a package: \emph{tests, examples} and \emph{vignettes} --
respectively, unit tests, code snippets from the documentation, and long-form
use-cases written in RMarkdown. Examples and vignettes can be turned into
scripts by extracting the relevant code. From \CranPackages packages we
extracted \CranRunnableScripts scripts that contain \CranRunnableCode lines of
code (\CranRunnableCodeExamplesRnd in examples, \CranRunnableCodeVignettesRnd in
vignettes and \CranRunnableCodeTestsRnd in tests).

\mypara{Kaggle.} The Kaggle website ({\small \url{kaggle.com}}) is an online
platform for data science. It allows users to submit problems and compete to
solve them. Solutions, called \emph{kernels}, are uploaded as plain scripts or
notebooks. The quality of code on Kaggle is not uniform. Each kernel is
runnable, and thus there is one script per kernel. Input data is provided. We
obtained \KaggleKernels kernels. Since Kaggle is not curated, we used SHA-1 hashes
to identify and remove \KaggleDuplicates duplicate entries. The remaining
\KaggleUnique kernels are unique solutions to \KaggleCompetitions competitions.
They contain \KaggleCode lines of R code. Only \KaggleWithEvals kernels call
\eval.

\subsection{Pipeline}

The analysis presented in this has been automated by a pipeline that acquires
packages, extract scripts, executes them, traces their behavior and summarizes
observations. Figure~\ref{fig:pipeline} shows the main steps along with their
running time, data size, and number of elements manipulated. Timings are from a
cluster of three 2.3GHz Intel Xeon 6140 servers, with 72 cores and 256GB of RAM,
and shared OCFS network storage. The pipeline steps are:


\begin{figure}[!h]\hspace{-5mm}
  \includegraphics[width=.95\linewidth]{pipeline.pdf}
  \caption{Pipeline}\label{fig:pipeline}
\end{figure}

\medskip
\begin{compactenum}
\item \emph{Download.} Packages are downloaded from CRAN. For scripts, a web
  crawler retrieves code and the Kaggle command line tool gets data.
  Installation is complicated by native dependencies which are not properly
  documented and thus hard to automatically resolve.
\item \emph{Extract.} Given installed packages and scripts, the next step is to
  create runnable programs. The \genthat tool helpfully extracts all runnable
  code snippets from a package and turns each of these into a self-standing
  program~\cite{issta18}. Some Kaggle kernels are already scripts, for those
  nothing more needs be done. Others kernels are packages as notebooks, either
  as Rmarkdown or Jupyter, for those we use \c{knitr} to extract runnable code.
  The body of each extracted program is instrumented with calls to our dynamic
  analyzer to ensure that we only record calls to \eval from the code of
  interest and not from bootstrapping or execution harness operations.
\item \emph{Trace.} Each program is executing using our dynamic analysis tool
  which is a heavily instrumented interpreter that captures calls to \eval and
  many other fine-grained runtime events. Packages are run twice, once to
  capture \eval calls originating from package code, and a second time to
  capture calls coming from the base libraries. To avoid any interference, each
  program is run in its own process.
  As the GNU R bytecode compiler is written in R, we turn it off
  to avoid recording \eval in its code, and fallback on the interpreter.
\item \emph{Analyze.} Finally, analysis output is merged, cleaned and summarized
  in a post-processing phase driven by series of R scripts. The summarized data
  is then analyzed in RMarkdown notebooks to gather insights. All figures and
  numbers appearing in the paper are generated automatically. Figures are
  produced in PDF by \c{ggplot2}, numbers are exported as \LaTeX macros.
\end{compactenum}

\medskip\noindent The code extraction and tracing steps of the pipeline are run
in parallel~\cite{GNUparallel} orchestrated by a Makefile. Servers have
identical environments thanks to docker images with all dependencies installed.

\subsection{Dynamic Analysis}

The dynamic analysis is performed by \rdyntrace, a modified R virtual machine
based on GNU R 4.0.2 that exposes low-level callbacks for a variety of runtime
events~\cite{oopsla19b}.

The tracer registers callbacks to all \eval functions as well as a few other
ones that allow us to better identify where the expressions come from. For
example, we taint the results of calls to \c{parse} (turns string into
expression) and \c{match.call} (reflects on current call). We also capture calls
to R API for dynamic code loading (\eg\xspace \c{library}, \c{require} and
\c{source}). Next, we subscribe to the events related to a variable definition
and assignment allowing us to record side effects that happen in environments
while evaluating code in \eval. One challenge we encountered during the analysis
was that essentially R only provides source references for top-level calls in a
function body or in the block surrounded by braces. That is to say, a method
whose body would \c{eval(x)} would not have debug information that we can use to
identify that particular \eval. The same expression surrounded by braces would
have a line number and source reference. This is unfortunately not easily fixed
in the R implementation. Instead, we extended the dynamic analysis tool to
attach synthetic source code references to all \eval call sites by traversing
ASTs. The tracer is implemented as an R package in 3.2K C++ and 1.3K of R code.
For performance reason, most of the tracing is done in C++. While in theory the
implementation is rather straightforward, it is not so in practice:
%
\begin{compactitem}[---]

\item The lazy evaluation makes it difficult to analyze function arguments
  while tracing as prematurely forcing a promise might have dangerous
  consequences.

\item While the R interpreter is implemented in C, a lot of the core
  functionality is done in R. For example, package loading is implemented in R
  using \eval or the S4 and R6 object systems. This makes it hard in the tracer
  to separate the \eval that are essential to user programs from the
  accidental ones that are products of the way R implements its basic
  operations. This is even more so for the side-effects analysis.

\item In the dynamic analysis, we run all the \emph{runnable} code we obtained
  from CRAN and Kaggle---\ie real code written by people with highly varying
  expertise in R and programming in general. This exercises a lot of the corner
  cases of the highly underspecified R behavior.

\end{compactitem}


\mypara{Limitations.} The analysis has some limitation. Even with the extension
described above, there are still \PkgUndefinedRnd \eval calls without source
references. This occurs when a reference to the \eval function is passed as
argument to a higher-order functions or when the \eval call originates from
native code. However, these missing source references account for a meager
\PkgUndefinedRatio of all calls; they are unlikely to affect our results. Other
limitations are that we ignore calls to the native \eval function and to the
alternate \c{rlang::tidy\_eval} function which uses native \eval internally.
The \c{rlang} package introduces a new kind of promise called \c{quosure} which
is evaluated by \c{tidy\_eval}. None of these limitations should invalidate our
conclusions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Usage Metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{wrapfigure}{r}{5cm} \hspace*{-12mm}
  \centering
  \includegraphics[width=72mm]{pkgs-eval-callsites-hist.pdf} \caption{CRAN
  \eval call sites}%
  \label{fig:pkgs-eval-callsites-hist}
\end{wrapfigure}
%
This section focuses on CRAN packages and reports statistics about the usage of
\eval. We use the word \emph{site} to an occurrence of a call site to the \eval
function in the source code, and \emph{call} to denote an observed invocation of
the \eval function.

\subsection{CRAN}

There are \PkgEvalCallSites \eval sites in \PkgPackages packages. The proportion
of packages calling \eval is \PkgPackagesRatio. Over half of these packages
have fewer than 3 sites, and with the exception \MaxEvalCallSitesPackage which
has \MaxEvalCallSitesCount sites, all packages contain fewer than
\MaxEvalCallSitesRest sites (\cf Fig~\ref{fig:pkgs-eval-callsites-hist} which
shows a histogram of sites per package). These sites appear in \PkgFunsWithEval
functions (\CranFunsWithEvalRatio of all functions in CRAN).


\begin{figure}[!b]
\small
\begin{tabular}{@{}l@{\hspace{1.5cm}}l@{}}
\begin{minipage} {5cm}
  \begin{tabular}{|r@{\,}r@{\,}l@{}r|r@{\,}r@{\,}l@{}r|} \hline
    \multicolumn{3}{|c}{\small\#calls} &\small \#pck
&     \multicolumn{3}{c}{\small\#calls} &\small\#pck \\\hline
\tt 1 &--& \tt 10      & \packageBina  & \tt 1K &--&\tt 100K  & \packageBine\\
\tt 11 &--& \tt 100    & \packageBinb  & \tt 100K &--&\tt 1M  & \packageBinf\\
\tt 101 &--& \tt 1K    & \packageBinc  & \tt 1M &--&\tt 10M   & \packageBing\\
\tt 1K &--& \tt 10K    & \packageBind  & \tt 10M &--& \tt 100M & \packageBinh\\\hline
\end{tabular}
\caption{Call frequency}\label{freq}
\end{minipage}
&
\begin{minipage}{7cm}
\begin{tabular}{|@{\,}r|rrrr|}\hline
  &\eval & \c{evalq} & \c{eval} & \c{local}\\[-2mm]
           & & & \c{.parent} &\\\hline
\small Static sites &\packageStaticeval&\packageStaticevalq&\packageStaticevalparent&\packageStaticlocal \\
\small Exercised sites&\packageTriggeredeval&\packageTriggeredevalq&\packageTriggeredevalparent&\packageTriggeredlocal\\
\small Invocations&\packageEvalsRnd&\packageEvalqsRnd&\packageEparentsRnd&\packageLocalsRnd\\\hline
\end{tabular}~\\[2mm]\caption{Variants}\label{tab:variantseval}
\end{minipage}\end{tabular}

%\end{figure}

%\begin{figure}[b] \centering
  \includegraphics[width=.78\textwidth]{traced-eval-callsites.pdf} \centering
  \caption{\eval call sites coverage of the \PkgPackages packages.}%
  \label{fig:traced-eval-callsites}
\end{figure}


For dynamic analysis, we run \CranRunnableScripts programs extracted from
\CranPackages packages. Any run that does not exercise \eval is discarded. This
left \packageNbruns runs from \packageCorpus packages. There were
\packageAllcalls calls in \packageTriggeredpkgs packages originating from
\PkgHitEvalCallSites unique sites. In terms of coverage, the data exercised
\PkgHitEvalCallSitesAvgRatio of sites, a coverage similar to the code coverage
metric for the packages which is \PkgCodeCoverage. The fact that not all sites
are exercised can be chalked down to incomplete tests and occasional analysis
failures (\PkgFailedProgramsRatio of programs crashed or timed out).
Fig~\ref{fig:traced-eval-callsites} shows the number of sites that were
exercised; coverage is unequal. Figure~\ref{freq} summarizes the frequency of
dynamic calls to \eval with the left column being the number of calls and the
right, the number of packages in that range. There are \packageFewcalls packages
with low \eval frequency, fewer than 100 calls; and a smaller, but still
significant, number of packages, \packageManycalls to be precise, that use \eval
more than 1,000 times. Package \packageMaxcallspack makes \packageMaxcalls calls
and thus accounts for over half of the observations.
Figure~\ref{tab:variantseval} summarizes the use of variants of \eval. For each
of the four variants, the first rows shows the number of sites in the corpus
(\emph{static}), the second row is the number of sites that were encountered
during analysis (\emph{exercised}), the last row is the number of calls
(\emph{invocations}). The overwhelming majority of sites and calls, are to \eval
itself, \c{eval.parent} is rare, and both \c{evalq} and \c{local} are barely
used at all. The difference between sites and exercised sites underscores the
limitations due to code coverage.

\begin{wrapfigure}{4}{5.8cm}
  \vspace*{-2mm}
\centering
  \begin{tabular}{|r@{\,}r@{\,}l@{\,}r|r@{\,}r@{\,}l@{}r|} \hline
\multicolumn{3}{|c}{\small\#calls} &\small\#sites &
\multicolumn{3}{c}{\small\#calls} &\small\#sites \\\hline
\tt 0 &--& \tt 50    & \packageRunbina & \tt 501 &--& \tt 1000   & \packageRunbine\\
\tt 51 &--& \tt 100  & \packageRunbinb & \tt 1001 &--& \tt 1500  & \packageRunbinf\\
\tt 101 &--& \tt 250 & \packageRunbinc & \tt 1501 &--& \tt 2000  & \packageRunbing\\
\tt 251 &--& \tt 500 & \packageRunbind & \tt 2001 &--& \tt 3000 & \packageRunbinh\\\hline
\end{tabular}

  \medskip  (a) All  \medskip  \medskip

  \vspace*{-1mm}
  \includegraphics[width=5.6cm, trim=5.5cm 0 5cm 0, clip]{package_calls_per_run_per_call_site}

  (b) Small

\caption{Normalized calls} \label{cn}\vspace{-2mm}

\medskip
\medskip

\begin{tabular}{c}
  \vspace*{-1mm}
  {\hspace{-3cm}\includegraphics[width=0.8\textwidth]{package_size_loaded_distribution}}
\end{tabular}
 \caption{Loaded code} \label{fig:sizedistribution}
\end{wrapfigure}

Figure~\ref{cn}(a) shows normalized call counts per site; on the left are
average number of calls from a given site and given run, on the right are counts
of sites that fall in that range. For instance, \packageRunbinh sites are
invoked 2,000+ times per run. Larger numbers suggest loops or recursive contexts
-- this seems to be the exception as most \evals, \packageRunbina, are exercised
50 times or less. Figure~\ref{cn}(b) zooms in on low frequency sites. The x-axis
shows normalized calls and the y-axis is the number of sites for that value. Most
low-frequency \evals are invoked only once, about half as many are invoked
twice; after that the frequency quickly drops.

\Eval takes any value, but if its argument is not an expression, \eval returns
it unchanged. Expressions account for \packageCodepercent of arguments in our
corpus. More specifically, \packageSymbolpercent are \texttt{symbol}s (single
variables such as \c{x}), \packageLanguagepercent are \c{language} objects
(function calls such as \c{f(x)} or \c{x+1}), and \packageExpressionpercent are
\c{expression} objects (lists of expressions such as function calls or
symbols). Further inspection reveals that most symbols,
\packageGgplotsymbolpercent to be exact, come from a single site in the
\c{ggplot2} package and have the value \c{\_inherit}.\footnote{This models
inheritance in \c{ggproto}, one of the many object-oriented systems in
R that is used exclusively in the \c{ggplot2} graphics library.}

To estimate how much executable code is injected through \eval, we measure the number of nodes in the expressions; for example, \c{x+1} counts as 3. We
also measured string lengths of unparsed expressions, but these measurements
were dominated by the size of data objects which could range in the MBs. The
median argument size is \packageMedianszeval (due to the symbols) and the
average is \packageAvgszeval nodes. The largest \eval input observed is
\packageMaxszeval\,-- a significant chunk of code.
Figure~\ref{fig:sizedistribution} shows the distribution of sizes for arguments
of fewer or equal to 25 nodes. The x-axis is the size of arguments in number of
nodes, and the y-axis is the count of arguments with that size. The size drops
rapidly, with few observations larger than 15 nodes. The long tail is omitted
for legibility.

To estimate the work performed in \evals, we count instructions executed by the
interpreter. Most invocations perform relatively little work with
\packageSmalleventspct of \evals executing 50 or fewer instructions. The violin
plot of Figure~\ref{ev}(a) corresponds to \evals executing $\leq$ 50
instructions, it is dominated by trivial symbol look ups. Figure~\ref{ev}(b) has
the work-intensive \evals which go all the way to \packageMaxeventsRnd
instructions.

\begin{figure}[tb!]
\begin{tabular}{@{}c@{}c@{}}
\begin{minipage}{7.5cm}
 \includegraphics[width=\textwidth]{package_events_per_pack_small}
\end{minipage}&\begin{minipage}{7.5cm}
  \includegraphics[width=\textwidth]{package_events_per_pack_large}
\end{minipage}\\[-3mm]
\small (a) Small & \small (b) Large
\end{tabular}
 \caption{Instructions per call} \label{ev}
\end{figure}

\subsection{Base}

We recorded \baseAllcalls \eval calls in the \BasePackages base libraries. This
comes from running 10\% randomly selected programs from the
\CranRunnableScriptsRnd extracted programs from CRAN packages, covering
\baseTriggeredevalpct of the \BaseEvalCallSites sites. Most of the calls
(\baseEvalsratio) are to the \c{eval} function and most arguments
(\baseCodepercent) are expressions. However, unlike CRAN, the majority
(\baseLanguagepercent) of the arguments are language objects. The median
argument size is \baseMedianszeval, which is more than for CRAN and the maximum
size is \baseMaxszeval, less than for CRAN. Small instruction counts ($<50$),
amount for \baseSmalleventspct of calls. A single site in the \c{match.arg}
function is responsible for \baseTopFuncPercent of all recorded calls. This
function provides a convenient way for argument verification using partial
matching. It is heavily relied on in R functions that use string arguments to
parameterize function behavior, a popular API mechanism in R. For example, in a
body of \c{center <- function(x, type = c("mean", "median", "trimmed"))} one
can get the value of \c{type} parameter using \c{type <- match.arg(type)}. A
call \c{center(x, "trim")} will match \c{type} to \c{"trimmed"}.

The challenge dealing with Base is that every program uses it. It can thus
generate extreme amounts of data but the data is quite predictable. For the
rest of the paper, we only mention Base when there are surprising
observations.

\medskip

\subsection{Kaggle}

In total, \kaggleAllcalls \eval were recorded, all to \eval function. Out of
\kaggleStaticeval sites from \KaggleWithEvals, only \kaggleTriggeredeval sites
in \kaggleNbruns scripts were hit. \KaggleFailedScripts scripts failed to run
and the other did not exercised the \eval sites. This is partially expected
as Kaggle code does not need to abide to any checks. Upon a manual inspection,
we observed that indeed the failing scripts were of a poor quality, often not
finished, using misspelled package names, hard-coded file paths or accessing
missing files.

Most call sites are invoked only once. Only one site is called \kaggleMaxcalls times.
Expressions amount for \kaggleCodepercent of arguments. Unlike CRAN corpus,
there are very few symbols (\kaggleSymbolpercent). Most expressions result from
calls to \c{parse}, thus most \evals start with strings. The median argument
size is \kaggleMedianszeval, which makes sense, as few arguments are only
symbols. The largest argument is \kaggleMaxszeval. The distribution of
instructions per eval is similar to CRAN; \kaggleSmalleventspct of \evals
execute fewer than 50 instructions. Manual inspection of \eval usage in Kaggle
suggest that it is consistent with the data obtained for CRAN. We do not
discuss it further.


\medskip

\subsection{Discussion}
These results show that \eval is central to the language implementation as it is
omnipresent in the relatively small Base library. The Base library is, in turn,
used by every single package. To avoid having Base dominate the result we have
excluded calls occurring there from the data we report when analyzing packages.
CRAN Packages, which are typically developed by experienced programmers, make
regular and varied use of \eval. They represent our most interesting data set as
these packages are the result of over 20 years of contributions by thousands of
authors. The code in packages is well maintained and relatively well tested.
Finally, Kaggle scripts are often written by less sophisticated users, and
likely perform simpler tasks, and thus have lesser need for \eval, and its uses
originate from strings as these are easier to manipulate for end-users. Due to
the relative paucity of data in Kaggle, we do not pursue the data set further.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Taxonomy of Eval}

The previous section gave a quantitative view of \eval usage; we now try to
elucidate \emph{what} it does.

\subsection{The expression in \eval} \label{sec:minimized}

The expressions passed to \eval vary widely. In order to categorize them, let us
use a minimization function $min(e)$ which for a given expression $e$ returns a
normal form that abstracts incidental details allowing the reader to focus on
the structure of the evaluated code. The minimization function performs constant
folding of arithmetic and string expressions for base operators, e.g.
$min(\c{1+1})=\c{V}$, value simplification, $min(\c{c(1,2,3+2)})=\c{V}$,
variable absorption, $min(\c{x+y})=\c{X}$, function absorption,
$min(\c{g(f(x),h(z))})=\c{F(F(X))}$ and a number of other simplifications.
Table~\ref{tab:minimizedexpressions} gives the 10 most frequent forms; \#sites
and \%sites are, respectively, the number and ratio of sites receiving arguments
of that form; \#packages is the number of packages with that form; \#operations
is the median number of instructions performed by the interpreter; and \%envir
is the ratio of sites that evaluate in a function environment. The example
column shows one sample expression $e$ that normalizes to the particular form.

\begin{table}[h]\small
\begin{tabular}{|c|r|r|r|r|r|c|}\hline
  $min(e)$& \#sites & \%sites & \#packages & \#operations & \%envir & example\\\hline
\c X&\packageMinimizedcallsitesa &\packageMinimizedpropsitesa &\packageMinimizedpackagea &\packageMinimizedmedianoperationsaRnd &\packageMinimizedpercentparentframesa & \c{y+1}\\\hline
\c{F(F(X))} & \packageMinimizedcallsitesb  & \packageMinimizedpropsitesb & \packageMinimizedpackageb  & \packageMinimizedmedianoperationsbRnd & \packageMinimizedpercentparentframesb & \c{gbov( mean(x), a-1)}\\\hline
\c{V}&\packageMinimizedcallsitesc &\packageMinimizedpropsitesc &\packageMinimizedpackagec &\packageMinimizedmedianoperationscRnd &\packageMinimizedpercentparentframesc& \c{c(42,21,0)}\\\hline
\c{F(X)}& \packageMinimizedcallsitesd & \packageMinimizedpropsitesd & \packageMinimizedpackaged & \packageMinimizedmedianoperationsdRnd & \packageMinimizedpercentparentframesd & \c{seq\_len(iters)} \\\hline
\c{\$} & \packageMinimizedcallsitese & \packageMinimizedpropsitese & \packageMinimizedpackagee & \packageMinimizedmedianoperationseRnd & \packageMinimizedpercentparentframese & \c{DF\$B}\\\hline
\c{model.frame}& \packageMinimizedcallsitesf & \packageMinimizedpropsitesf & \packageMinimizedpackagef & \packageMinimizedmedianoperationsfRnd & \packageMinimizedpercentparentframesf &  \c{model.frame(formula = Z $\sim$ U)}   \\\hline
\c{F()}& \packageMinimizedcallsitesg & \packageMinimizedpropsitesg & \packageMinimizedpackageg & \packageMinimizedmedianoperationsgRnd & \packageMinimizedpercentparentframesg & \c{rgamma(3, 2, n = 10L)} \\\hline
\c{FUN} & \packageMinimizedcallsitesh & \packageMinimizedpropsitesh & \packageMinimizedpackageh & \packageMinimizedmedianoperationshRnd & \packageMinimizedpercentparentframesh & \c{function(x, y) x + 3 * y} \\\hline
\c{<-} & \packageMinimizedcallsitesi  & \packageMinimizedpropsitesi & \packageMinimizedpackagei & \packageMinimizedmedianoperationsiRnd & \packageMinimizedpercentparentframesi & \c{x[1, 2:3, 2:3] <- value}\\\hline
\c{BLOCK} & \packageMinimizedcallsitesj & \packageMinimizedpropsitesj & \packageMinimizedpackagej & \packageMinimizedmedianoperationsjRnd & \packageMinimizedpercentparentframesj & \c{\{ write.csv(iris, tf) ; file.size(tf) \}} \\\hline
\end{tabular}
\caption{Minimized expressions} \label{tab:minimizedexpressions}
\end{table}

\noindent
We detail these forms and discuss their implication for the behavior of \eval.

\newcommand{\EE}[1]{{{\emph{\framebox{#1}}}}\\[1mm]}

\medskip\noindent\EE{$min(e)=\c{V}$} Expressions that represent values are
frequently passed to \eval -- they occur in 17\% of sites. The majority of
those, \packageValOneNodePercent, are inline constants (integer or double
vectors). The rest trivially evaluate to a value, \eg~\c{1+1}.\footnote{True as
long as base functions such as \c{+} are not redefined. They typically aren't,
but this is a limitation of this categorization.} In our corpus,
\packageNbCallSitesUniqueActualValue call sites only ever see a simple value.
Manual inspection reveals cases such as the following
\begin{lstlisting}
 f1 <- eval(paste("A~",paste(paste(names(X[,-1])),collapse="+")))
\end{lstlisting}
where the argument to \eval is a string which \eval simply returns. From twenty
randomly selected \evals, \packageUsefulValueEvalPercent of them do not need
\eval. For the other cases, either we are dealing with a value that needs to be
constructed dynamically or the value is a default case that sometimes is
replaced by a more interesting expression. This form is usually evaluated in few
interpreter steps, in fact the median is only
\packageMinimizedmedianoperationscRnd. The environment in which they evaluate is
mostly irrelevant (unless a built-in operator is redefined).

\medskip\noindent\EE{$min(e)=\c{X}$} Variables lookups are the most common form,
they are found in 28\% of the sites. This form includes simple variable reads,
\eg~\c{x}, those are \packageNbSymbolVarSitePercent of \c X. The form also
subsumes \c{V}, so it includes a mixture of arithmetic expressions,
\eg~\c{x+y+1}. The operations allowed are limited to built-in arithmetics. It is
noteworthy that, while most \c{X}\!s evaluate in a single step, the variable can
be bound to a promise, and accessing it may trigger evaluation of that promise,
thus resulting in an arbitrary amount of computation. The median number of
interpreted operations is \packageMinimizedmedianoperationsaRnd, suggesting that
it is not the common case. Lookups are often evaluated in constructed
environments, as few as \packageMinimizedpercentparentframesc of these
expressions are evaluated in a function environment.

\medskip\noindent\EE{$min(e)=\c{\$}$} This form extends \c X to include lookup
with the dollar operator, \eg~\c{x\$f}, and vector indexing, \eg~\c{x[42]} or
\c{x[[24]]}. As with \c X we allow arithmetics and values in this form. Lookup
occurs in 6\% of the sites. The interpreter evaluates
\packageMinimizedmedianoperationsgRnd operations on average; the minimum is 3
operations. This is typically used in a function environment,
\packageMinimizedpercentparentframese of the time to be precise.

\medskip\noindent\EE{$min(e)=$~\c{<-}} This form includes both assignments
operator, the direct assignment {\tt <-}, and assignment to the parent
environment {\tt <\,\!<-}, and the \c{\$} form. Assignments occur in 4\% of the
sites. They represent the most obvious source of side-effects. The median number
of operations is \packageMinimizedmedianoperationsiRnd; the minimum is 3.

\medskip\noindent\EE{$min(e)=\c{F()}$} This form captures simple function calls,
\eg~\c{f(2)}, with neither variables or assignments. Or, more specifically it
allows for variables in the function position but not in arguments. Usually,
looking up function names does not trigger computation, but that is not a given
if the function is returned by a promise or if the function name is shadowed by
a promise containing a value, then computation will occur. This form occurs in
6\% of sites and typically does not perform much work in the interpreter.

\medskip\noindent\framebox{$min(e)=\c{F(X)}$}~\EE{$min(e)=\c{F(F(X))}$} These
forms allow for function calls whose arguments may include variable references.
The latter allows nested calls.Assignments are excluded from this form. They
occurs in, respectively, 14\% and 20\% of the sites in the corpus. Together they
are the most frequent forms. The median numbers of interpreter steps are,
respectively, \packageMinimizedmedianoperationsdRnd and
\packageMinimizedmedianoperationsbRnd. The \packageMinimizedpercentparentframesj
of these expressions run in function environments.

\medskip\noindent\EE{$min(e)=\c{FUN}$} This form captures expressions define
functions, \eg~\c{function(x)x+1}, and do nothing else. \c{FUN} occurs in
\packageFunctionDefinitionSitesPercent of sites. Evaluating a function
definition is done in 2 interpreter step; the data does not record the work
performed by the interpreter when the generated functions are eventually run. In
addition to \c{FUN}, \packageGeneralizedFunctionDefinitionSitesPercent of sites
have function definitions nested in other expressions, this gives an idea of the
use of higher-order functions.

\medskip\noindent\EE{$min(e)=\c{BLOCK}$} This form captures multi-statements
code blocks, which occur in only 4\% of sites. These are larger expressions, we
do not inspect contents of the blocks. The median number of executed operations
is \packageMinimizedmedianoperationsjRnd. They typically run in function
environments.

\medskip\noindent\EE{$min(e)=\c{model.frame}$} The \c{model.frame} function
returns a dataframe that is the result of fitting the model described in a given
formula. This form subsumes \c{F(F(X))}, \c{FUN} and assignments. It is the
single most popular function invoked from \eval, it occurs in 7\% of the sites.
Each call does quite a lot of work with a 2K instructions median.

\paragraph{Consistency} It is interesting to consider how many different
forms any given site sees. The more forms, the harder it will be to characterize
the behavior of the program at that site. Luckily, \packageNbOneMinimizedPercent
of sites only see a single form. There are few sites that are highly polymorphic
(8 or more different forms), these include the pipe operator of the \c{magrittr}
package which is used to compose functions.

\paragraph{Discussion} The variety of uses of \eval is evidenced by the number
of different forms observed in CRAN. In comparison, JavaScript \eval usage was
simpler and more predictable as reported by \citet{oopsla12b}. Nevertheless,
simple forms dominate and there are many cases where \eval could be replaced
by less powerful constructs.


\subsection{The environments of \eval}\label{sec:env}

The environment in which evaluation happens determines what is visible to the
computation started by \eval and the potential reach of its side-effects. The
second argument to \eval specifies that environment. Environment of \eval can be
classified into the following four kinds:

\begin{compactitem}[---]
\item \emph{Function:} environment for the local variables of some function
  currently active on the call stack. Obtained by calling \c{parent.frame()} or
  \c{sys.frame()}.
\item \emph{Synthetic:} environments built from data structures such as lists,
  dataframes, or constructed explicitly with \c{new.env}, \c{list2env} or
  \c{as.environment}. Also includes the empty environment.
\item \emph{Global:} the environment in which scripts or interactive commands
  are evaluated.
\item \emph{Package:} the environment of a loaded library.
\end{compactitem}

\noindent
As shown by Table~\ref{tab:highlevelenvironments}, most calls evaluate in a
function environment with the \c{global} kind as a distant second. This means
that most variable lookups and most side effects either read/update existing
local variables or introduce new ones. But for which function? From the point of
view of the function that called \eval, Table~\ref{tab:funoffset} gives an
offset on the call stack. Thus 0 is the direct caller and corresponds to local variables; 1 is its parent, and so on. The data suggests that in 81\% of cases, \eval access its caller's environment -- this means, the variables of the function where \eval textually
occurs are read and written to. It is interesting that some 1.5\% of sites
evaluate a code in an environment that is three frame or above from \eval. This
distance implies that, in general, modular reasoning is impossible, to
understand the behavior of any piece of code entails fully understanding the
behavior of all functions that the code may call as the actions of \eval may
happen at a distance. Finally note that any particular site may have several
kinds of environments, but in \packageNbOneCategoryEnvirSitePercent of the case a
site has a single kind.

\begin{table}[h]
  \centering\small\hspace{-.5cm}
\begin{minipage}{3.7cm}
  \begin{tabular}{@{}r|r|r@{}}\hline
 Kind & \#sites & \%sites \\\hline
 Function & \packageNbFunctionEnvSites &  \packageNbFunctionEnvSitePercent\\
 Synthetic & \packageNbSyntheticEnvSites & \packageNbSyntheticEnvSitePercent \\
 Global &  \packageNbStrictGlobalEnvSites & \packageNbStrictGlobalEnvSitePercent \\
 Package & \packageNbPackageNamespaceEnvSites & \packageNbPackageNamespaceEnvSitePercent \\\hline
\end{tabular}
\caption{Kinds per site} \label{tab:highlevelenvironments}
\end{minipage}\hspace{-.2cm}
\begin{minipage}{3.7cm}\centering
\begin{tabular}{@{}r|r|r@{}}\hline
 Offset & \#sites & \%sites \\\hline
  \packageCallerEnvHierarchyNamea & \packageCallerEnvHierarchySitesaRnd & \packageCallerEnvHierarchySitePercenta \\
  \packageCallerEnvHierarchyNameb& \packageCallerEnvHierarchySitesbRnd & \packageCallerEnvHierarchySitePercentb \\
  \packageCallerEnvHierarchyNamec& \packageCallerEnvHierarchySitescRnd & \packageCallerEnvHierarchySitePercentc  \\
$\ge 3$& \packageNbFarAwayCallerSites &  \packageNbFarAwayCallerSitePercent \\\hline
 \end{tabular}
\caption{Function offset}\label{tab:funoffset}
\end{minipage}\hspace{-.2cm}
\begin{minipage}{3.7cm}
\begin{tabular}{@{}r|r|r@{}} \hline
Parent & \#sites & \%sites \\\hline
Function & \packageNewEnvCategorySitesa & \packageNewEnvCategorySitePercenta \\
Package & \packageNewEnvCategorySitesb &  \packageNewEnvCategorySitePercentb\\
Global & \packageNewEnvCategorySitesc & \packageNewEnvCategorySitePercentc \\
Empty & \packageNewEnvCategorySitesd & \packageNewEnvCategorySitePercentd \\\hline
\end{tabular}
\caption{Wrapper envs.} \label{tab:newenvs}
\end{minipage}\hspace{-.2cm}
\begin{minipage}{3.7cm}\centering
 \begin{tabular}{@{}c|c|c@{}} \hline
 \#kinds & \#sites &  \%sites \\ \hline
 \packageNbCategoryEnvira & \packageNbCategoryEnvirSitesaRnd &  \packageNbCategoryEnvirPercenta\\
 \packageNbCategoryEnvirb &  \packageNbCategoryEnvirSitesbRnd & \packageNbCategoryEnvirPercentb \\
 \packageNbCategoryEnvirc & \packageNbCategoryEnvirSitescRnd &  \packageNbCategoryEnvirPercentc\\
 \packageNbCategoryEnvird & \packageNbCategoryEnvirSitesdRnd & \packageNbCategoryEnvirPercentd\\\hline
\end{tabular}\caption{Multiplicities}\label{tab:polyenvir}
\end{minipage}\hspace{-1cm}
\end{table}

\noindent
Sites that evaluate in the global environment are likely split between
intentional and accidental use. Direct references to the top-level, using
\c{globalenv()} or \c{.GlobalEnv}, are rare, they occur in only
\packageNbExplicitGlobalSites sites. We suspect that the majority of uses of the
global environment come from the fact that our corpus consists of scripts that
are loaded at the top level, thus that environment may often be the caller of
\eval or close to it. The reason we make this point is that values stored in the
global environment are visible to all functions and are not reclaimed by the
garbage collector. So accidental uses may pollute that name space.

Synthetic environments need a parent. It is specified as an argument of the \c{new.env} function, or when \c{envir} is a list or a data frame, \eval uses its third argument
(\c{enclos}) as the parent.  The parent is used to lookup variables not found in its
child (side effects go to the child only). Table~\ref{tab:newenvs} shows the
kinds of the parents of synthetic environments. Over half are functions, then
come packages and global.


\paragraph{Discussion}
The data presented in this section is what one could expect. ({\it Function})
The majority of \eval call sites access the environment of some function on the
call stack. Usually, the current function, but sometimes frames arbitrarily far
up the call stack. One thing the data does not say is how that environment was
obtained. The expected case is that the environment was the one obtained from a
promise; as promises combine code and the environment in which that code
originates. Likely less frequent are cases where \eval is provided the results
of programmatically selecting some call stack. ({\it Synthetic}) The relative
high frequency of synthetic environments corresponds to cases when one wants to
evaluate an expression in either a restricted environment, use a data
structure as an environment. ({\it Global}) The fact that global environment
show up so frequently is likely an artifact of the way the code is run. ({\it
  Package}) There are only 92 sites that use a package environment in \eval,
this is probably best as it is bad form to add bindings or modify functions
of a loaded package. Many more sites rather wrap a package environment in a new
environment to create singletons in a package.

%% The \c{imchange} function of package \c{imager} makes it possible to modify
%% images using a dedicated formula syntax.\footnote{Inspired by {map} in
%% package \emph{purr}.} Here, \eval is evaluated in \c{newenv}, which creates a
%% new environment that inherits from \c{parent.frame()} by default (classified
%% as 1+).
%% \begin{lstlisting}
%%  newenv <- new.env()
%%  ...
%%  fo <- parse(text=as.character(fo)[2])
%%  im[where] <- eval(fo,envir=newenv,enclos=env)
%% \end{lstlisting}

%% \c{adjCoef} in package \emph{actuar} find the root of an equation defined by
%% a function \c{h} whose arguments must be named \c{x} and \c{y}. \c{h} is
%% transformed into an auxiliary function \c{h2} that can be optimized. Here,
%% the list used for \c{envir} ensures the correspondance between the textual
%% arguments of \c{h} and the arguments of \c{h2}.
%% \begin{lstlisting}
%%  sh <- substitute(h)
%%  fcall <- paste(sh, "(x, y)")
%%  ...
%%  h2 <- function(x, y)
%%  eval(parse(text = fcall),
%%  envir = list(x = x, y = y),
%%  enclos = parent.frame(2))
%% \end{lstlisting}

\subsection{The origins of \eval}

Where does the expression passed to \eval come from? There are various means of
creating that expression, these are associated with particular use cases, we
classify them in three categories:

\begin{compactitem}[---]
\item {\it Reflection:} This group corresponds to uses of the \c{match.call}
  function to reflectively capture the expression that invoked the current
  function.
\item {\it Constructed:} Expressions can be constructed by invoking the
  \c{quote}, \c{enquote}, \c{expression}. Function arguments are passed as
  promises and \c{substitute} is used to retrieve the source expression
  associated to the promise.
\item {\it String:} Finally, expressions can be created from strings by invoking
   \c{parse}, \c{str2expression} or \c{str2lang}.
\end{compactitem}

\noindent
Table~\ref{tab:provenance} summarizes the expression provenance in our corpus.
This data is obtained by dynamically tainting values as they are produced by the
various sources, due to technical reasons there is some imprecision in the
results. In particular, we are not able to classify all sites. Manual inspection
of numerous examples suggest that the classified results are accurate.

\begin{wraptable}{r}{5cm}\small\centering
\begin{tabular}{r|r|r} \hline
Origin  & \#sites & \%sites \\\hline
Constructed & \packageNbConstructedSites & \packageNbConstructedSitePercent \\
String & \packageNbStringSites & \packageNbStringSitePercent \\
Reflection &  \packageNbMatchCallExprsSites & \packageMatchCallExprsSitePercent\\\hline
\end{tabular}
\caption{Provenance}\label{tab:provenance}
\end{wraptable}

Strings could correlate with dynamic code loading. This is what the base
functions \c{source} and \c{sys.source} do. We observed few calls
(\packageNbParseFromFileSites in total) that consume the result of calling
\c{parse} on a file. Most of the calls build strings programmatically. We also
identified one function \c{invokeRestartInteractively} that prompts the user for
input, parses it, and passes it to \eval. The use of strings seems to correlate
with less sophisticated programmers; in our Kaggle corpus
\kaggleParseExprsSitePercent of site use strings.

\paragraph{Discussion}
The origin data suggests that constructing expressions from strings is a
minority of the use cases, instead the constructed category shows that the
majority of \evals comes code that was processed by the compiler, and may be
slightly modified by the programmer before invoking \eval. Both constructed and
reflection categories roughly correspond to meta-programming. Some of these use
cases could likely be replaced by macros if the R designers could be convinced
to overcome their distaste for those.

\subsection{Side effects in \eval}

The code executed by \eval can do side effects. From the compiler perspective,
we care about the observable side effects in environments---\ie variable
definitions, updates, and removals that are visible after a call to \eval
finishes. Knowing where---\ie in which environments do these side-effects
happen, can help us to determine how much of the compiler knowledge about the
program will be potentially invalidated.

The \rdyntrace contains low-level hooks capturing the life-cycle of an
environment and variables defined in it. In the tracer, we register callbacks
that record each environment creation and removal together with updates to its
bindings. From the recorded data, we ignore side effects
coming from \eval sites from unit testing frameworks.\footnote{In the corpus we
have \c{RUnit, testthat, tinytest} and \c{unitizer} unit testing frameworks.}
They run the testing code via \eval and thus the results would be biased
because of the high number of code run in these tests
(\CranRunnableCodeTestsRnd lines).

From the \packageNbrunsRnd programs we capture \SEAll side effects from
\SEAllCalls \eval calls in \SEAllSites sites. The challenge is again to remove
the accidental side-effects that are caused by the R virtual machine
implementation and are not related to the user code. For example, the
\c{.Random.seed} variable, which contains the state of the random number
generator is saved and restored from and to the global environment every time
user calls one of the base routines for getting random numbers. Removing them
leaves us with \SEUser side effects (\SEUserRatio) from \SEUserCalls \eval
calls (\SEUserCallsRatio) in \SEUserSites sites (\SEUserSitesRatio). These
sites are part of \SEUserFunctions in \SEUserPackages packages. \SEFunsNighty
functions are responsible for 90\% of side effects. Half of the side-effects
comes just from three functions: \c{plyr::allocate\_column} (allocates space
for a new data frame column), \c{withr::execute\_handlers} (executes deferred
expressions) and \c{foreach::doSEQ} (executes an expression on each element in
a collection, possibly in parallel).

Most of the \eval sites (\SESitesInEnvirRatio) do side effects in the
environment specified by the \c{envir} parameter (\cf
Section~\ref{sec:eval-in-r}), \SESitesNotInEnvirRatio modifies other
environment and finally \SESitesBothEnvirRatio does both.
Table~\ref{tab:se-env} shows the class of the environment where \eval side
effects happen (\emph{target environment}). It follows the classification
described in Section~\ref{sec:env} with two extra classes: \emph{Object} which
represents a side-effect that happens in S4 or R6 object environment, and
\emph{Local} that corresponds to a side effect in local variables of the \eval
caller. The table shows data for both \eval sites and functions. For a function
to have a given target environment class means that all of its \eval sites that
do side effect must have the same target environment class. Majority of the
\eval sites (\SESitesInOneClass) do all side effects consistently in one
environment class. The same happens on the function level. Almost half of the
sites and over a third of the functions do side effects in either \emph{Local}
or \emph{Object} environments. This gives a ray of hope for the compiler. Even
though it is possible to do anything anywhere, the data suggest that most
side-effects are sane.

\begin{table}[h]
  \small
  \centering
  \begin{tabular}{l|r|r|r|r}\hline
    Environments & \#sites & \%sites & \#funs. & \%funs. \\%
    \expandableinput tag/table-se-target-envs.tex
  \end{tabular}
  \caption{Most used target environments for \eval side-effects} \label{tab:se-env}
\end{table}

Table~\ref{tab:se-types} shows the proportion of the different side effects
that were recorded. In terms of calls we see mostly assignments and in terms of
sites definitions. This is expected. A subsequent \eval call will turn a
definition into an update. Variable removal, while rare, it does happen.
However, the vast majority comes from the already mentioned
\c{withr::execute\_handlers} function used to defer evaluation of an expression
to after the function exit. In our dataset, it is used almost exclusively by
the \c{tidyselect} package that uses it to remove the reference to current
quosure environment while interpreting a data frame column
selectors.\footnote{\cf \url{https://tidyselect.r-lib.org/}}

\begin{table}[h]
  \small
  \centering
  \begin{tabular}{l|r|r|r|r|r|r}\hline
    Side effect & \#events & \%events & \#calls & \%calls & \#sites & \%sites \\%
    \expandableinput tag/table-se-types.tex
  \end{tabular}
  \caption{Types of \eval side-effects} \label{tab:se-types}
\end{table}

\mypara{Discussion}
\todo{Is there any comparison with JS?}

\section{Usage of \eval}

\todo{Illustrate things that we have discussed in Section 5 - examples}
\todo{Things to discuss: model.frame, match.call, subset as example of NSE, substitute, parse, plots}


% We present here the main uses of \eval we have witnessed in R. Sometimes, they are design patterns, sometimes, \eval could be replaced by more specific functions. For each use of \eval, we discuss whether it is necessary to use.

% \subsection{Variable lookup}

% \eval is used to look-up the value of a variable in a given environment, different from the current scope so the \c{envir} argument of \eval is explicitly specified.

% \subsection{Metaprogramming}

% \mypara{\c{substitute}}

% \mypara{\c{match.call}}

% \todo{Talk about model.frame}

% \subsection{Logging}

% \subsection{Code transform}

% % Derivation of symbolic expressions

% \subsection{Plotting}

% \section{\eval expressivity and macros}

% The Thomas Lumley's article, Programmer's Niche: Macros in R, (https://www.r-project.org/doc/Rnews/Rnews_2001-3.pdf) describes how to write a `defmacro` function that would like as a macro creator, using `substitute` and `eval`.  There are no local macro variables in this implementation though.

The R language was intended to be extensible. The combination of lazy
evaluation, \c{substitute} and \c{eval} are the tools given to
developers to this end. This API is slightly more complex than just
passing a string, it is conceivable that this may discourage some
casual users. \Eval is also being used to reduce boilerplate code and
provide convenience features for programmers. We give examples for the minimized expressions of Section~\ref{sec:minimized} and then dig into high-level \eval design patterns.

\mypara{Variable lookup and values.}
This corresponds to minimized expressions \c{X}, \c{V}. We also include here lookup to slots in a list or data frame, \ie minimized expression \c{\$}.
It is the predominant usage of \eval in Base, through function \c{match.args}. In CRAN, it correlates with more exotic environments passed to \eval.

The dominant site in numbers of calls in CRAN, in the custom object system of \emph{ggplot2}, \c{ggproto}, also evaluates a variable \c{`\_inherit`} in the definition environment of the class. It leads to a large diversity of environments, many being package environment or package environments wrapped in \c{new}.

Package \emph{statnet.common} handles formulas, R object that associates an expression of the form \c{x ~ expr} and a carry-on environment where to evaluate the formula. It evaluates the left-hand side of the formula, which is always a variable, in \c{eval\_lhs.formula}, as \c{eval(object[[2L]],envir=environment(object))} where \c{object} is the formula and \c{environment} returns the formula associated to the object.


Slot lookup happens very often to evaluate some of the elements of a call list returned by \c{match.call}.
For instance in the \emph{pwrFDR} package:
\begin{lstlisting}
m <- match.call()
...
average.power <- m$average.power <- eval(m$average.power, sys.parent())
\end{lstlisting}

Here, \c{m\$average.power}, depending on the shape of the arguments to the surrounding function,  can be resolved to a variable, a more complex expression, or a number, leading to several possible minimized expressions, including \c{X} and \c{V}.

\mypara{Assigments.} It corresponds to minimized expression \c{<-} and groups together \c{<-}, \c{<<-} and \c{assign}. Package \emph{plyr} has a function combine data frames by row, filling in missing columns. During the allocation of the missing columns, an assignment call is built with \c{assignment <- quote(column[rows] <<- what)} and later evaluated on demand in a setter function, with \c{eval(assigmment)}. Package \emph{overture} is used to run Markov Chain Monte Carlo. It runs an R expression repeatedly and saves any assignments that happen, as samples of the Markov chains. 
The expression is captured with \c{substitute} and a new environment \c{env} wrapping the parent of the caller is created, in which all assignments will happen. Regularly, the content of \c{env} is inspected to sample the assigned variables.
\begin{lstlisting}
function(expr, overwrite=over.write) {
	expr_q <- substitute(expr)
	env <- new.env(parent=parent.frame(1))
	samps <- list()
	RunMcmc(samps, 1, expr_q, env, n.save, backing.path, thin, exclude, overwrite)
}

RunMcmc <- function... {
	 ...
	 eval(expr_q, envir=env)
\end{lstlisting}

\mypara{Function definition.} This corresponds to minimized expression \c{FUN}. Package \emph{Rcpp} is used to easily interface C++ and R and it uses \eval to bind C++ functions and classes to R objects.
For instance, \c{.makeCppMethods} create a R function for each C++ method named \c{what} of a given class.
\begin{lstlisting}
methods[[what]] <- eval(substitute(
	function(...) .CppObject$WHAT(...), list(WHAT = as.name(what))),
	env)
\end{lstlisting}

\mypara{Domain Specific Language.} R expressions are built and transformed symbolically and then evalled to a concrete value. For instance,   base R packages \emph{stats} provides two operators for \emph{symbolic differentiation}, \c{D} and \c{deriv}. They support arithmetic operations and functions on real numbers such as \c{sin}.
In \emph{MCMCglmm} (Monte Carl Markov Chain Generalised Linear Mixed Models), a \c{Dtensor} object is created, which contains expressions derived twice (with \c{DD}) and then typically in a list or a data frame.
\begin{lstlisting}
 expr<-expression(beta_1 + time*beta_2+u)
 ...
 mu = data.frame(beta_1=0.5, beta_2=1, time=3, u=2.3)
 ...
 D[i]<-eval(DD(expr, name[unlist(comb.pos[i,])]), mu)
\end{lstlisting}
% See https://github.com/cran/MCMCglmm/blob/cfac9e67ec73a3db2142826faae35e5d2318da31/man/evalDtensor.Rd

Package \c{tidyselect} is the backend of several \c{dplyr} and \c{tidyr} functions to select columns. It offers a DSL that makes it possible to select a set of columns, over a range of columns, a complement, an intersection, an union, with a regular expression and so on. \eval is used to traverse selectors stored as slots of a dataframe.


\mypara{Capturing a call.}  A common use case for \eval is to be combined with \c{match.call}.
\c{match.call} walks up the call stack, captures the code that invoked the
currently executing function, and returns it as an unevaluated expression that includes the call function and its unevaluated arguments. The
pattern is to transform a call to some function \c{f} into a call to \c{g} with
some arguments retained and others modified.  \todo{or change the target
function. Both are described as "patterns" in the R documentation.} As an
illustration, consider the \c{vcpart} package's function \c{tvcglm} that is
translated to a call to \c{tvcm} with two modifications to the argument list:
argument \c{control} can't be missing and \c{fit} is set \c{"glm"}. The
function ends with a call to \c{eval.parent} to ensure that the rewritten call
is evaluated in the same environment the original call was.  \begin{lstlisting}
tvcglm <- function(formula, data, family, ...) { k <- match.call() k[[1]] <-
as.name("tvcm") if ("fit" %in% names(list(...))) warning("'fit' ignored.")
k$fit <- "glm" eval.parent(k) } \end{lstlisting} This pattern is recognizable
by the fact that the expression is a call and the target environment is that of
the parent.

% We have quantitative data on that if needed...
This pattern is very often (more than half of the cases) used in relation with statistical models, to build a new model, \c{model.frame} to build a new generic model, or a more specialized model like \c{glm} or, \c{lm}. The R documentation of \c{match.call} explicitly describes this usage.

In package \emph{survival}\footnote{It is a \emph{recommended package}, so recommended by the CRAN maintainers, thoroughly tested, and following R best practices.}, in function \c{coxph}, the surrounding call is first captured, then \c{model.frame} is injected as the function name in the call, and finally, the new call is evaluated in the parent frame.

\begin{lstlisting}
Call <- match.call()
...
tform <- Call[c(1,indx)]  # only keep the arguments we wanted
tform[[1L]] <- quote(stats::model.frame)  # change the function called
...
mf <- eval(tform, parent.frame())
\end{lstlisting}

A related pattern is not to change the call but rather to evaluate the arguments in the parent frame. This could also be achieved with \c{substitute}, but \c{match.call} does it in bulk.
In package \emph{MedDietCalc},  in function \c{computeMDS95}, \c{match.call} retrieves the argument expressions of the call, which are evaluated in a dataframe \c{data} which contains the numeric data. Each argument of function \c{computeMDS95} is also a column in dataframe \c{data}. The advantage of using \c{eval} here is to be able to express an argument as a combination of several columns in the dataframe.
\begin{lstlisting}
computeMDS95 <- function (data, Vegetables, Legumes, FruitAndNuts, Cereals,
	 Potatoes = NULL, Meat, Dairy, 	Alcohol, Fats = NULL, MUFA = NULL,
	  SFA = NULL, Sex, men = "male", women = "female", 	frequency = NULL,
	  output = "percent", rm.na = FALSE) {

	arguments <- as.list( match.call())
	Vegetables <- eval(arguments$Vegetables, data)
	Legumes <- eval(arguments$Legumes, data)
	FruitAndNuts <- eval(arguments$FruitAndNuts, data)
	Cereals <- eval(arguments$Cereals, data)
	Potatoes <- eval(arguments$Potatoes, data)
	...
\end{lstlisting}


In the R community, the call capturing pattern is often referred~\cite{hadley} as \emph{Non-Standard Evaluation}.

\mypara{Injecting code in package environments.} R distinguishes two kinds of environments for packages: the \emph{package} environment itself, which contains every publicly accessible functions, and the \emph{namespace} environment, which contains all functions including private ones.

 A call to \c{library(packageName)}, which loads libraries in R, for instance, will
populate environment \c{package:packageName} using \eval. It is a \emph{base} function
though, so we do not see it in the CRAN dataset. However, R still makes it
possible to access any package environments, which we see in
\packageNbPackageEnvPackages different packages in our corpus. Those packages perform
introspection, to represent and search any environment by name, in package
\emph{envnames}, to containerize tests, as with \emph{testthat} and
\emph{unitizer}, to create a custom OOP system with \emph{R.oo}, or to bypass the visibility of functions in \emph{USSCXenaTools}. It is also used to
implement a custom caching mechanism in packages \emph{g.data} and \emph{SOAR}.

\mypara{Code Generation.} The more traditional use of \eval is to execute code
that was assembled by the programmer into a string. Here we show the method
\c{plot} for class \c{sback} in package \c{wsbackfit}, simplified for
explanatory purposes. The function takes a long argument list, the names of
which are captured in the list \c{opt}. The string \c{stub} is composed of a
subset of the arguments passed to this function; the variable is used to
construct a call to \c{base::plot} which will draw a plot. \NOTE{This example
  is a bit messed up. What is var? Does the use of ... in the eval grab the
arguments of this call? If yes does it not end up with xlab twice? HELP}
\todo{Does this comment still applies?} The \parse and \eval combination is
used by the the \source function in the base library to load R code from a file
in the current workspace.  \begin{lstlisting} plot.sback <- function(x,...) {
opt <- names(list(...)) stub <- paste( ifelse("xlab" %in%
opt,"",paste(",xlab=\"",var,"\"",sep="")), ifelse("type" %in%
  opt,"",",type=\"l\""), sep="") plot <-
  paste("plot(x.data,",stub,",...)",sep="") eval(parse(text=plot))
  \end{lstlisting} This pattern is recognizable by its use of \c{parse} to turn
  a string into an expression.


\mypara{Debloating.} \todo{Wouldn't reducing boilerplate be a better name?} \Eval is often used as a means to reduce boilerplate code;
simple and repetitive code can easily be replaced with judicious use of \eval.
For example, the \c{data.table} package uses \eval to calls the \c{options}
function with named arguments taken from a vector of strings. While the benefits
are limited in this example, it is an attractive tool for programmers.
\begin{lstlisting}
  opts = c("datatable.verbose"="FALSE", # ...many others
  for (i in names(opts))
    eval(parse(text=paste0("options(",i,"=",opts[i],")")))
\end{lstlisting}
This pattern is a special case of code generation, recognizable by the
fact that \eval is executed in a loop.

\mypara{Trivial.} When values are passed to \eval, they are returned
unchanged. They are an example of trivial uses of \eval. Another
trivial use is the empty expression, often found in JavaScript, but
rare in R. \todo{Given that we do not have any data I would get rid of it}
\todo{We have the normalized expressions V and the small study I did on values passed to eval. And I did not find any eval() (empty) in the dataset. }

% Creating macros?

\mypara{Unnecessary use of \eval}

Similarly to JavaScript, there are also unnecessary uses of \eval. For example,
the \c{PerformanceAnalytics} package contains a function \c{chart.QQPlot} that
uses \eval to resolve a string into function and another to call it and assign
its results into a variable:
\begin{lstlisting}
function (R, d="norm", dp, ...) {
q.f <- eval(parse(text=paste("q",d,sep="")))
z <- NULL
eval(parse(text=paste("z<-q.f(",dp,",...)")))
}
\end{lstlisting}
  In both cases, there is no need for \eval:
\begin{lstlisting}
function (R, d="norm", dp, ...) {
q.f <- get(paste0("q",d))
z <- q.f(dp, ...)
}
\end{lstlisting}
or even to a oneliner \c{do.call(paste0("q",d), as.list(dp, ...))}.

\todo{Aviral's story when he contacted a developer about his use of eval}

\mypara{Discussion.} % more...

\eval is a powerful tool and can be sometimes with functions with more specialized goals. For instance, variable lookup in any environment can be performed with \c{get}, and assignment in any environment, with \c{assign}. Building a call and executing can be done with \c{do.call}. Nevertheless, \eval shines when expressions passed to it can be of many different types. % And it is shorter to type, and no need to remember all the specialized functions.

\section{Conclusion}

\bibliography{bib/bibliography,bib/jv}

\end{document}
