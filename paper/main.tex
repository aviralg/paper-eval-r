\documentclass[USenglish,cleveref, autoref, thm-restate]{lipics-v2019}
\usepackage{listings,hyperref,multirow,paralist,xspace,url,wrapfig}
\input{macro}
\newcommand{\mypara}[1]{\medskip\noindent\emph{#1}\xspace}
\newcommand{\NOTE}[1]{{\it Note: #1}\xspace}

\title{Why do we Eval?\\[2mm]\Large A large-scale study of Eval usage in R}
\titlerunning{Why Eval (in R)?}

\begin{document}
\maketitle

\begin{abstract}
  \noindent Most dynamic languages allow users to turn text into code
  using various functions, often named \eval, with language-dependent
  semantics. The widespread use of these reflective functions hinders
  static analysis and prevents compilers from performing
  optimizations. This paper aims to provide a better sense of why
  programmers use \eval. Understanding why \eval is used in practice
  is key to finding ways to mitigate its negative impact. We have
  reasons to believe that reflective feature usage is language and
  application domain specific; we focus on data science code written
  in R, and compare our results to previous work that analyzed web
  programming in JavaScript. Dynamic analysis of a corpus of
  \CorpusAllCodeRnd lines of libraries and end-user code confirms that
  \eval is indeed in widespread use; R's \eval is more dangerous in
  some ways, and safer in others, than what was previously reported
  for JavaScript.
  %% TODO ::: Add a sentance about summarizing why users resort to
  %% eval -- Jan
\end{abstract}


\section{Introduction}

Most dynamic languages provide their users with a facility to
transform unstructured text into executable code and evaluate that
code. We refer to this reflective facility as \eval bowing to its
origins in LISP, all the way back in 1956~\cite{lisp}. \Eval has been
much maligned over the years. In computing lore, it is as close to a
boogeyman as it gets. Yet, for McCarthy, \eval was simply the way to
write down the definition of LISP, he was surprised that someone coded
it up and offered it to end users. Since then, reflective facilities
have been used to parameterize programs over code patterns that can be
provided after the program is written. The presence of such a feature
in a language is a hallmark of dynamism; it is a form of delayed
binding as the behavior of any particular call to \eval will only be
known when the program is run and that particular call site is
evaluated.

\vspace{2mm}\noindent\emph{Trouble in Paradise.} Reflective facilities
hinder most attempts to reason about, or apply meaning-preserving
transformation to, the code using them. In practice, \eval causes
static analysis techniques to loose so much precision as to become
pointless. For compilers, anything but the most trivial, local,
optimizations are unsound after a use of \eval. Furthermore, the
addition of arbitrary code --- code that could have been obtained from
a network connection --- as a program is running is a security
vulnerability waiting to happen. To illustrate these challenges,
consider the interaction of a static analysis tool with a dynamic
language. An abstract interpretation-based program analyzer computes
an over-approximation of the set of possible behaviors exhibited by
the program under study~\cite{cc77}. A reflective may have \emph{any}
behavior that can be expressed in the target language; i.e. \eval can
be replaced by any legal sequence of instructions. As dynamic
languages tend to be permissive, the analysis has to, for example,
assume that many (or all) functions in scope may have been redefined,
e.g. that \texttt{`+`} now opens a network connection or something
equally surprising. A single occurrence of \eval causes the static
analyzer to loose all information about program state and
meaning of identifiers. This loss of precision can sometimes be
mitigated by analyzing the string argument~\cite{moller03} to bound its
possible behavior but when the string comes from outside the program
not much can be done. A frustrated group researchers argued giving up
on soundness and, instead, under-approximating dynamic features
(soundiness)~\cite{soundy}. In their words ``a practical analysis,
therefore, may pretend that \eval does nothing, unless it can
precisely resolve its string argument at compile time.'' Alas,
assuming that \eval does not have side-effects, or that side-effects
will not affect the results of the analysis, may be unduly optimistic.

\vspace{2mm}\noindent\emph{Is Past Prologue?} Previous work
investigated how \eval is used in web programming, specifically in
websites that use JavaScript~\cite{pldi10a}. In 2010, 17 of the
largest website used the feature. In 2011, 82\% of the 10,000 most
accessed sites used \eval~\cite{ecoop11}.  Yet, the strings passed to
\eval, and their behaviors when executed, are far from random; it was
shown that when one can observe several calls to \eval, the ``shape''
of future calls can be predicted with 97\% accuracy~\cite{oopsla12b}.
Overall, practical usage suggested that most reflective calls were
relatively harmless. While this backs up the soundiness squad's
approach, does it generalize to other application domains than web
programming and to other languages?

\vspace{2mm}\noindent\emph{The Here and Now.} In this study, we
investigate the usage of \eval in programs written in the R
programming language. R is language designed by statisticians for
applications in data science~\cite{r,R96}. What makes looking at R
after JavaScript interesting is that, while both languages are
dynamic, they are quite different. While one can program in an
object-oriented style in R, like in JavaScript, R is primarily a lazy,
untyped, functional language. JavaScript was designed to run untrusted
code in browser, while R is used for statistical computing on
desktops. JavaScript is a general purpose language used by a wide
community of programmers; while R is used for scientific computing by
data scientists and domain experts with, often, limited programming
experience. One can distinguish between library implementers,
developers with some programming experience and a working knowledge of
R, and end-users, who are typically not expert programmers and often
have only a cursory knowledge of the language.\footnote{Consider that
R evaluates function argument lazily, just like Haskell. We informally
surveyed end-users, including computer scientists, and did not find a
single user aware of this fact. Library developers, on the other hand,
know about laziness and program defensively around it.} Our goal is
thus to highlight the differences in usage between JavaScript and R,
and try to explain those differences in terms of language features,
application domain and programmer experience. Hopefully some of our
observations will generalize to other languages.

\vspace{2mm}\noindent\emph{The What and How.} One significant benefit
of choosing R is that every package in the CRAN repository is curated
and comes with examples of typical usage. This gives us a large code
base that we can analyze dynamically. To observe \eval we built a
two-level monitoring infrastructure:\footnote{Our infrastructure is
open source and publicly available, our anonymized code is at {\tt
  http://url.com}, a more complete artifact will be submitted to the
artifact evaluation committee. } we can monitor R programs by
instrumentation --- this gives us access to many user-visible
properties of R programs --- but we can also monitor the
inner-workings of the R interpreter --- this allows us to capture
details not exposed at the source level. Dynamic analysis is limited,
it can only observe behaviors triggered by the particular inputs
passed to a program. Luckily, CRAN libraries come with many tests and
use-cases. The choice of corpus is crucial. Our corpus has been
constructed to reflect the levels of sophistication of the R
community. We distinguish between \emph{CRAN packages}
(\CorpusPackages curated packages that pass stringent quality checks
and are equipped with tests and sample data) and \emph{Kaggle scripts}
(\CorpusFinishedKaggle end-user written programs that performs a
particular data analysis task). It is reasonable to expect that \eval
usage differ between these datasets: the libraries represent a lively
ecosystem with new libraries added each day, while end user code is
often thrown together, run once, and never revisited.


\vspace{2mm}\noindent\emph{Why do we Eval?} {\bf A short summary of the
results should go here.}


\section{Background and Previous work}

This section provides a short introduction to R as it has a few
surprising features that impact the use of reflective features of the
language. We then look at the semantics of \eval in R and discuss some
design choices. Lastly, we put this paper in context of previous work.

\subsection{R in a nutshell}

R is a lazy functional programming language~\cite{ecoop12} with
dynamic features that allow to write object-oriented code. Most data
types are vectorized. Values are constructed by the \c{c(...)}
function, e.g. \c{c("a","bc")} creates a vector of two strings. To
enable equational reasoning, R copies values accessible through
multiple variables when they are written to. Values can be tagged by
user-defined attributes. For instance, one can attach the attribute
\c{dim} to the value \c{x<-c(1,2,3,4)} with \c{attr(x,"dim")<-c(2,2)}.
This causes arithmetic functions to treat \c x as a $2 \times 2$
matrix. Another attribute is \c{class} which can be bound to a list of
names, e.g., \c{class(x)<-"human"}. This sets the class of \c{x} to
\c{human}; classes are used for object-oriented dispatch. Every R
linguistic construct is desugared to a function call, even control
flow statements, assignments, and bracketing. Furthermore, all
functions can be redefined in user code. This makes R both flexible
and challenging to compile~\cite{dls19}. Arguments to user-defined
functions are bundled into thunks called \emph{promises}. Logically, a
promise combines an expression's code, its environment, and its value.
To access the value of a promise, one must force it. Forcing a promise
triggers evaluation and the computed value is captured for future
reference.


\subsection{Eval Semantics}

The expressive power of \eval depends on design decisions along two
axes:
\begin{itemize}
\item {\bf Scoping:} What environment does \eval executes its argument
  in? JavaScript and R evaluate it in the current environment, thus
  exposing local variables and parameters and breaking the caller's
  abstraction boundary. Julia is more restrictive, \eval runs at the
  ``top level'', in the global environment. JavaScript has a global
  \eval that behaves like Julia, and a ``strict mode'' in which \eval
  may access local variables, but fobidden from injecting new ones in
  the local environment. In Java, one can implement \eval with
  Julia-semantics.\footnote{While \eval used to be the purview of
  interpreted languages, just-in-time compilation lifted this
  restriction. \Eval takes its input, wraps it in a static method of a
  new anonymous class, generates bytecode for that class, invokes
  the class loader to install that code, and finally reflectively
  calls the method. }
\item {\bf Reflective API:} What other reflective operations are
  exposed to user code? JavaScript provides few reflective functions
  (other than enumeration of an object's properties and string-based
  indexing), the impact of \eval is thus limited. R, on the other
  hand, has a rich reflective API. For example, R allows user code to
  walk the call stack and arbitrarily add and delete local variables.
  This means that there are few limits to the side-effects of an
  \eval.
\end{itemize}

\noindent
These design choices impact our ability to reason about code when
reflection is used. A more expressive \eval translates directly into
additional restrictions for program analysis and transformation tools.
The reason Julia restricts \eval to the top level is to preserve the
compiler's ability to optimize code~\cite{oopsla18a}. In particular,
\eval had to be prevented from changing the type of local variables as
this would require recompilation of the method. Julia even adds a
versoning mechanism, called world age, to ensure that code added
during an \eval does not invalidate inlining
optimizations~\cite{oopsla20a}. More permissive languages such as R
are much harder to deal with from a compiler's perspective.

\mypara{Eval in R.} The reflective interface exposed by
R is somewhat complex. The core library exports four functions with
slightly different semantics, \c{eval}, \c{evalq}, \c{eval.parent},
and \c{local}. \Eval is the more general function, and, unlike in
JavaScript, it takes three arguments, an expression, its environment
and the enclosing environment. The following discussion simplify some
details which are not relevant to this paper. For the interested
reader we recommend the excellent book by Hadley
Wickham~\cite{hadley}. The definition of \eval starts as follows:

\begin{lstlisting}
 eval
   <- function(expr,
                envir = parent.frame(),
                enclos = if(is.list(envir)) parent.frame() else baseenv())
          { ...body... }
\end{lstlisting}
The parameters are \c{expr}, the value to be evaluated, \c{envir}, the
environment in which evaluation happens, and \c{enclos}, the
environment to look up objects not found in \c{envir} (by default
either the top level, \c{baseenv} or the environment of the caller).
The \c{expr} parameter can take values of different type, for our
purposes we focus on the most common, {\tt expression}s. It is easiest
to think of an \c{expression} as an abstract syntax tree as returned
by R's parser. The simplest way to create an \c{expression} is to call
\c{quote} passing some R expression:
\begin{lstlisting}
  > quote(a + b)
  # a + b
\end{lstlisting}
Here the return value is an abstract syntax tree. From a
string, use  the \c{parse} function:
\begin{lstlisting}
  > parse(text="a+b")
  # expression(a+b)
\end{lstlisting}
The most common way to create an expression is to extract it from an
argument. Each argument comes packaged inside a promise which retains
the source code of the argument. Consider the following function
definition, \c{f} is a function with a single parameter \c{x}. When
the call \c{f(a+b)} is evaluated, \c{a+b} is stored inside a newly
created promise.
\begin{lstlisting}
  > f <- function(x) substitute(x), list(a=1)))
  > f(a+b)
  # expression(1+b)
  > substitute(a+b,list(a=1,b=2))
  # 1+2
\end{lstlisting}
The call to \c{substitute(x)} extracts, from the promise bound to
\c{x}, the expression passed in the call to \c{f}, i.e., \c{a+b}. The
\c{substitute(exp,env)} function takes two arguments, the second is an
object that can be used as an environment. This can be either a list
of named values, a data frame or an actual environment. \c{substitute}
will look for occurrences of each symbol from \c{env} in \c{exp} and
replace them with their value taken from \c{env}.
\begin{lstlisting}
  > environment()
  # <environment: R_GlobalEnv>
\end{lstlisting}
The above shows how to access the current environment, this can belong
to the current function or, as above, the top level. Environments are
nested, with each environment having a parent. This nesting is used
when looking up names. When calling \c{new.env}, the default parent is
the current environment. The chain of environments can be traversed
with \c{parent.env}, ultimately one arrives at \c{emptyenv}.
\c{parent.frame} and \c{sys.frame} grant access to environments
further up in the calling stack, up to the global environment
\c{.GlobalEnv}, and packages. One can also directly read, modify or
create new bindings, given any environment:
\begin{itemize}
\item \c{env$v} and \c{get("v",envir=env)} read variable \c{v} in
  environment \c{env};
\item \c{env$v<-2} and \c{assign("v",2,envir=env)} write 2 in
  \c{v}. If not found, \c{v} is created in \c{env}.
\end{itemize}
\noindent Environments are often used as hashtables by programmers as
they have reference semantics (other are copy-on-write) and have a
built-in string look up. across function calls, or to create package
namespaces.
\begin{lstlisting}
  evalq <- function(exp,env,enclos) eval(quote(exp),env,enclos)
  eval.parent <- function(exp,n) eval(exp,parent.frame(n))
  local <- function(exp,env,enclos) evalq(exp,new.env())
\end{lstlisting}
The three \eval variants can be expressed as calls to \eval. The
\c{evalq} form quotes the argument to prevent it from being evaluated
in the current environment. The \c{eval.parent(e,n)} form evaluates
\c{e} in the environment of the {\tt n}-th caller of this function.
Finally, \c{local} evaluates an \c{exp} in a new environment to
avoid polluting the current one.

\subsection{Eval Usage in R}

The R language was intended to be extensible, the combination of lazy
evaluation, \c{substitute} and \c{eval} are the tools given to
developers to this end. This API is slightly more complex than just
passing a string, it is conceivable that this may discourage some
casual users. \Eval is also being used to reduce boilerplate code and
provide convenience features for programmers. We now give some
representative examples of its usage.

\mypara{Intercession.} A common use case for \eval is to be combined
with \c{match.call}. \c{match.call} walks up the call stack, captures
the code that invoked the currently executing function, and returns it
as an unevaluated expression. The pattern is to transform a call to
some function \c{f} into a call to \c{g} with some arguments retained
and others modified. As an illustration, consider the \c{vcpart}
package's function \c{tvcglm} that is translated to a call to \c{tvcm}
with two modifications to the argument list: argument \c{control}
can't be missing and \c{fit} is set  \c{"glm"}. The
function ends with a call to \c{eval.parent} to ensure that the
rewritten call is evaluted in the same environment the original call
was.
\begin{lstlisting}
 tvcglm <- function(formula, data, family, control=tvcglm_control(), ...) {
   Call <- match.call()
   Call[[1L]] <- as.name("tvcm")
   if (!"control" %in% names(Call)) Call$control<-formals(tvcglm)$control
   if ("fit" %in% names(list(...))) warning("'fit' is ignored. ")
   Call$fit <- "glm"
   return(eval.parent(Call))
 }
\end{lstlisting}
This pattern is recognizable by the fact that the expression is a call
and the target environment is that of the parent.

\mypara{Code Generation.} The more traditional use of \eval is to
execute code that was assembled by the programmer into a string. Here
we show the method \c{plot} for class \c{sback} in package
\c{wsbackfit}, simplified for explanatory purposes. The function takes
a long argument list, the names of which are captured in the list
\c{opt}. The string \c{stub} is composed of a subset of the arguments
passed to this function; the variable is used to construct a call to
\c{base::plot} which will draw a plot. \NOTE{This example is a bit
  messed up. What is var? Does the use of ... in the eval grab the
  arguments of this call? If yes does it not end up with xlab twice?
  HELP} The \parse and \eval combination is used by the the \source
function in the base library to load R code from a file in the current
workspace.
\begin{lstlisting}
 plot.sback <- function(x,...) {
   opt <- names(list(...))
   stub <- paste(
    ifelse("xlab" %in% opt,"",paste(",xlab=\"",var,"\"",sep="")),
    ifelse("main" %in% opt,"",main.aux),
    ifelse("type" %in% opt,"",",type=\"l\""),
    sep = "")
   plot <- paste("plot(x.data,",stub,",...)",sep="")
   eval(parse(text=plot))
   ...
\end{lstlisting}
This pattern is recognizable by its use of \c{parse} to turn a string
into an expression.


\mypara{Debloating.} \Eval is often used as a means to reduce
boilerplate code; simple and repetitive code can easily be replaced
with judcious use of \eval. For example, the \c{data.table} package
uses \eval to calls the \c{options} function with named arguments
taken from a vector of strings. While the benefits are limited in this
example, it is an attractive tool for programmers.
\begin{lstlisting}
  opts = c("datatable.verbose"="FALSE", # ... many others
  for (i in names(opts))
    eval(parse(text=paste0("options(",i,"=",opts[i],")")))
\end{lstlisting}
This pattern is a special case of code generation, recognizable by the
fact that \eval is executed in a loop.

\mypara{Trivial.} When values are passed to \eval, they are returned
unchanged. They are an example of trivial uses of \eval. Another
trivial use is the empty expression, often found in JavaScript, but
rare in R.


\subsection{Previous Work}

Richards et al.~\cite{ecoop11} provided the first large-scale study of
the runtime behavior of \eval in JavaScript. They dynamically analyzed
a corpus of the 10,000 most popular websites with an instrumented web
browser to gather execution traces. They show that \eval is pervasive
with 82\% of the most popular websites using it. The reasons for its
use include the desire to load code on demand, deserialization of JSON
dataq and lightweight meta-programming to customize web pages. While
many uses were legitimate, just as many were unnecessary and could be
replaced with equivalent and safer code. They categorized inputs to
\eval so as to cover the vast majority of input strings. Restricting
themselves to \eval in which all named variables refer to the global
scope, many patterns could be replaced by more disciplined
code~\cite{oopsla12b, moller12}. The work did not measure code
coverage, so the numbers presented are a lower bound on the possible
behaviors. Furthermore, JavaScript usage in 2011 is likely different
from today, e.g. Node.js was not covered by Richards. More details
about dynamic analysis of JavaScript can be found in~\cite{liang}.

Wang et al.~\cite{wang} analyzed use of dynamic features in 18 Python
programs to find if they affect file change-proneness. Files with
dynamic features are significantly more likely to be the subject of
changes, than other files. Chen et al. looked at the correlation
between code changes and dynamic features, including \eval, in 17
Python programs~\cite{chen}. They did not observe many uses of \eval.
Callau et al.~\cite{oscar} performed an empirical study of the usage
of dynamic features in 1,000 Smalltalk projects. While \eval itself is
not present, Smalltalk has a rich reflective interface. The authors
found that reflective are used in less than 2\% of methods. The most
common reflective method is \c{perform:}; it send a message that is
specified by a string. These features are mostly used in the core
libraries.

Bodden et al.~\cite{bodden} looked at usage of reflection in the Java
DaCapo benchmark suite. They found that dynamic loading was triggered
by the benchmark harness. The harness then executes methods via
reflection, this caused static analysis tools to generate an incorrect
call graph for the programs in DaCapo.

Morandat et al.~\cite{ecoop12} had a short section on the usage of
\eval in R. They found the it widely used in R code with 8500 call
sites in CRAN and 2.2 million dynamic calls. The 15 most frequent call
sites account for 88\% of those. The \c{match.arg} function is the
highest used one with 54\% of all calls. In the other call sites, they
saw two uses cases. The most common is the evaluation of the source
code of a promise retrieved by \c{substitute} in a new environment;
e.g. as done in the \c{with} function. The other use case is the
invocation of a function whose name or arguments are determined
dynamically. For this purpose, R provides \c{do.call} and thus \eval
is overkill.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

This section explains our methodology for selecting the corpus that
will be analyzed and how we implemented our dynamic analysis.

\subsection{Corpus}

Our corpus is assembled out of two set of programs, one consisting of
popular libraries obtained from the Comprehensive R Archive
Network\footnote{CRAN is the largest repository of R code with over
\CorpusAllCranRnd packages. It receives about 6 new package
submissions a day~\cite{Ligges2017}. Unlike other open code
repositories such as GitHub, CRAN is a curated repository. Each
submitted package must abide to a number of well-formedness rules that
are automatically checked asserting certain quality. Most relevant for
this work is that all of the runnable code is tested and only a
successfully running package is admitted in the archive.} (we refer to
it as {\bf CRAN}) and the other from the Kaggle data science
competition (written {\bf Kaggle}). The intent here is to contrast
code written by experienced developpers (CRAN) with code authored by
typical end users of the language (Kaggle).

All the code in our is runnable, i.e. we have scripts that invoke the
CRAN packages and the Kaggle programs have input data. Clearly code
coverage will vary; as is the case for any dynamic analysis, the
quality of results is predicated on how representative the executions
we can observe are with respect to all possible runs of the corpus. To
mitigate the threat to validity attached to limited coverage, we
obtain clients that will generate multiple \emph{runs} of CRAN
packages. In this context, a run is the execution of a CRAN package or
Kaggle program with a particular set of input values.

\mypara{CRAN Packages.} The packages that are included
in this study are the top \CorpusPackages packages based on their
reverse dependencies. The set of reverse dependencies for some package
\emph{P} is transitive closure of packages that import \emph{P}. The
hypothesis underlying that choice is that packages with a large set of
reverse depencies are likely to be higher in quality and their tests
will provide better coverage. The packages are implemented in R with
some C and Fortran code. Using \c{cloc}, we measure a total
\CorpusRCodeRnd lines of R code and \CorpusNativeCodeRnd lines of
native code. For each package, we use its runnable code (tests, etc.)
to compute code coverage. On average, the cde coverage is
\CorpusMeanExprCoverage which is acceptable without being exhaustive.
Figure~\ref{fig:corpus} shows these packages, the size of the dots
reflects the project's size in lines of code. The x-axis indicates
code coverage in percents and the y-axis gives the number of call
sites to \eval that were traced, in log scale. Dotted lines indicate
means. Packages with over \CorpusEvalsPackageTreshold eval call sites
are named.

\begin{figure}[!h]\centering\includegraphics[width=.7\linewidth]
  {corpus.pdf}\caption{CRAN packages}\label{fig:corpus}
\end{figure}

To analyze a package, we need client code that invokes its methods.
There are three sources of built-in runnable code that come with each
CRAN package: \emph{tests, examples} and \emph{vignettes}. They are,
respectively, traditional unit tests, code snippets from the
documentation, and long-form use-cases written in Rmarkdown. Examples
and vignettes are automatically extracted and turned into scripts,
their input is bundled with the package. Most tests had to be discared
due to a limitation of our pipeline: the \c{testthat} harness uses
\eval and thus causes the entire test to register as an \eval call.
The selected packages are bundled with \CorpusPackagePrograms
programs; \CorpusExamplesProgramsRnd examples and
\CorpusVignettesProgramsRnd vignettes.

\mypara{Kaggle Scripts.} Kaggle is an online platform
for data-science and machine-learning. The website allows people to
submit data-analysis problems, users compete to find the best
solution. The solutions are uploaded to the platform as either plain
scripts or notebooks. We chose one of the most popular competition,
predicting the survival of passengers the
Titanic\footnote{\url{https://www.kaggle.com/c/titanic}}. Unlike CRAN,
Kaggle is not curated. After downloading the \CorpusKaggle solutions
and extracting the R code, we found that \CorpusDuplicatedKaggle were
duplicates. From the remaining \CorpusRunnableKaggle solutions,
\CorpusFailedKaggle failed to execute. Next to various runtime
exceptions, common problems were parse errors and misspelled package
names. The final set contains \CorpusFinishedKaggle programs
implemented in \CorpusFinishedKaggleCodeRnd lines of R code.



\subsection{Analysis Pipeline}

The results presented in this paper are the result of an automated
analysis pipeline that acquires the code of packages, extract
metadata, executes programs, traces their behavior and summarizes the
observations. Figure~\ref{fig:pipeline} shows the main steps of the
pipeline along with approximate time to execute each step, the data
size, and the number of elements manipulated by the stage. Timings are
for runs on an Intel Xeon 6140, 2.30GHz with 72 cores and 256GB of
RAM.

\begin{figure}[!h]\hspace{-5mm}
  \includegraphics[width=1.05\linewidth]{pipeline.pdf}
  \caption{Analysis Pipeline}\label{fig:pipeline}
\end{figure}

The first step in the pipeline consist of downloading CRAN packages
along with their dependencies and acquiring Kaggle programs with the
help of a web crawler. The second step, is to compute code size and
coverage metrics for the CRAN packages (we use the \covr package for
coverage). The third step consist in extracting runnable programs from
packages: i.e. the tests, examples and vignettes. Each extracted
program is wrapped into a call to our dynamic analyzer --- the tool is
called \emph{evil} for \underline{ev}al \underline{i}nspection
\underline{l}ibrary. This step is needed to ensure that we record
\eval usage only for the target package. Without this, the data would
include eval calls from the unit testing frameworks as well as from
bootstrapping R virtual machine itself. To avoid any interference,
each program is run in its own R instance.
The fourth step in the pipeline is to perform dynamic analysis for
each run of a CRAN package or Kaggle program.

The dynamic analyzer builds upon the dynamic analysis framework,
\instrumentr that we have implemented to enable us to write dynamic
analysis logic in R. \instrumentr serves as an intermediary between
\rdyntrace and \evil, it intercepts the hooks exposed by \rdyntrace
and attaches R functions exported by \evil as callbacks. The \evil
callbacks execute on corresponding interpreter events.


The data extracted by \evil from each program is
concatenated, cleaned and summarized in the post-processing phase by
custom R scripts. Finally, the summarized data is analyzed in
RMarkdown notebooks to gather insights. Apart from the figures, the
data points included in the paper are also generated by RMarkdown
notebooks as latex macros.


The \emph{evil} framework is implemented as a R package in 2K lines of
R and 400 lines of C++ code. \instrumentr is an R package implemented
in 2.5K lines of R and 6K lines of C++ code. It internally uses a
modified R interpreter, \rdyntrace~\cite{oopsla19a}, that exposes
hooks from within the interpreter implementation for events of
interest.

All steps of this pipeline are parallelized using GNU
parallel~\cite{GNUparallel} and orchestrated by GNU make. To schedule
and parallelize extraction and analysis of programs, we use the \runr
package. Furthermore, \runr gracefully handles and reports failures
across large-scale program runs which greatly aids debugging of the
analysis pipeline.



\mypara{Limitations.} Dynamic analysis can only observe
calls that are triggered by the program's input. We believe that
focusing on R packages with high code coverage does mitigate this to
some extent. The results we report here were obtained with R's
bytecode compiler turned off, this should not affect the results as
the compiler does not optimize \eval.


We turn off the bytecode compiler for this study. The bytecode
compiler can also call \eval. We do not get source locations for
\UndefinedEvalsRnd \eval calls. In these cases \eval is either passed
as an argument to a higher-order functions or is defined in a function
returned by a higher-order function and the R parser does not retain
location information for \eval. However, this is a meager
\PercentUndefinedEval of all \eval calls and is unlikely to affect our
analysis. We ignore calls to the native \eval function exposed by R.
We also ignore the \c{rlang::tidy_eval} function which uses native
\eval internally because \c{rlang} is used to implement a DSL for data
analysis in R. It introduces a new first-class promise object called
\c{quosure} for which it implements special evaluation support in
\c{tidy_eval}.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Usage Metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section focuses on our corpus of \Corpus CRAN packages that use
\eval; these represent {\bf XX\%} of the entire repository. To provide
a picture of \eval usage, all runnable code in CRAN was executed. Runs
that did not exercise an \eval in the corpus were discarded. We
retained \Nbruns to generate the data presented here. In total,
\Allcalls invocations of \eval and its variants were recorded. Out of
the \Staticatleastonecallsite packages that use \eval, \Triggeredpkgs
packages were observed calling it in our runs --- the difference can
be chalked down to incomplete code coverage and occasional
infrastructure failures.


Figure~\ref{freq} summarizes the frequency of calls to \eval over all
runs. The left column is the number of observed calls, the right
column is the number of packages falling in that range. There are
\Fewcalls packages with low \eval frequency, i.e. fewer than 100
calls; and a similar number of packages, \Manycalls to be precise,
that use \eval more than 1,000 times. One package, \Maxcallspack,
calls it \Maxcalls times and thus accounts for over half of the
observed data.

\begin{figure}[!h]

\begin{tabular}{@{}l@{\hspace{1.5cm}}l@{}}
\begin{minipage} {5cm}
  \begin{tabular}{|r@{\,}r@{\,}l@{}r|r@{\,}r@{\,}l@{}r|} \hline
    \multicolumn{3}{|c}{\small\#calls} &\small \#pck
&     \multicolumn{3}{c}{\small\#calls} &\small\#pck \\\hline
\tt 0 &--& \tt 10      & \Bina  & \tt 1K &--&\tt 100K  & \Bine\\
\tt 11 &--& \tt 100    & \Binb  & \tt 100K &--&\tt 1M  & \Binf\\
\tt 101 &--& \tt 1K    & \Binc  & \tt 1M &--&\tt 10M   & \Bing\\
\tt 1K &--& \tt 10K    & \Bind  & \tt 10M &--& \tt 100M & \Binh\\\hline
\end{tabular}
\caption{Call frequency}\label{freq}
\end{minipage}
&
\begin{minipage}{7cm}
\begin{tabular}{|@{\,}r@{\,}|@{\,}rrrr@{\,}|}\hline
  &\eval & \c{evalq} & \c{eval} & \c{local}\\[-2mm]
           & & & \c{.parent} &\\\hline
\small Static sites &\Staticeval&\Staticevalq&\Staticevalparent&\Staticlocal \\
\small Exercised sites&\Triggeredeval&\Triggeredevalq&\Triggeredevalparent&\Triggeredlocal\\
\small Invocations&\EvalsRnd&\EvalqsRnd&\EparentsRnd&\LocalsRnd\\\hline
\end{tabular}~\\[2mm]\caption{Variants}\label{tab:variantseval}
\end{minipage}
\end{tabular}
\end{figure}



\begin{wrapfigure}{r}{5.8cm}
\vspace{-4mm}
\centering
  \begin{tabular}{|r@{\,}r@{\,}l@{\,}r|r@{\,}r@{\,}l@{}r|} \hline
\multicolumn{3}{|c}{\small\#calls} &\small\#sites &
\multicolumn{3}{c}{\small\#calls} &\small\#sites \\\hline
\tt 0 &--& \tt 50    & \Runbina & \tt 501 &--& \tt 1000   & \Runbine\\
\tt 51 &--& \tt 100  & \Runbinb & \tt 1001 &--& \tt 1500  & \Runbinf\\
\tt 101 &--& \tt 250 & \Runbinc & \tt 1501 &--& \tt 2000  & \Runbing\\
\tt 251 &--& \tt 500 & \Runbind & \tt 2001 &--& \tt 3000 & \Runbinh\\\hline
\end{tabular}

\medskip  (a) All

  \includegraphics[width=5.8cm, trim=7.5cm 0 6cm 0, clip]{calls_per_run_per_call_site}

  (b) Small

\vspace{-2mm}
    \caption{Normalized invocations} \label{cn}
\vspace{-2mm}
\end{wrapfigure}


Figure~\ref{tab:variantseval} illustrates the use of variants of
\eval. For each of the four variants, the first rows shows the number
of call sites occurring in the corpus (\emph{static}), the second row
is the number of call sites that were encountered during tracing
(\emph{exercised}), the last row is the number of calls
(\emph{invocations}). The vast majority of call sites, and calls, are
to \eval itself, \c{eval.parent} is rare, as for \c{evalq} and
\c{local} are barely used at all. The difference between static and
exercised reflects spotty code coverage and is a limitation
of dynamic analysis.

Figure~\ref{cn}(a) shows normalized invocation counts. The column on
the left is the average number of calls from a particular site for any
given run (\emph{\#calls}). On the right is the count of sites that
fall in that range (\emph{\#sites}). For instance, \Runbinh sites are
invoked on average over 2,001 times per run. Larger numbers suggest
uses from within a loop or a recursive context -- this seems to be the
exception as most \evals, \Runbina, are exercised 50 times or less.
Figure~\ref{cn}(b) zooms in on low frequency call sites. The x-axis
shows normalized invocations and the y-axis is the number sites for
that value. Most low-frequency \evals are invoked only once, about
half as many are invoked twice; after that the frequency quickly
drops.

Arguments to \eval can be either code or values, in the latter case
\eval has no effect. Code accounts for \Codepercent of arguments in
our corpus. More specifically, \Symbolpercent of arguments are
\c{symbol}s (i.e. these are single variables such as \c{x}),
\Languagepercent are \c{language} object (an internal R type roughly
corresponding to a single expression, such as \c{x+1}), and
\Expressionpercent are \c{expression} objects (multiple statements).
Further inspection reveals that most symbols, \Ggplotsymbolpercent to
be exact, come from a single call in the \c{ggplot2} package and have
have the value \c{_inherit}. Even if we remove this outlier, the
majority of \evals are simple expressions.

To estimate how much executable code is dynamically injected into the
program through \eval, we measure the size of arguments in number of
abstract syntax tree nodes as returned by R's parser; for example,
\c{x+1} counts as 3 nodes.\footnote{We experimented with measuring
code size by de-parsing the expressions to strings, but these
measurements were dominated by de-parsed data objects, these could
range in the MBs.} The median argument size is \Medianszeval (recall
the large number of symbols) and the average is \Avgszeval nodes. The
largest \eval input observed is \Maxszeval\,-- a significant chunk of
code. Figure~\ref{fig:sizedistribution} shows the distribution of
sizes for arguments of fewer or equal to 25 nodes. The x-axis is the
size of arguments in number of nodes, and the y-axis is the count of
arguments with that size. The size drops rapidly, with few
observations larger than 15 nodes. The long tail is omitted from the
graph for legibility.

\begin{figure}[h!]
 \includegraphics[width=1.061\textwidth]{size_loaded_distribution}
\caption{Loaded code} \label{fig:sizedistribution}
\end{figure}

To measure the work performed in \evals, we count the instructions
executed by interpreter during evaluation of the argument. In our
corpus, \EvalEventAllPerc of all instructions (\EventsRnd) are
performed within \eval. Most invocations perform relatively little
work with \Smalleventspct of \evals executing 50 or fewer
instructions. The violin plot of Figure~\ref{ev}(a) corresponds to
\evals executing $\leq$ 50 instructions, it is dominated by trivial
symbol look ups. Figure~\ref{ev}(b) has the work-intensive \evals
which go all the way to \MaxeventsRnd instructions.

\begin{figure}[h!]
\begin{tabular}{@{}c@{}c@{}}
\begin{minipage}{7.5cm}
 \includegraphics[width=\textwidth]{events_per_pack_small}
\end{minipage}&\begin{minipage}{7.5cm}
  \includegraphics[width=\textwidth]{events_per_pack_large}
\end{minipage}\\[-3mm]
\small (a) Small & \small (b) Large
\end{tabular}
 \caption{Events per packages} \label{ev}
\end{figure}

\subsection{Things to do}

\begin{itemize}
\item Figure 5(b) -- make `count` and `average` smaller.
\item Figure 6: (1) Replace `ast-nodes` with `nodes`
\item Figure 6: (2) Remove gray lines
\item Figure 6: increase font size
\item Figure 6: Try to make the figure less wide. Thinner bars/aspect
  ratio change.
\item Figure 7: check that element counts are correct
\item Figure 7: replace events by instructions. Check what we are
  counting? Are these bytecodes or AST nodes.
\item Check that the percentage of operations performed within
  an \eval (5\%) is correct. That feels surprising to me.
\item I am having second thoughts about keeping the \c{ggplot2} data
  -- it is a big outlier that skews all the graphs...
\item Add a ``Discussion'' paragraph at the end that sums up the
  observations
\item Add paragraphs for Kaggle and CORE -- only discuss the
  differences from CRAN
\end{itemize}


\section{A Taxonomy of Eval}

The previous section gave a quantitative view of the usage of \eval;
in this section we try to elucidate \emph{what} it does.

\mypara{Side-effects.} XXX026 total number of reads performed by
\eval. \NOTE{What is the definition of a read? An env lookup?} XXX027
average number of reads. XXX028 total number of writes performed by
\eval. Average number XXX029.

\NOTE{Aviral wrote''measure number of reads and writes by eval outside
  of its bubble'' --- what is the bubble here?}

XXX029 reads to caller environment -- where caller is the caller of
\eval.

XXX030 reads to other caller environment -- i.e. envs that are on the
call stack.

XXX031 reads to caller environment -- where caller is the caller of
\eval.

XXX032 reads to other caller environment -- i.e. envs that are on the
call stack.

Categorize side-effects between env passed to eval and other envs:
\begin{itemize}
\item ephemeral -- read to envs that are created during \eval  XXX033
\item local -- reads to env of the function in which the call to \eval
  lexically occurred; XXX034
\item parent -- reads to parent envs of the local one. XXX035
\item call-stack -- reads to envs that are on the call stack but not
  caller. XXX035.
\end{itemize}

In the corpus we observe \AllWritesRnd writes to variables of which
\EvalWritesRnd writes happen inside \eval. However, all writes are not
dangerous. Only writes to environments not local to the computation
spawned by \eval are side-effecting. These writes outlive the
computation and hence are visible outside it. The remaining writes are
local to the computation. We observe that \EvalSideEffectingWritesRnd
writes inside evals are side-effecting. This is only
\EvalSideEffectingWritesEvalPerc of all variable writes inside \eval
and \EvalSideEffectingWritesAllPerc of all variable writes in the
corpus.

An \eval is considered side-effecting if it performs a side-effecting
write to a variable, directly or indirectly. Only
\SideEffectingCoreCallPerc \eval calls in Core R are side-effecting
and \SideEffectingPackageCallPerc \eval calls in CRAN packages are
side-effecting.


\mypara{New Bindings.} Find number of times, eval introduces new
bindings. There are many ways -- library, load, attach, source, and
explicitly introduce bindings using super-assign, assign and define.

\mypara{C Calls.} Find number of times, eval makes call to native code
using - .Call, .External, etc. Find out if there are native functions
that are called only from within eval.

\mypara{Non-local returns}
Eval can do non-local returns effective bypassing evaluation of the
rest of the function. This can be useful for static analyzers.

\mypara{Purity.}
Conclude with number of evals that are ``pure'', i.e. evals which
could be ignored by a static analyzer without any problem.

\mypara{Source.}
There are various ways to obtain expressions:
\begin{itemize}
  \item \c{substitute} synthesizes ASTs from expressions by replacing
    symbols with their bindings in the specified environment.
  \item \c{expression}  creates a vector of expression
    objects from text.
  \item \c{parse}, \c{str2expression} and \c{str2lang} turn strings into
    expressions.
\end{itemize}

We observe 5.2\% cases where \eval directly evaluates the output of
\c{substitute}, 0.7\% cases where output of \c{parse} is read directly
and only 704 cases for \c{expression}. Most expressions consumed by
\eval are generated by other functions.

\c{eval(parse(...))} can be used for dynamic code loading. This forms
the core of \c{source} and \c{sys.source} functions in R that are
commonly used for loading code in R files in interactive settings. We
investigated the number of cases in which the output of \c{parse} and
its variants is passed to \eval, directly or transitively by tainting
their output. This corresponds to \PercentParsedCallSites of the total
\eval call sites and \PercentParsedEvals of the eval calls. We
observed that very few of the eval call (\NbParseFilesRnd in total)
consume the result of calling \c{parse} on a file. Most of the eval
calls consume the result of calling \c{parse} on a string. We also
identified one function in core R, \c{invokeRestartInteractively} that
prompts the user for input, parses it, and passes it to \eval.


\mypara{Scope.} The environment argument can be \c{NULL}, a \c{List}
or a data frame. This happens in 3.5\% of the cases: \eval copies the
fields of the list or data frame and creates bindings for them in a
new environment. This pattern is used to evaluate formulas which can
directly refer to the fields of the data. The \c{envir} argument can
also be a number $n$. It means that the environment in which the
expressions is evaluated will be the result of \c{sys.call(n)} where
$n$ refers to the $n$-th stack frame.

The top-level environment in R is called the global environment. New
environments can be created using \c{new.env}. They can be provided a
parent environment which becomes the enclosing scope of the new
environment.

We looked at the environments passed to all the \eval calls in our
corpus. Table~\ref{tab:environments} summarizes the results. A numeric
environment class \c{n} denotes the environment of the $n$-th call
stack frame from the current function. \c{global} denotes the
top-level environment and \c{list} denotes a list passed for
evaluation of formulas. Environment classes of the form $n+$ denote
the $n$-th environment extended with a new environment. The new
environment provides a limited form of sandboxing. All assignments
using the \c{\<-} function occur inside it and prevent the extended
environment from mutation. However, it is still possible to mutate the
extended environment using the \c{\<\<-} or \c{assign} functions; but,
that happens rarely.

\begin{table}[htbp]{ \centering
\begin{tabular}{c|c|c|c}\hline
\multicolumn{2}{c|}{Core} & \multicolumn{2}{|c}{Packages}  \\\hline
Environment Class & Eval Calls \% & Environment Class & Eval Calls\\\hline
\CoreEnvClassA & \CoreProportionA & \PackagesEnvClassA &  \PackagesProportionA\\
\CoreEnvClassB & \CoreProportionB & \PackagesEnvClassB &  \PackagesProportionB\\
\CoreEnvClassC & \CoreProportionC & \PackagesEnvClassC &  \PackagesProportionC\\
\CoreEnvClassD & \CoreProportionD & \PackagesEnvClassD &  \PackagesProportionD\\
\CoreEnvClassE & \CoreProportionE & \PackagesEnvClassE &  \PackagesProportionE\\\hline
\end{tabular}} \label{tab:environments}
\caption{Environments in terms of \eval calls}
\end{table}

We observe that a disproportionately high number of core R \eval calls
access the caller's caller's environment. This is because many core R
functions call functions that pass the result of \c{parent.frame()} to
an \eval. A disproportionately high number of calls to \eval happens
in an extended top-level environment. This can be explained by the
fact that many packages evaluate code passed from the user's workspace
in the top-level environment to access the bindings.


The \c{imchange} function of package \c{imager} makes it possible to
modify images using a dedicated formula syntax using
\c{\~}.\footnote{Inspired by {map} in package \emph{purr}.} Here,
\eval is evaluated in \c{newenv}, which creates a new environment that
inherits from \c{parent.frame()} by default (classified as 1+).


\begin{lstlisting}
newenv <- new.env()
...
fo <- parse(text=as.character(fo)[2])
im[where] <- eval(fo,envir=newenv,enclos=env)
\end{lstlisting}

\c{adjCoef} in package \emph{actuar} find the root of an equation
defined by a function \c{h} whose arguments must be named \c{x} and
\c{y}. \c{h} is transformed into an auxiliary function \c{h2} that can
be optimized. Here, the list used for \c{envir} ensures the
correspondance between the textual arguments of \c{h} and the
arguments of \c{h2}.


\begin{lstlisting}
sh <- substitute(h)
fcall <- paste(sh, "(x, y)")
...
h2 <- function(x, y)
    eval(parse(text = fcall),
    envir = list(x = x, y = y),
    enclos = parent.frame(2))
\end{lstlisting}


\section{Case Studies}

\NOTE{5(?) examples from real code?}

\subsection{LEGACY TEXT}

We looked at the top ten expressions passed to core and package \eval
calls. The most frequent ten expressions to eval calls from core R
contribute to 85\% of all \eval calls.

\begin{table}[!h] \centering
\begin{tabular}{@{}l|rr@{}} \hline
Expression & Eval Call &  \% \\\hline
\c{c("auto", "shell", "radix")} & 1,987,105 & 29\%\\
\c{c("auto", "shell", "quick", "radix")} & 1,593,169  & 23\%\\
\c{\{info <- loadingNamespaceInfo(...} & 1,008,632 &       14\%\\
\c{c("onLoad", "attach", "detach", "onUnload")}   & 470,566 &      6.9\%\\
\c{c("append", "prepend", "replace")} &              261,587&       3.9\% \\
\c{c("left", "right", "centre", "none")} & 162,086     & 2.4\%\\
\c{c("no", "ifany", "always")}   &                71,580 &       1.1\%\\
\c{c("pearson", "kendall", "spearman")}  & 72,962 &      1.1\%\\
\c{NULL}& 75,330  &      1.1\% \\
\c{Symbol}&                 66,279&       1\%\\\hline
\end{tabular}
\caption{Top ten eval calls in Core}\label{B}
\end{table}

The expression \c{\{info <- loadingNamespaceInfo(...} is added by core
R to a package directory during installation. To load the package,
this code is executed. It creates a namespace for the package, injects
the package bindings, and attaches the namespace to the program search
path. The \c{NULL} comes from a call to \c{substitute(subset)} in
\c{stats::model.frame.default} function which has a default value of
\c{subset} as \c{NULL}. The \c{Symbol} arises from a call to
\c{as.name} in \c{base::str} function that returns a symbol that is
looked up by evaluating it in a specific environment. The remaining
cases arise from calls to \c{match.arg} which is used to look up the
default choices for a variable and match against the choice passed by
the caller.

The most frequent ten expressions to eval calls from CRAN packages
contribute to 77.1\% of all \eval calls.

\begin{table}[!h]  \centering
\begin{tabular}{@{}l@{~}|@{~}r@{~}r@{}} \hline
Expression & Eval Call &  \% \\\hline
\c{Environment} &                                  989302   & 61\%\\
\c{column[rows] <<- what} &                        55677    & 3.5\%\\
\c{function(value) freduce(value, `_function_list`)} & 37251& 2.3\%\\
\c{NULL} &                         32005    & 2\%\\
\c{List} &                         22293    & 1.4\%\\
\c{c("default", "default2012", "default2011" ...}& 20610    & 1.3\%\\
\c{force(..1)}            &                        20461    & 1.3\%\\
\c{alist(`_spec`)}       &                         18532    & 1.2\%\\
\c{inner}               &                          18530    & 1.2\%\\
\c{String Vector}      &                           17487     & 1.1\%\\
\end{tabular}\caption{Top ten eval calls in CRAN} \label{C}
\end{table}

The expression \c{Environment} occurs because of the four callsites
explained above, \c{ggplot2::ggproto}, \c{R6::generator_funs},
\c{future::backtrace} and \c{RModel::str.RMmodel}. The next
expression, \c{column[rows] <<- what}, is used inside the
\c{plyr::rbind.fill} function to merge data frames by assigning
concatenated vectors to rows. The \c{<<-} operator is interesting in
that it skips the current scope and assigns in a parent scope in which
the variable is already present. In our corpus, all these \eval calls
contribute to a single side-effect. The expression \c{function(value)
  freduce(value, `_function_list`)} arises from the
\c{magrittr::\%>\%} function which is a pipe operator that pipes the
output of previous command to the next one. The expression is
evaluated in a custom environment to create a function binding for
evaluating the components of the pipe.\c{String Vector} and \c{List}
also arise from the same function when a string or a list is piped
using the \c{\%>\%} function into the next expression. The \c{NULL}
arises from \c{R6::generator_funs} function when the \eval is passed a
\c{NULL} argument by the \c{DataMask_generator} package. The
\c{c("default", "default2012", "default2011" ...} pattern arises from
\c{copula::polyG} where it reflectively access the default expression
for its formal parameter and evaluates it. The \c{force(..1)} and
\c{alist(`_spec`)} patterns occur in \c{glue::glue_data} function
which concatenates and interpolates strings. The two patterns occur
because the function captures unevaluated unnamed arguments and maps
the evaluation of \c{force(..1)} on them. The \c{force} function
forces promises and returns the result of evaluation. The \c{inner}
pattern arises from \c{glue::identity_transformer} which enables the
creation of custom transformation functions for affecting the
interpolation and concatenation of input by the \c{glue} package.


\section{Why do we Eval}

\subsection{Discussion}

Yeah, why?

\subsection{Can we do without?}


We look at how \emph{consistent} the \c{expr} argument of \eval can
be, \ie how many different types of the resolved \c{expr} there are
per call sites. Most of the call sites, \ie \PercentMonomorphic, are
\emph{consistent}, and this is similar to javascript. However, a few
ones are highly \emph{polymorphic} (10 different types). They are the
pipe operators \c{\%>\%}, \c{\%<>\%} and \c{\%\$\%} in package
\emph{magrittr}. It is effectively used to compose functions on their
first argument, which can be of any type.

Similarly to JavaScript, there are also unnecessary uses of \eval. For
example, the \c{PerformanceAnalytics} package contains a function
\c{chart.QQPlot} that uses \eval to resolve a string into function and
another to call it and assign its results into a variable:
\begin{lstlisting}
function (R, d="norm", dp, ...) {
q.f <- eval(parse(text=paste("q",d,sep="")))
z <- NULL
eval(parse(text=paste("z<-q.f(",dp,",...)")))
}
\end{lstlisting}
  In both cases, there is no need for \eval:
\begin{lstlisting}
function (R, d="norm", dp, ...) {
q.f <- get(paste0("q",d))
z <- q.f(dp, ...)
}
\end{lstlisting}
  or even to a oneliner \c{do.call(paste0("q",d), as.list(dp, ...))}.

\subsection{Comparison with Javascript}



\section{Conclusion}



\bibliography{bib/bibliography,bib/jv}

\end{document}
