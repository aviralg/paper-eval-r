\documentclass[screen,acmsmall]{acmart}
\settopmatter{printfolios=true,printccs=true,printacmref=true}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}   %% For author/year citations
\usepackage{listings,hyperref,multirow,paralist,xspace,url,wrapfig}
\usepackage{tabularx}
\input{macro}
\newcommand{\mypara}[1]{\medskip\noindent\emph{#1}\xspace}
\newcommand{\NOTE}[1]{{\it Note: #1}\xspace}

\begin{document}
\title{What we {\sf eval} in the shadows}
\subtitle{A large-scale study of {\sf eval} usage in R programs}

%% Aviral Goel
%% Pierre Donat-Bouillud
%% Filip Krikava
%% Jan Vitek
%% What is the right order?

\author{Author }
\authornote{Both authors contributed equally to this research.}
\email{a@b.com}
\orcid{}
\author{Author}
\authornotemark[1]
\email{r@s.com}
\affiliation{\institution{Institute}}

\begin{abstract}
  \noindent Most dynamic languages allow users to turn text into code
  using various functions, often named \eval, with language-dependent
  semantics. The widespread use of these reflective functions hinders
  static analysis and prevents compilers from performing
  optimizations. This paper aims to provide a better sense of why
  programmers use \eval. Understanding why \eval is used in practice
  is key to finding ways to mitigate its negative impact. We have
  reasons to believe that reflective feature usage is language and
  application domain specific; we focus on data science code written
  in R, and compare our results to previous work that analyzed web
  programming in JavaScript. Dynamic analysis of a corpus of
  \CranAllPackages libraries and \KaggleCode end-user code confirms that
  \eval is indeed in widespread use; R's \eval is more dangerous in
  some ways, and safer in others, than what was previously reported
  for JavaScript.
  %% TODO ::: Add a sentence about summarizing why users resort to
  %% eval -- Jan
\end{abstract}

\maketitle

\section{Introduction}

Most dynamic languages provide their users with a facility to
transform unstructured text into executable code and evaluate that
code. We refer to this reflective facility as \eval bowing to its
origins in LISP, all the way back in 1956~\cite{lisp}. \Eval has been
much maligned over the years. In computing lore, it is as close to a
boogeyman as it gets. Yet, for McCarthy, \eval was simply the way to
write down the definition of LISP, he was surprised that someone coded
it up and offered it to end users. Since then, reflective facilities
have been used to parameterize programs over code patterns that can be
provided after the program is written. The presence of such a feature
in a language is a hallmark of dynamism; it is a form of delayed
binding as the behavior of any particular call to \eval will only be
known when the program is run and that particular call site is
evaluated.

\vspace{2mm}\noindent\emph{Trouble in Paradise.} Reflective facilities
hinder most attempts to reason about, or apply meaning-preserving
transformation to, the code using them. In practice, \eval causes
static analysis techniques to loose so much precision as to become
pointless. For compilers, anything but the most trivial, local,
optimizations are unsound after a use of \eval. Furthermore, the
addition of arbitrary code --- code that could have been obtained from
a network connection --- as a program is running is a security
vulnerability waiting to happen. To illustrate these challenges,
consider the interaction of a static analysis tool with a dynamic
language. An abstract interpretation-based program analyzer computes
an over-approximation of the set of possible behaviors exhibited by
the program under study~\cite{cc77}. A reflective may have \emph{any}
behavior that can be expressed in the target language; i.e. \eval can
be replaced by any legal sequence of instructions. As dynamic
languages tend to be permissive, the analysis has to, for example,
assume that many (or all) functions in scope may have been redefined,
e.g. that \texttt{`+`} now opens a network connection or something
equally surprising. A single occurrence of \eval causes the static
analyzer to loose all information about program state and
meaning of identifiers. This loss of precision can sometimes be
mitigated by analyzing the string argument~\cite{moller03} to bound its
possible behavior but when the string comes from outside the program
not much can be done. A frustrated group researchers argued giving up
on soundness and, instead, under-approximating dynamic features
(soundiness)~\cite{soundy}. In their words ``a practical analysis,
therefore, may pretend that \eval does nothing, unless it can
precisely resolve its string argument at compile time.'' Alas,
assuming that \eval does not have side-effects, or that side-effects
will not affect the results of the analysis, may be unduly optimistic.

\vspace{2mm}\noindent\emph{Is Past Prologue?} Previous work investigated how
\eval is used in web programming, specifically in websites that use
JavaScript~\cite{pldi10a}. In 2010, 17 of the largest website used the feature.
In 2011, 82\% of the 10,000 most accessed sites used \eval~\cite{ecoop11}. Yet,
the strings passed to \eval, and their behaviors when executed, are far from
random; it was shown that when one can observe several calls to \eval, the
``shape'' of future calls can be predicted with 97\% accuracy~\cite{oopsla12b}.
Overall, practical usage suggested that most reflective calls were relatively
harmless. While this backs up the soundiness squad's approach, does it
generalize to other application domains than web programming and to other
languages?

\vspace{2mm}\noindent\emph{The Here and Now.} In this study, we investigate the
usage of \eval in programs written in the R programming language. R is language
designed by statisticians for applications in data science~\cite{r,R96}. What
makes looking at R after JavaScript interesting is that, while both languages
are dynamic, they are quite different. While one can program in an
object-oriented style in R, like in JavaScript, R is primarily a lazy, untyped,
functional language. JavaScript was designed to run untrusted code in browser,
while R is used for statistical computing on desktops. JavaScript is a general
purpose language used by a wide community of programmers; while R is used for
scientific computing by data scientists and domain experts with, often, limited
programming experience. One can distinguish between library implementers,
developers with some programming experience and a working knowledge of R, and
end-users, who are typically not expert programmers and often have only a
cursory knowledge of the language.\footnote{Consider that R evaluates function
argument lazily, just like Haskell. We informally surveyed end-users, including
computer scientists, and did not find a single user aware of this fact. Library
developers, on the other hand, know about laziness and program defensively
around it.} Our goal is thus to highlight the differences in usage between
JavaScript and R, and try to explain those differences in terms of language
features, application domain and programmer experience. Hopefully some of our
observations will generalize to other languages.

\vspace{2mm}\noindent\emph{The What and How.} One significant benefit of
choosing R is that every package in the CRAN repository is curated and comes
with examples of typical usage. This gives us a large code base that we can
analyze dynamically. To observe \eval we built a two-level monitoring
infrastructure:\footnote{Our infrastructure is open source and publicly
available, our anonymized code is at {\tt http://url.com}, a more complete
artifact will be submitted to the artifact evaluation committee. } we can
monitor R programs by instrumentation --- this gives us access to many
user-visible properties of R programs --- but we can also monitor the
inner-workings of the R interpreter --- this allows us to capture details not
exposed at the source level. Dynamic analysis is limited, it can only observe
behaviors triggered by the particular inputs passed to a program. Luckily, CRAN
libraries come with many tests and use-cases. The choice of corpus is crucial.
Our corpus has been constructed to reflect the levels of sophistication of the R
community. We distinguish between \emph{CRAN packages} (\CranAllPackages curated
packages that pass stringent quality checks and are equipped with tests and
sample data) and \emph{Kaggle scripts} (\KaggleUnique end-user written programs
that performs a particular data analysis task). It is reasonable to expect that
\eval usage differ between these datasets: the libraries represent a lively
ecosystem with new libraries added each day, while end user code is often thrown
together, run once, and never revisited.


\vspace{2mm}\noindent\emph{Why do we Eval?} {\bf A short summary of the
results should go here.}


\section{Background and Previous work}

This section provides a short introduction to R as it has a few
surprising features that impact the use of reflective features of the
language. We then look at the semantics of \eval in R and discuss some
design choices. Lastly, we put this paper in context of previous work.

\subsection{R, Briefly}

\citet{ecoop12} gave a programming language-centric overview of the R language.
They characterized it as a lazy, vectorized, functional language with rich
dynamic features that allow layering several object systems. Most data types are
sequences of primitive values constructed by calling the combination function
\c{c}, so that \c{c(1,2)} evaluates to a vector of two strings, and constants
such as \c{42} are vectors of length one. To enable equational reasoning, values
accessible through multiple aliases are copied when written to. Furthermore,
values can be tagged by user-defined attributes, key-value pairs. For instance,
the attribute \c{dim}-\c{c(2,2)} can be attached to the value at \c{x} by the
call \c{attr(x,"dim")$\leftarrow$c(2,2)}. In this case, the addition of this
attribute ensure that the vector \c x will be treated as a $2 \times 2$ matrix
by all arithmetic operations. The \c{class} attribute is used to give a 'class',
in the object-oriented sense, to values. So, \c{class(x)<-"human"} sets the
class of \c{x} to \c{human}; classes are used for method dispatch. Every
linguistic construct is desugared to a function call, even control flow
statements, assignments, and bracketing. All functions can be shadowed or
redefined, making the language at the same time remarkably flexible and
exceeding challenging to compile statically as vividly detailed
by~\citet{dls19}. R uses a relaxed call-by-need convention for passing arguments
to functions. Each argument is a thunk composed of an expression, its
environment, and a slot for the result, these are called \emph{promises}. To get
the value of an argument, the corresponding promise must be forced, once forced
the promise's result is cached for future use.


\subsection{On the Expressive Power of Eval}

While a facility to transform data into code is available in many languages,
some design choices affect the expressive power of \eval and related functions.
The key design choices that we review here are the input format, the environment
in which \eval-generated code is evaluated and the reflective operations that
are available to that code. Fig.~\ref{comp} summarizes design choices for
several popular languages.

\begin{figure}[!t]\center\small
\begin{tabular}{r@{~}l|l|l|l}\hline
\tiny\sc Language&&\sc\tiny Input&\sc\tiny Scope&\tiny\sc Reflective operations\\\hline
\bf Julia&\cite{julia}     & expression& toplevel         & data\\
\bf Java&\cite{cl}  & bytecode  & classloader       & data\\
\bf JavaScript&\cite{ecoop11}& text      & current, toplevel& data\\
\bf R&\cite{R96}  & expression& programmatic      & data, stack, envir.\\\hline
\end{tabular}
\caption{Design space of \eval}\label{comp}
\end{figure}

The input to \eval can be any format that can be turned into code. Both Julia
and R mandate that the input format be parsed expressions. JavaScript allows any
expression as a character string. Java is the most restrictive as a classloader
only accepts complete classes in the bytecode format recognized by the virtual
machine.

The environment in which \eval executes the code can be the lexically enclosing
environment of \eval (current), the global environment (toplevel), a limited
global environment (classloader), or an environment chosen by the caller of
\eval (programmatic). The most restrictive variant is that of Java, where the
scope of newly loaded code is the set of classes exposed by the current class
loader. Julia is the next most restrictive, as only global definitions can be
observed and modified by \eval. JavaScript has a variant of \eval that runs in
the current scope and a most strict mode that is restricted to global variables.
Finally, R allows the caller to choose which environment to use with \eval.

The last degree of freedom in the design of \eval is what can be done from
within the generated code. The main difference between designs lies in what part
of the system state can be accessed and modified. In R, there is very little
that cannot be observed from an \eval context and modified. The code can
override the definition of variables and functions. Also, thanks to a rich
reflective interface, code can introspect on values, walk and modify environment
chains, and access the stack frames on the call stacks and their environments.

Given the above, it is reasonable to claim that R's \eval are amongst the
languages that afford the facility the most expressive power. This comes at the
cost of the programmer's ability to reason about code in the presence of \eval.
Expressive power translates directly into limits on the usefulness of program
analysis.

By contrast, the designers of Julia chose to limit \eval. In Julia, only global
variables can be side-effected and environments cannot be readily manipulated.
This is designed to shield optimized code from some of the most pernicious uses
of the facility~\cite{oopsla18a}. Furthermore, the language provides a
versioning mechanism, called world age, to ensure that any methods defined
within an \eval only become visible at well-defined program points and thus that
recompilation does not have to occur when optimized code is
running~\cite{oopsla20a}.

\subsection{Eval in R}

The R language exposes a rich reflective interface with four functions with
different semantics, \c{eval}, \c{evalq}, \c{eval.parent}, and
\c{local}. The first is the most general, taking three arguments, an expression,
its environment and an enclosing environment. The following discussion
simplifies details not relevant to this paper, the interested reader should
consult \citet{hadley}. It is defined as:

\begin{lstlisting}
 eval <- function(e, envir = parent.frame(),
                   encl = if(is.list(envir)) parent.frame() else baseenv()) ...
\end{lstlisting}
Parameter \c e is the expression to evaluate, \c{envir} is the evaluation
environment, and \c{encl} is used to look up variables not found in \c{envir}.
The latter is, by default, either the environment of the function that called
\eval or the top level. Parameter \c e can be of different types, for our
purposes we focus on {\tt expression}s. These can be thought of as abstract
syntax trees returned by the parser. An \c{expression} can be obtained by
calling \c{quote} with an expression or \c{parse} wit a string:
\begin{lstlisting}
 > quote(a + b)
 # a + b
 > parse(text="a+b")
 # expression(a+b)
\end{lstlisting}
Even more commonly, expressions are obtained from promises using \c{substitute}.
Consider a one-parameter function \c{f}, when called with \c{a+b}, the function
returns \c{a+b} as expression.
\begin{lstlisting}
 > f <- function(x) substitute(x)
 > f(a+b)
 # expression(a+b)
 > substitute(a+b,list(a=1,b=2))
 # 1+2
\end{lstlisting}
The call to \c{substitute(x)} extracts the unevaluated expression from the
promise bound to variable \c x. Function \c{substitute(e,envir)} takes two
arguments, the second is used as a substitution environment for free variables
in \c e.
\begin{lstlisting}
 > environment()
 # <environment: R_GlobalEnv>
\end{lstlisting}
The \c{environment} function returns a variable environment, either from the
currently executing function or the top level environment. Environments nest,
each has a parent. When creating a new environment with \c{new.env}, the parent
is the current environment. Environment chains can be traversed with
\c{parent.env}, until \c{emptyenv} is reached. The top level environment is
\c{.GlobalEnv}, it has parents that represent the packages that have been
loaded. One can also directly read, modify or create new bindings, given any
environment:
\begin{itemize}
\item \c{envir\$v} and \c{get("v",envir=envir)}: read  \c{v} from \c{envir};
\item \c{envir\$v<-2} and \c{assign("v",2,envir=envir)}: store 2 in \c{v}, if
  not found, \c{v} is created in \c{envir}.
\end{itemize}
\noindent Programmers sometimes use environments as hash maps as they are the
only data type with reference semantics and have a built-in string look up.
Functions \c{parent.frame} and \c{sys.frame} return environments further up the
call stack.
 \begin{lstlisting}
  evalq <- function(e,envir,encl) eval(quote(e),envir,encl)
  eval.parent <- function(e,n) eval(e,parent.frame(n))
  local <- function(e,envir,encl) evalq(e,new.env())
\end{lstlisting}
The three \eval variants can be expressed with \eval. The \c{evalq} form quotes
the argument to prevent it from being evaluated in the current environment. The
\c{eval.parent(e,n)} form evaluates \c{e} in the environment of the {\tt n}-th
caller of this function. Finally, \c{local} evaluates \c{e} in a new environment
to avoid polluting the current one.



\subsection{Previous Work}

Richards et al.~\cite{ecoop11} provided the first large-scale study of
the runtime behavior of \eval in JavaScript. They dynamically analyzed
a corpus of the 10,000 most popular websites with an instrumented web
browser to gather execution traces. They show that \eval is pervasive
with 82\% of the most popular websites using it. The reasons for its
use include the desire to load code on demand, deserialization of JSON
dataq and lightweight meta-programming to customize web pages. While
many uses were legitimate, just as many were unnecessary and could be
replaced with equivalent and safer code. They categorized inputs to
\eval so as to cover the vast majority of input strings. Restricting
themselves to \eval in which all named variables refer to the global
scope, many patterns could be replaced by more disciplined
code~\cite{oopsla12b, moller12}. The work did not measure code
coverage, so the numbers presented are a lower bound on the possible
behaviors. Furthermore, JavaScript usage in 2011 is likely different
from today, e.g. Node.js was not covered by Richards. More details
about dynamic analysis of JavaScript can be found in~\cite{liang}.

Wang et al.~\cite{wang} analyzed use of dynamic features in 18 Python
programs to find if they affect file change-proneness. Files with
dynamic features are significantly more likely to be the subject of
changes, than other files. Chen et al. looked at the correlation
between code changes and dynamic features, including \eval, in 17
Python programs~\cite{chen}. They did not observe many uses of \eval.
Callau et al.~\cite{oscar} performed an empirical study of the usage
of dynamic features in 1,000 Smalltalk projects. While \eval itself is
not present, Smalltalk has a rich reflective interface. The authors
found that reflective are used in less than 2\% of methods. The most
common reflective method is \c{perform:}; it send a message that is
specified by a string. These features are mostly used in the core
libraries.

Bodden et al.~\cite{bodden} looked at usage of reflection in the Java
DaCapo benchmark suite. They found that dynamic loading was triggered
by the benchmark harness. The harness then executes methods via
reflection, this caused static analysis tools to generate an incorrect
call graph for the programs in DaCapo.

Morandat et al.~\cite{ecoop12} had a short section on the usage of
\eval in R. They found the it widely used in R code with 8500 call
sites in CRAN and 2.2 million dynamic calls. The 15 most frequent call
sites account for 88\% of those. The \c{match.arg} function is the
highest used one with 54\% of all calls. In the other call sites, they
saw two uses cases. The most common is the evaluation of the source
code of a promise retrieved by \c{substitute} in a new environment;
e.g. as done in the \c{with} function. The other use case is the
invocation of a function whose name or arguments are determined
dynamically. For this purpose, R provides \c{do.call} and thus \eval
is overkill.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

This section explains how we selected our corpus and how we obtained the
reported results.

\subsection{Corpus}

Our corpus is assembled from three sources: the \emph{Base} libraries that are
bundled with the language implementation, packages hosted on \emph{CRAN} and
scripts from \emph{Kaggle}.

\mypara{Base.} The base dataset contains 13 core libraries performing basic
arithmetics, statistics, and operating system related functionalities. These
libraries are bundled with R and are executed pervasively.


\mypara{CRAN.} The Comprehensive R Archive
Network\footnote{\url{cran.r-project.org}} is the largest curated repository of
R packages. It hosts \CranAvailablePackagesRnd packages with 6 new one submitted
each day~\cite{Ligges2017}. Packages accepted to CRAN are authored by
experienced developers and abide to a number of well-formedness rules
automatically checked on each commit. Furthermore, each package comes with
sample data. There are three sources of runnable code in a package: \emph{tests,
examples} and \emph{vignettes} -- respectively, unit tests, code snippets from
the documentation, and long-form use-cases written in RMarkdown.
Examples and vignettes can be turned into scripts by extracting the relevant
code. We obtained \CranPackages that call \eval. These packages represent
\CranPackagesRatio of all packages or \CranCodeRatio of the entire code size of
CRAN. From these we extracted \CranRunnableScripts scripts that contain
\CranRunnableCode lines of code (\CranRunnableCodeExamplesRnd in examples,
\CranRunnableCodeVignettesRnd in vignettes and \CranRunnableCodeTestsRnd in
tests).

\mypara{Scripts.} Kaggle is an online platform for data
science.\footnote{\url{kaggle.com}} It allows users to submit problems and
compete to solve them. Solutions, called \emph{kernels}, are uploaded as plain
scripts or notebooks. The quality of code on Kaggle is not uniform. Each kernel
is runnable, and thus there is one script per kernel. Input data is provided. We
obtained \KaggleKernels kernels. Since Kaggle is not curated, used SHA-1 hashes
to identify and remove \KaggleDuplicates duplicate entries. The remaining
\KaggleUnique kernels are unique solutions to \KaggleCompetitions competition.
They contain \KaggleCode lines of R code. Only \KaggleWithEvals kernels call
\eval.

\subsection{Pipeline}

The analysis presented in this has been automated by a pipeline that acquires
packages, extract scripts, executes them, traces their behavior and summarizes
observations. Figure~\ref{fig:pipeline} shows the main steps along with their
running time, data size, and number of elements manipulated. Timings are from a
cluster of three 2.3GHz Intel Xeon 6140 servers, with 72 cores and 256GB of RAM,
and shared OCFS network storage. The pipeline steps are:


\begin{figure}[!h]\hspace{-5mm}
  \includegraphics[width=1.05\linewidth]{pipeline.pdf}
  \caption{Pipeline UPDATE!!}\label{fig:pipeline}
\end{figure}

\medskip
\begin{compactenum}
\item \emph{Download.} Packages are downloaded from CRAN. For scripts, a web
  crawler retrieves code and the Kaggle command line tool gets data.
  Installation is complicated by native dependencies which are not properly
  documented and thus hard to automatically resolve.
\item \emph{Extract.} Given installed packages and scripts, the next step is to
  create runnable programs. The \genthat tool helpfully extracts all runnable
  code snippets from a package and turns each of these into a self-standing
  program~\cite{issta18}. Some Kaggle kernels are already scripts, for those
  nothing more needs be done. Others kernels are packages as notebooks, either
  as Rmarkdown or Jupyter, for those we use \c{knitr} to extract runnable code.
  The body of each extracted program is instrumented with calls our dynamic
  analyzer to ensure that we only record calls to \eval from the code of
  interest and not from bootstrapping or execution harness operations.
\item \emph{Trace.} Each program is executed using our dynamic analysis tool
  which is heavily instrumented interpreter that captures calls to \eval and
  many other fine-grained runtime events. Packages are run twice, once to
  capture \eval calls originating from package code, and a second time to
  capture calls coming from the base libraries. To avoid any interference, each
  program is run in its own process.
\item \emph{Analyze.} Finally, analysis output is merged, cleaned and summarized
  in a post-processing phase driven by series of R scripts. The summarized data
  is then analyzed in RMarkdown notebooks to gather insights. All figures and
  numbers appearing in the paper are generated automatically. Figures are
  produced in PDF by \c{ggplot2}, numbers are exported as \LaTeX macros.
\end{compactenum}

\medskip\noindent
The code extraction and tracing steps of the pipeline are run in
parallel~\cite{GNUparallel}. Servers have identical environment thanks to docker
images with all dependencies installed. The pipeline consists of 3.5K lines of
Make, R and shell scripts.

\subsection{Analysis}

The dynamic analysis is performed by \rdyntrace, a modified R virtual machine
based on GNU R 4.0.2 that exposes low-level callbacks for a variety of runtime
events~\cite{oopsla19a}. As the GNU R bytecode compiler is written in R, we turn
it off to avoid recording \eval in its code, and fallback on the interpreter.
This study requires recording a large number of events, calls to \eval,
side-effects, function entries and exits, environments, etc. One challenge we
encountered during the analysis was that R does not produce source references
for top-level blocks. That is to say, a method whose body would \c{eval(x)}
would not have debug information that we can use to identify that particular
\eval. The same expression surrounded by braces would have a line number and
source reference. This is unfortunately not easily fixed in the R
implementation. Instead, we extended the dynamic analysis tool to add missing
source references heuristically.

The analysis has some limitation. Even with the extension described above, there
are still \CranUndefinedRnd \eval calls without source references. This occurs
when a reference to the \eval function is passed as argument to a higher-order
functions or when the \eval call originates from native code. However, these
missing source references account for a meager \CranUndefinedRatio of all calls;
they are unlikely to affect our results. Other limitations are that we ignore
calls to the native \eval function and to the alternate \c{rlang::tidy\_eval}
function which uses native \eval internally. The \c{rlang} package introduces a
new kind of promise called \c{quosure} which is evaluated by \c{tidy\_eval}.
None of these limitation should invalidate our conclusions.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Usage Metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section focuses on CRAN packages and reports statistics about the usage of
\eval. Calls to \eval appear in {\bf 28\%} of the packages hosted on CRAN. There
are \Corpus packages with an \eval, all programs extracted from these packages
were executed, any run that did not exercise one of the \eval call sites in the
packages was discarded. This left \Nbruns runs of some of the \Corpus packages
to generate the data presented here. There were \Allcalls invocations of \eval
and its variants occurring in \Triggeredpkgs packages.\footnote{Not all packages
with static call sites of \eval were observed triggering that call site, this
can be chalked down to incomplete code coverage and occasional infrastructure
failures of our analysis infrastructure.} Figure~\ref{freq} summarizes the
frequency of dynamic calls to \eval with the left column being the number of
calls and the right, the number of packages in that range. There are \Fewcalls
packages with low \eval frequency, fewer than 100 calls; and a similar number of
packages, \Manycalls to be precise, that use \eval more than 1,000 times. One
package, \Maxcallspack, calls it \Maxcalls times and thus accounts for over half
of the observed calls.

\begin{figure}[!h]

\begin{tabular}{@{}l@{\hspace{1.5cm}}l@{}}
\begin{minipage} {5cm}
  \begin{tabular}{|r@{\,}r@{\,}l@{}r|r@{\,}r@{\,}l@{}r|} \hline
    \multicolumn{3}{|c}{\small\#calls} &\small \#pck
&     \multicolumn{3}{c}{\small\#calls} &\small\#pck \\\hline
\tt 1 &--& \tt 10      & \Bina  & \tt 1K &--&\tt 100K  & \Bine\\
\tt 11 &--& \tt 100    & \Binb  & \tt 100K &--&\tt 1M  & \Binf\\
\tt 101 &--& \tt 1K    & \Binc  & \tt 1M &--&\tt 10M   & \Bing\\
\tt 1K &--& \tt 10K    & \Bind  & \tt 10M &--& \tt 100M & \Binh\\\hline
\end{tabular}
\caption{Call frequency}\label{freq}
\end{minipage}
&
\begin{minipage}{7cm}
\begin{tabular}{|@{\,}r|rrrr|}\hline
  &\eval & \c{evalq} & \c{eval} & \c{local}\\[-2mm]
           & & & \c{.parent} &\\\hline
\small Static sites &\Staticeval&\Staticevalq&\Staticevalparent&\Staticlocal \\
\small Exercised sites&\Triggeredeval&\Triggeredevalq&\Triggeredevalparent&\Triggeredlocal\\
\small Invocations&\EvalsRnd&\EvalqsRnd&\EparentsRnd&\LocalsRnd\\\hline
\end{tabular}~\\[2mm]\caption{Variants}\label{tab:variantseval}
\end{minipage}\end{tabular}\end{figure}

\begin{wrapfigure}{r}{5.8cm}
\vspace{-4mm}
\centering
  \begin{tabular}{|r@{\,}r@{\,}l@{\,}r|r@{\,}r@{\,}l@{}r|} \hline
\multicolumn{3}{|c}{\small\#calls} &\small\#sites &
\multicolumn{3}{c}{\small\#calls} &\small\#sites \\\hline
\tt 0 &--& \tt 50    & \Runbina & \tt 501 &--& \tt 1000   & \Runbine\\
\tt 51 &--& \tt 100  & \Runbinb & \tt 1001 &--& \tt 1500  & \Runbinf\\
\tt 101 &--& \tt 250 & \Runbinc & \tt 1501 &--& \tt 2000  & \Runbing\\
\tt 251 &--& \tt 500 & \Runbind & \tt 2001 &--& \tt 3000 & \Runbinh\\\hline
\end{tabular}

  \medskip  (a) All
  \medskip
  \medskip

  \includegraphics[width=5.4cm, trim=5.5cm 0 5cm 0, clip]{calls_per_run_per_call_site}

  (b) Small

\vspace{-2mm} \caption{Normalized invocations} \label{cn}\vspace{-2mm}
\end{wrapfigure}


Figure~\ref{tab:variantseval} enumerate the use of variants of \eval. For each
of the four variants, the first rows shows the number of call sites in the
corpus (\emph{static}), the second row is the number of call sites that were
encountered during analysis (\emph{exercised}), the last row is the number of
calls (\emph{invocations}). The overwhelming majority of call sites and dynamic
calls, are to \eval itself, \c{eval.parent} is rare, and both \c{evalq} and
\c{local} are barely used at all. The difference between sites and exercised
sites underscores the limitations due to code coverage.

Figure~\ref{cn}(a) shows normalized invocation counts per call site; on the left
are average number of calls from a given site and given run, on the right are
counts of sites that fall in that range. For instance, \Runbinh sites are
invoked on average over 2,000+ times per run. Larger numbers suggest loops or
recursive contexts -- this seems to be the exception as most \evals, \Runbina,
are exercised 50 times or less. Figure~\ref{cn}(b) zooms in on low frequency
call sites. The x-axis shows normalized invocations and the y-axis is the number
sites for that value. Most low-frequency \evals are invoked only once, about
half as many are invoked twice; after that the frequency quickly drops.

The \eval can be passed any value, but if the argument is not an expression,
\eval returns it without doing anything. Expressions account for \Codepercent of
arguments in our corpus. More specifically, \Symbolpercent of arguments are
\c{symbol}s (single variables such as \c{x}), \Languagepercent are \c{language}
object (a single expression, such as \c{x+1}), and \Expressionpercent are
\c{expression} objects (multiple statements). Further inspection reveals that
most symbols, \Ggplotsymbolpercent to be exact, come from a single call in the
\c{ggplot2} package and have have the value \c{\_inherit}. Even if we remove
this outlier, the majority of \evals are simple expressions.

\begin{wrapfigure}{r}{5.8cm}
 \includegraphics[width=0.5\textwidth, trim=4.8cm 0 6cm 0, clip]{size_loaded_distribution}
\caption{Loaded code} \label{fig:sizedistribution}
\end{wrapfigure}

To estimate how much executable code is injected through \eval, we measure the
the number of abstract syntax tree nodes as returned by the parser; for example,
\c{x+1} counts as 3.\footnote{We attempted to measure string lengths after
de-parsing expressions, but these measurements were dominated by de-parsed data
objects, which could range in the MBs.} The median argument size is
\Medianszeval (due to the many symbols) and the average is \Avgszeval nodes. The
largest \eval input observed is \Maxszeval\,-- a significant chunk of code.
Figure~\ref{fig:sizedistribution} shows the distribution of sizes for arguments
of fewer or equal to 25 nodes. The x-axis is the size of arguments in number of
nodes, and the y-axis is the count of arguments with that size. The size drops
rapidly, with few observations larger than 15 nodes. The long tail is omitted
from the graph for legibility.


To estimate the work performed in \evals, we count the instructions executed by
interpreter. Most invocations perform relatively little work with
\Smalleventspct of \evals executing 50 or fewer instructions. The violin plot of
Figure~\ref{ev}(a) corresponds to \evals executing $\leq$ 50 instructions, it is
dominated by trivial symbol look ups. Figure~\ref{ev}(b) has the work-intensive
\evals which go all the way to \MaxeventsRnd instructions.

\begin{figure}[h!]
\begin{tabular}{@{}c@{}c@{}}
\begin{minipage}{7.5cm}
 \includegraphics[width=\textwidth]{events_per_pack_small}
\end{minipage}&\begin{minipage}{7.5cm}
  \includegraphics[width=\textwidth]{events_per_pack_large}
\end{minipage}\\[-3mm]
\small (a) Small & \small (b) Large
\end{tabular}
 \caption{Instructions per \eval} \label{ev}
\end{figure}

\paragraph{Base}
We recorded \baseAllcalls \eval calls in the base libraries with fewer than 4\%
of those being to any variants. Unlike CRAN, Base calls \c{evalq} (\baseEvalqs).
Out of \baseStaticeval call sites, \baseTriggeredevalpct are exercised. Most
arguments, \baseCodepercent, are expressions. However, unlike CRAN,
\baseLanguagepercent of the arguments are language objects. The median argument
size is \baseMedianszeval, which is more than for CRAN. The maximum size is
\baseMaxszeval. Small instruction counts, less than 50, amount for
\baseSmalleventspct of calls.

\paragraph{Kaggle}
Only \kaggleNbruns scripts trigger \eval. In total, \kaggleAllcalls \eval were
recorded; none are calls to variants. Out of \kaggleStaticeval call sites,
\kaggleTriggeredeval were exercised. Most call sites are invoked only once. Only
one site is called \kaggleMaxcalls times. Expressions amount for
\kaggleCodepercent of arguments. Unlike CRAN corpus, there are very few symbols
(\kaggleSymbolpercent). Most expressions result from calls to \c{parse}, thus
most \evals start with strings. The median argument size is \kaggleMedianszeval,
Which makes sense, as few arguments are only symbols. The largest argument is
\kaggleMaxszeval. The distribution of instructions per eval is similar to CRAN;
\kaggleSmalleventspct of \evals execute fewer than 50 instructions.


\paragraph{Discussion}
These results show that \eval is central to the language implementation as it is
omnipresent in the relatively small Base library. Packages, which are developed
by experienced programmers, make regular and varied use of the facility.
Finally, scripts are written by less sophisticated users, and likely perform
simpler tasks, and thus have lesser need for \eval, and its uses originate from
strings which are easier to manipulate for end-users.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Taxonomy of Eval}

The previous section gave a quantitative view of \eval usage; we now try to
elucidate \emph{what} it does.

\subsection{Arguments of \eval}

%%%%%%%% JAN:::: NOT SURE WHAT THIS IS
%% It refers to the \emph{namespace} of a package. A call to
%% \c{library(packageName)}, which load libraries in R, for instance, will populate
%% environment \c{package:packageName}. It is a \emph{base} function though, so we
%% do not see it in the CRAN dataset. However, R still makes it possible to access
%% any package environments and \packageNbPackageEnvSites call sites, \ie
%% \packagePackageEnvSitePercent access a package environment, from
%% \packageNbPackageEnvPackages different packages. Those packages perform
%% introspection, to represent and search any environment by name, in package
%% \emph{envnames}, to containerize tests, as with \emph{testthat} and
%% \emph{unitizer}, to create a custom OOP system with \emph{R.oo}, or to create
%% poor man's hidden functions in \emph{USSCXenaTools}.

%% It is also used to implement a custom caching mechanism in packages
%% \emph{g.data} and \emph{SOAR}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The argument passed to a call to \eval determines what the function will do. As
we have seen in the previous section, there are cases where the argument is a
value, but these are rare. The common cases are that the argument is some form
of expression, either a single variable, a simple expression or a statement.

To categorize the expressions, consider a function $min(e)$ which for a given
expression $e$ returns a normal form that abstracts many incidental details
allowing the reader to focus on the structure.

We classify the resolved expressions to take into account the structure of the
expression passed to \eval. However, we do not want to discriminate expressions
based on specific values. Hence, the expression goes through several
simplification steps: constant folding of arithmetic and string expressions that
use commonly used functions (addition, string concatenation), discarding of
specific value to a symbolic value holder, variable absorption. We keep track of
variables, as they are evaluated in the environment specified to \eval, contrary
to values that are left unchanged. We also track whether assignment operators
are used in the expression or not. After the simplification steps, we obtain 15
different \emph{minimized} expressions, of which we show the eleven first most
frequent in number of sites in Table~\ref{tab:minimizedexpressions}.

\begin{table}[h]\small
\begin{tabular}{|c|r|r|r|r|r|c|}\hline
$min(e)$& \#sites & \%sites & \#packages & \#operations & \%envir & example\\\hline
\c X&\packageMinimizedcallsitesa &\packageMinimizedpropsitesa &\packageMinimizedpackagea &\packageMinimizedoperationsaRnd &\packageMinimizedpercentenvirnonparenta & \c{y+1}\\\hline
\c{F(F())} & \packageMinimizedcallsitesb  & \packageMinimizedpropsitesb & \packageMinimizedpackageb  & \packageMinimizedoperationsbRnd & \packageMinimizedpercentenvirnonparentb & \c{gbov( mean(x), a-1)}\\\hline
\c{V}&\packageMinimizedcallsitesc &\packageMinimizedpropsitesc &\packageMinimizedpackagec &\packageMinimizedoperationscRnd &\packageMinimizedpercentenvirnonparentc&c(42,21,0)\\\hline
\c{F(X)}& \packageMinimizedcallsitesd & \packageMinimizedpropsitesd & \packageMinimizedpackaged & \packageMinimizedoperationsdRnd & \packageMinimizedpercentenvirnonparentd \\\hline
\c{model.frame} & \packageMinimizedcallsitese & \packageMinimizedpropsitese & \packageMinimizedpackagee & \packageMinimizedoperationseRnd & \packageMinimizedpercentenvirnonparente \\\hline
\c{F()}& \packageMinimizedcallsitesf & \packageMinimizedpropsitesf & \packageMinimizedpackagef & \packageMinimizedoperationsfRnd & \packageMinimizedpercentenvirnonparentf \\\hline
\c{\$}& \packageMinimizedcallsitesg & \packageMinimizedpropsitesg & \packageMinimizedpackageg & \packageMinimizedoperationsgRnd & \packageMinimizedpercentenvirnonparentg \\\hline
\c{FUN} & \packageMinimizedcallsitesh & \packageMinimizedpropsitesh & \packageMinimizedpackageh & \packageMinimizedoperationshRnd & \packageMinimizedpercentenvirnonparenth \\\hline
\c{<-} & \packageMinimizedcallsitesi  & \packageMinimizedpropsitesi & \packageMinimizedpackagei & \packageMinimizedoperationsiRnd & \packageMinimizedpercentenvirnonparenti \\\hline
\c{\{\}} & \packageMinimizedcallsitesj & \packageMinimizedpropsitesj & \packageMinimizedpackagej & \packageMinimizedoperationsjRnd & \packageMinimizedpercentenvirnonparentj \\\hline
\c{F(F(<-))} & \packageMinimizedcallsitesk & \packageMinimizedpropsitesk & \packageMinimizedpackagek & \packageMinimizedoperationskRnd & \packageMinimizedpercentenvirnonparentk \\\hline
\end{tabular}
\caption{Minimized expressions: eleven most frequent in number of sites. \#sites
  is the number of sites; \%sites the percentage of sites across the total
  number of sites (\packageNbCallSites sites) with at least this minimized
  expression; \#packages is the number of packages with at least one of the
  minimized expression and measures how widespread a minimized expression is.
  \#operations is the average number of steps performed by the R interpreter
  when given that minimized expression, and \%envir is the percentage of call
  sites for which the \c{envir} argument is explicitly passed, and is not
  \c{parent.frame()}, which is the most common explicit environment passed to
  \eval.} \label{tab:minimizedexpressions}
\end{table}


Note that a site can exhibit several minimized expressions (see
Figure~\ref{fig:frequentcombinationminimized}). However,
\packageNbOneMinimizedPercent of the sites have only one minimized expression.
Note also that if a function is called in that expression, we see only the
function call but not the body of the function. We will trace what happens
during the evaluation of the expression by \eval in
Section~\ref{sec:dangerseval}, especially about side effects performed in \eval.


\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{package_combination_minimized.pdf}
	\caption{Most frequent combinations of minimized expressions per site,
          with a cumulative share of 95\% of the
          sites.} \label{fig:frequentcombinationminimized}
\end{figure}

\paragraph{Variables.}
The most common minimized expression is a variable.\footnote{An arithmetic
expression with some variables and constant, \eg \c{1 + x}, will also be
normalized to \c{X}, i.e. a variable.} It happens in
\packageMinimizedpropsitesa\% of the sites and is widespread across packages as
it is found in \packageMinimizedpackagea packages across our CRAN corpus.
\packageNbSymbolVarSitePercent are only symbols, \ie not complex expressions.
The \c{envir} argument is explicitly passed in \packageMinimizedpercentenvira\%
of the call sites. The call sites without an explicit \c{envir} argument all use
a specific \eval variant, \c{eval.parent}. Moreover, the \c{envir} argument is
both explicit and different from \c{parent.frame()} in
\packageMinimizedpercentenvirnonparenta\% of the sites, which suggests that when
given a variable, \eval is often used to get value in a non-local, non-parent
environment.


Similarly, when a function is evaluated on some variable (\c{F(X)} minimized
expression), the percentage of non default, non parent environments is rather
high, \packageMinimizedpercentenvirnonparentd\%.

Minimized expression \texttt{\$} refers to a slot access, with \texttt{\$},
\texttt{[]} or \texttt{[[]]}, as the top call in a simple expression, for
instance \texttt{a\$slot + 1}. It amounts for \packageNbSlotAccess sites, \ie
\packageSlotAccessPercent of the sites. The non-default, non-parent environment
percentage is here \packageMinimizedpercentenvirnonparentg.

As a whole, variable access, either as the top call, or inside a function call,
possibly with accessing a slot, represents
\packageGeneralizedVarAccessSitePercent of the sites.


\paragraph{Values.}
Many sites evaluates to \c{V}. \packageValOneNodePercent of those sites are only
passed an expression with one node, \ie one value, not a more complex expression
that is later constant folded. \c{V} corresponds to two main categories of
expressions passed to \eval:

\begin{itemize}
\item \eval is passed an actual value, a double, an integer, an environment, a
  closure and other simple types,
\item \eval is passed a language expression that trivially evaluates to a value,
  for instance, \c{quote(1)}, \c{quote(1 + 1)}, \quote{c("a", "b", "c")}, and
  does not include variables, so does not depend on a specific environment.
\end{itemize}

For the first category, \eval is not necessary. In
\packageNbCallSitesUniqueActualValue sites exhibiting \c{V}, \ie
\packageCallSitesUniqueActualValuePercent\% sites, it is only passed such actual
values. The median number of runs for those sites is
\packageMedianRunSitesUniqueActualValue (average
\packageAverageRunSitesUniqueActualValue, standard deviation
\packageSdRunSitesUniqueActualValue), suggesting it happens because of a lack of
coverage, because there are rarely triggered in our dataset and all paths are
not taken. However, manual inspection of the source code of some examples shows
that some sites actually pass values. For instance, in package \emph{ctmle}, in
function \c{evaluate\_candidate\_glmnet}:

\begin{lstlisting}
f1 <- eval(paste("A ~ ", paste(paste(names(X[,-1]), sep=''), collapse=" + ")))
f2 <- as.formula(f1)
\end{lstlisting}

\c{paste} concatenates strings and \eval will just return that string.
\c{as.formula} can take a string and convert it to a formula; \eval is not
needed here. In 20 random sites among the ones with values, a conservative
manual inspection showed that \packageUsefulValueEvalPercent of the sites had a
legitimate reason to use \eval.

The percentage of non default environment passed to \eval is
\packageMinimizedpercentenvirc when the minimized expression is a value. Some
sites which are passed a value, are also passed different minimized expressions.
For those ones, \packageNonDefaultEnvirValuePercent use a non-default
environment, and among the one that use a non-default environment,
\packageNonDefaultEnvirWithVarPercent have at least one minimized expression
with a variable inside.


\paragraph{\c{model.frame}}
This is a function used to create statistical models. We explain it in more
details in the section about the usage of \eval (see Section~REF).

\paragraph{Function definition.}
\c{FUN} represents a function definition at the top level of the expression, and
happens in \packageFunctionDefinitionSites sites, \ie
\packageFunctionDefinitionSitesPercent of the sites. It entails very few steps
of the interpreter, as the function is returned by \eval, and the body of the
new function is just stored for later execution. If we count also function
definition inside more complex expressions, it amounts to
\packageGeneralizedFunctionDefinitionSites sites, \ie
\packageGeneralizedFunctionDefinitionSitesPercent and suggests the widespread
use of higher-order functions in R, such as \c{lapply}, the R version of
\c{map}.

\paragraph{Assignments.}
This amounts to \packageAssignSitesPercent of the sites and corresponds to
minimized expressions \c{<-}, where the assignment is the top call, and
\c{F(F(<-))}, where the assignment happens inside, and others that we were not
enough present to be shown in table~\ref{tab:minimizedexpressions}. Assignments
are performed with \c{<-}, \c{<<-} or \c{assign}. Most often, assignment happens
in top call position. A non-default environment is specified only in
\packageNonDefaultEnvirAssignSitesPercent of the sites with assignments. Indeed,
most of those assignments occur in the default environment, \ie in the caller of
\eval. This hints that side effects are not widespread in \eval.\footnote{Side
effects can also happen in a function call in the \eval expression, and that
function call will hide assignment operators from us. We more precisely
instrument side-effects to environment in Section~REF.}

\paragraph{Large expressions.}  Compositions of functions, without variables
(\c{F(F(X))} minimized expression) is the second most frequent minimized
expression. \c{\{BLOCK\}} is a sequence of statements. It is usually large, with
in average \packageMinimizedoperationsjRnd operations in it.


The minimized expression classification suggests that the presence of variables
in the expression is highly correlated to the specification of a specific
environment, and that \eval is used here for its power in precising the
evaluation environment.


\NOTE{Maybe we should ditch the table of the minimized expressions and rather
  use the ones of combinations. It has the advantage of showing what is the most
  frequent, per call site. But it's a long tail, and sites that have several
  minimized expressions are not that numerous, only 13\%. But if include
  combinations, percentages change. For instance, 3314 call sites have only X,
  i.e. 22.51, instead of 4353 and 29.6\%. }




\subsubsection{The environment passed to \eval.}

The environment argument can be \c{NULL}, a \c{List} or a data frame. This
happens in 3.5\% of the cases: \eval copies the fields of the list or data frame
and creates bindings for them in a new environment. This pattern is used to
evaluate formulas which can directly refer to the fields of the data. The
\c{envir} argument can also be a number $n$. It means that the environment in
which the expressions is evaluated will be the result of \c{sys.call(n)} where
$n$ refers to the $n$-th stack frame.

The top-level environment in R is called the global environment. New
environments can be created using \c{new.env}. They can be provided a parent
environment which becomes the enclosing scope of the new environment.

We looked at the environments passed to all the \eval calls in our corpus.
Table~\ref{tab:environments} summarizes the results. A numeric environment class
\c{n} denotes the environment of the $n$-th call stack frame from the current
function. \c{global} denotes the top-level environment and \c{list} denotes a
list passed for evaluation of formulas. Environment classes of the form $n+$
denote the $n$-th environment extended with a new environment. The new
environment provides a limited form of sandboxing. All assignments using the
$\leftarrow$ function occur inside it and prevent the extended environment from
mutation. However, it is still possible to mutate the extended environment using
the $\leftarrow$ or \c{assign} functions; but, that happens rarely.

\begin{table}[htbp]{ \centering
\begin{tabular}{c|c|c|c}\hline
\multicolumn{2}{c|}{Core} & \multicolumn{2}{|c}{Packages}  \\\hline
Environment Class & Eval Calls \% & Environment Class & Eval Calls\\\hline
\CoreEnvClassA & \CoreProportionA & \PackagesEnvClassA &  \PackagesProportionA\\
\CoreEnvClassB & \CoreProportionB & \PackagesEnvClassB &  \PackagesProportionB\\
\CoreEnvClassC & \CoreProportionC & \PackagesEnvClassC &  \PackagesProportionC\\
\CoreEnvClassD & \CoreProportionD & \PackagesEnvClassD &  \PackagesProportionD\\
\CoreEnvClassE & \CoreProportionE & \PackagesEnvClassE &  \PackagesProportionE\\\hline
\end{tabular}} \label{tab:environments}
\caption{Environments in terms of \eval calls}
\end{table}

We observe that a disproportionately high number of core R \eval calls access
the caller's environment. This is because many core R functions call functions
that pass the result of \c{parent.frame()} to an \eval. A disproportionately
high number of calls to \eval happens in an extended top-level environment. This
can be explained by the fact that many packages evaluate code passed from the
user's workspace in the top-level environment to access the bindings.

The \c{imchange} function of package \c{imager} makes it possible to modify
images using a dedicated formula syntax using \~.\footnote{Inspired by {map} in
package \emph{purr}.} Here, \eval is evaluated in \c{newenv}, which creates a
new environment that inherits from \c{parent.frame()} by default (classified as
1+).

\begin{lstlisting}
 newenv <- new.env()
 ...
 fo <- parse(text=as.character(fo)[2])
 im[where] <- eval(fo,envir=newenv,enclos=env)
\end{lstlisting}

\c{adjCoef} in package \emph{actuar} find the root of an equation defined by a
function \c{h} whose arguments must be named \c{x} and \c{y}. \c{h} is
transformed into an auxiliary function \c{h2} that can be optimized. Here, the
list used for \c{envir} ensures the correspondance between the textual arguments
of \c{h} and the arguments of \c{h2}.

\begin{lstlisting}
 sh <- substitute(h)
 fcall <- paste(sh, "(x, y)")
 ...
 h2 <- function(x, y)
 eval(parse(text = fcall),
 envir = list(x = x, y = y),
 enclos = parent.frame(2))
\end{lstlisting}

\subsubsection{The expression passed to \eval.}
\paragraph{Touching package environments.}


\paragraph{Base}
The ranking of minimized expressions in \emph{base} is similar to the one in
CRAN bu the share of \c{V} is much larger, as it is present in
\baseMinimizedpropsitesa\% of the sites and \c{X} is a distant second, in
\baseMinimizedpropsitesb\% of the sites.

\paragraph{Kaggle}
Kaggle is dominated by sites with minimized expression \c{V}, with
\kaggleMinimizedcallsitesa sites, and \kaggleMinimizedpropsitesa\% of the
\kaggleNbCallSites sites in total. All of those values are an AST size of one
node, suggesting they do not come. \NOTE{TODO}
%TODO: all the src ref here seem to be incorrect and not from kaggle.


\mypara{Side-effects.} XXX026 total number of reads performed by
\eval. \NOTE{What is the definition of a read? An env lookup?} XXX027
average number of reads. XXX028 total number of writes performed by
\eval. Average number XXX029.

\NOTE{Aviral wrote''measure number of reads and writes by eval outside
  of its bubble'' --- what is the bubble here?}

XXX029 reads to caller environment -- where caller is the caller of
\eval.

XXX030 reads to other caller environment -- i.e. envs that are on the
call stack.

XXX031 reads to caller environment -- where caller is the caller of
\eval.

XXX032 reads to other caller environment -- i.e. envs that are on the
call stack.

Categorize side-effects between env passed to eval and other envs:
\begin{itemize}
\item ephemeral -- read to envs that are created during \eval  XXX033
\item local -- reads to env of the function in which the call to \eval
  lexically occurred; XXX034
\item parent -- reads to parent envs of the local one. XXX035
\item call-stack -- reads to envs that are on the call stack but not
  caller. XXX035.
\end{itemize}

In the corpus we observe \AllWritesRnd writes to variables of which
\EvalWritesRnd writes happen inside \eval. However, all writes are not
dangerous. Only writes to environments not local to the computation
spawned by \eval are side-effecting. These writes outlive the
computation and hence are visible outside it. The remaining writes are
local to the computation. We observe that \EvalSideEffectingWritesRnd
writes inside evals are side-effecting. This is only
\EvalSideEffectingWritesEvalPerc of all variable writes inside \eval
and \EvalSideEffectingWritesAllPerc of all variable writes in the
corpus.

An \eval is considered side-effecting if it performs a side-effecting
write to a variable, directly or indirectly. Only
\SideEffectingCoreCallPerc \eval calls in Core R are side-effecting
and \SideEffectingPackageCallPerc \eval calls in CRAN packages are
side-effecting.


\mypara{New Bindings.} Find number of times, eval introduces new
bindings. There are many ways -- library, load, attach, source, and
explicitly introduce bindings using super-assign, assign and define.

\mypara{C Calls.} Find number of times, eval makes call to native code
using - .Call, .External, etc. Find out if there are native functions
that are called only from within eval.

\mypara{Non-local returns}
Eval can do non-local returns effective bypassing evaluation of the
rest of the function. This can be useful for static analyzers.

\mypara{Purity.}
Conclude with number of evals that are ``pure'', i.e. evals which
could be ignored by a static analyzer without any problem.

\mypara{Source.}
There are various ways to obtain expressions:
\begin{itemize}
  \item \c{substitute} synthesizes ASTs from expressions by replacing
    symbols with their bindings in the specified environment.
  \item \c{expression}  creates a vector of expression
    objects from text.
  \item \c{parse}, \c{str2expression} and \c{str2lang} turn strings into
    expressions.
\end{itemize}

We observe 5.2\% cases where \eval directly evaluates the output of
\c{substitute}, 0.7\% cases where output of \c{parse} is read directly
and only 704 cases for \c{expression}. Most expressions consumed by
\eval are generated by other functions.

\c{eval(parse(...))} can be used for dynamic code loading. This forms
the core of \c{source} and \c{sys.source} functions in R that are
commonly used for loading code in R files in interactive settings. We
investigated the number of cases in which the output of \c{parse} and
its variants is passed to \eval, directly or transitively by tainting
their output. This corresponds to \PercentParsedCallSites of the total
\eval call sites and \PercentParsedEvals of the eval calls. We
observed that very few of the eval call (\NbParseFilesRnd in total)
consume the result of calling \c{parse} on a file. Most of the eval
calls consume the result of calling \c{parse} on a string. We also
identified one function in core R, \c{invokeRestartInteractively} that
prompts the user for input, parses it, and passes it to \eval.




\section{\eval patterns and anti-patterns}


\subsection{Evaluating variables in a specific environment}

\subsection{\c{match.call}}

\subsection{Meta-programming}

\subsection{Logging}

\subsection{Code transform}

\subsection{Plotting}

\section{\eval expressivity and macros}

% The Thomas Lumley's article, Programmer's Niche: Macros in R, (https://www.r-project.org/doc/Rnews/Rnews_2001-3.pdf) describes how to write a `defmacro` function that would like as a macro creator, using `substitute` and `eval`.  There are no local macro variables in this implementation though.

\section{The dangers of \eval} \label{sec:dangerseval}


\subsection{Eval Usage in R}

The R language was intended to be extensible, the combination of lazy
evaluation, \c{substitute} and \c{eval} are the tools given to
developers to this end. This API is slightly more complex than just
passing a string, it is conceivable that this may discourage some
casual users. \Eval is also being used to reduce boilerplate code and
provide convenience features for programmers. We now give some
representative examples of its usage.

\mypara{Intercession.} A common use case for \eval is to be combined with
\c{match.call}. \c{match.call} walks up the call stack, captures the code that
invoked the currently executing function, and returns it as an unevaluated
expression. The pattern is to transform a call to some function \c{f} into a
call to \c{g} with some arguments retained and others modified. As an
illustration, consider the \c{vcpart} package's function \c{tvcglm} that is
translated to a call to \c{tvcm} with two modifications to the argument list:
argument \c{control} can't be missing and \c{fit} is set \c{"glm"}. The function
ends with a call to \c{eval.parent} to ensure that the rewritten call is
evaluted in the same environment the original call was.
\begin{lstlisting}
 tvcglm <- function(formula, data, family, ...) {
   k <- match.call()
   k[[1]] <- as.name("tvcm")
   if ("fit" %in% names(list(...))) warning("'fit' ignored.")
   k$fit <- "glm"
   eval.parent(k)
 }
\end{lstlisting}
This pattern is recognizable by the fact that the expression is a call
and the target environment is that of the parent.

\mypara{Code Generation.} The more traditional use of \eval is to execute code
that was assembled by the programmer into a string. Here we show the method
\c{plot} for class \c{sback} in package \c{wsbackfit}, simplified for
explanatory purposes. The function takes a long argument list, the names of
which are captured in the list \c{opt}. The string \c{stub} is composed of a
subset of the arguments passed to this function; the variable is used to
construct a call to \c{base::plot} which will draw a plot. \NOTE{This example is
  a bit messed up. What is var? Does the use of ... in the eval grab the
  arguments of this call? If yes does it not end up with xlab twice? HELP} The
\parse and \eval combination is used by the the \source function in the base
library to load R code from a file in the current workspace.
\begin{lstlisting}
 plot.sback <- function(x,...) {
   opt <- names(list(...))
   stub <- paste(
    ifelse("xlab" %in% opt,"",paste(",xlab=\"",var,"\"",sep="")),
    ifelse("type" %in% opt,"",",type=\"l\""), sep="")
   plot <- paste("plot(x.data,",stub,",...)",sep="")
   eval(parse(text=plot))
\end{lstlisting}
This pattern is recognizable by its use of \c{parse} to turn a string
into an expression.


\mypara{Debloating.} \Eval is often used as a means to reduce
boilerplate code; simple and repetitive code can easily be replaced
with judcious use of \eval. For example, the \c{data.table} package
uses \eval to calls the \c{options} function with named arguments
taken from a vector of strings. While the benefits are limited in this
example, it is an attractive tool for programmers.
\begin{lstlisting}
  opts = c("datatable.verbose"="FALSE", # ...many others
  for (i in names(opts))
    eval(parse(text=paste0("options(",i,"=",opts[i],")")))
\end{lstlisting}
This pattern is a special case of code generation, recognizable by the
fact that \eval is executed in a loop.

\mypara{Trivial.} When values are passed to \eval, they are returned
unchanged. They are an example of trivial uses of \eval. Another
trivial use is the empty expression, often found in JavaScript, but
rare in R.

\section{LEGACY TEXT}

We looked at the top ten expressions passed to core and package \eval
calls. The most frequent ten expressions to eval calls from core R
contribute to 85\% of all \eval calls.

\begin{table}[!h] \centering
\begin{tabular}{@{}l|rr@{}} \hline
Expression & Eval Call &  \% \\\hline
\c{c("auto", "shell", "radix")} & 1,987,105 & 29\%\\
\c{c("auto", "shell", "quick", "radix")} & 1,593,169  & 23\%\\
\c{\{info <- loadingNamespaceInfo(...} & 1,008,632 &       14\%\\
\c{c("onLoad", "attach", "detach", "onUnload")}   & 470,566 &      6.9\%\\
\c{c("append", "prepend", "replace")} &              261,587&       3.9\% \\
\c{c("left", "right", "centre", "none")} & 162,086     & 2.4\%\\
\c{c("no", "ifany", "always")}   &                71,580 &       1.1\%\\
\c{c("pearson", "kendall", "spearman")}  & 72,962 &      1.1\%\\
\c{NULL}& 75,330  &      1.1\% \\
\c{Symbol}&                 66,279&       1\%\\\hline
\end{tabular}
\caption{Top ten eval calls in Core}\label{B}
\end{table}

The expression \c{\{info <- loadingNamespaceInfo(...} is added by core
R to a package directory during installation. To load the package,
this code is executed. It creates a namespace for the package, injects
the package bindings, and attaches the namespace to the program search
path. The \c{NULL} comes from a call to \c{substitute(subset)} in
\c{stats::model.frame.default} function which has a default value of
\c{subset} as \c{NULL}. The \c{Symbol} arises from a call to
\c{as.name} in \c{base::str} function that returns a symbol that is
looked up by evaluating it in a specific environment. The remaining
cases arise from calls to \c{match.arg} which is used to look up the
default choices for a variable and match against the choice passed by
the caller.

The most frequent ten expressions to eval calls from CRAN packages
contribute to 77.1\% of all \eval calls.

\begin{table}[!h]  \centering
\begin{tabular}{@{}l@{~}|@{~}r@{~}r@{}} \hline
Expression & Eval Call &  \% \\\hline
\c{Environment} &                                  989302   & 61\%\\
\c{column[rows] <<- what} &                        55677    & 3.5\%\\
\c{function(value) freduce(value, `function.list`)} & 37251& 2.3\%\\
\c{NULL} &                         32005    & 2\%\\
\c{List} &                         22293    & 1.4\%\\
\c{c("default", "default2012", "default2011" ...}& 20610    & 1.3\%\\
\c{force(..1)}            &                        20461    & 1.3\%\\
\c{alist(`spec`)}       &                         18532    & 1.2\%\\
\c{inner}               &                          18530    & 1.2\%\\
\c{String Vector}      &                           17487     & 1.1\%\\
\end{tabular}\caption{Top ten eval calls in CRAN} \label{C}
\end{table}

The expression \c{Environment} occurs because of the four callsites
explained above, \c{ggplot2::ggproto}, \c{R6::generator\_funs},
\c{future::backtrace} and \c{RModel::str.RMmodel}. The next
expression, \c{column[rows] <<- what}, is used inside the
\c{plyr::rbind.fill} function to merge data frames by assigning
concatenated vectors to rows. The \c{<<-} operator is interesting in
that it skips the current scope and assigns in a parent scope in which
the variable is already present. In our corpus, all these \eval calls
contribute to a single side-effect. The expression \c{function(value)
  freduce(value, `function.list`)} arises from the
\c{magrittr::\%>\%} function which is a pipe operator that pipes the
output of previous command to the next one. The expression is
evaluated in a custom environment to create a function binding for
evaluating the components of the pipe.\c{String Vector} and \c{List}
also arise from the same function when a string or a list is piped
using the \c{\%>\%} function into the next expression. The \c{NULL}
arises from \c{R6::generator\_funs} function when the \eval is passed a
\c{NULL} argument by the \c{DataMask\_generator} package. The
\c{c("default", "default2012", "default2011" ...} pattern arises from
\c{copula::polyG} where it reflectively access the default expression
for its formal parameter and evaluates it. The \c{force(..1)} and
\c{alist(`spec`)} patterns occur in \c{glue::glue\_data} function
which concatenates and interpolates strings. The two patterns occur
because the function captures unevaluated unnamed arguments and maps
the evaluation of \c{force(..1)} on them. The \c{force} function
forces promises and returns the result of evaluation. The \c{inner}
pattern arises from \c{glue::identity\_transformer} which enables the
creation of custom transformation functions for affecting the
interpolation and concatenation of input by the \c{glue} package.


\section{Why do we Eval}

\subsection{Discussion}

Yeah, why?

\subsection{Can we do without?}


We look at how \emph{consistent} the \c{expr} argument of \eval can
be, \ie how many different types of the resolved \c{expr} there are
per call sites. Most of the call sites, \ie \PercentMonomorphic, are
\emph{consistent}, and this is similar to javascript. However, a few
ones are highly \emph{polymorphic} (10 different types). They are the
pipe operators \c{\%>\%}, \c{\%<>\%} and \c{\%\$\%} in package
\emph{magrittr}. It is effectively used to compose functions on their
first argument, which can be of any type.

Similarly to JavaScript, there are also unnecessary uses of \eval. For
example, the \c{PerformanceAnalytics} package contains a function
\c{chart.QQPlot} that uses \eval to resolve a string into function and
another to call it and assign its results into a variable:
\begin{lstlisting}
function (R, d="norm", dp, ...) {
q.f <- eval(parse(text=paste("q",d,sep="")))
z <- NULL
eval(parse(text=paste("z<-q.f(",dp,",...)")))
}
\end{lstlisting}
  In both cases, there is no need for \eval:
\begin{lstlisting}
function (R, d="norm", dp, ...) {
q.f <- get(paste0("q",d))
z <- q.f(dp, ...)
}
\end{lstlisting}
  or even to a oneliner \c{do.call(paste0("q",d), as.list(dp, ...))}.

\subsection{Comparison with Javascript}




\begin{table}[h]
	\begin{tabularx}{\textwidth}{|X|X|X|X|X|}
		\hline
		& Julia & Javascript & Python & R \tabularnewline
		\hline % lstlinline does not like tabular environments...
		{\eval} & \texttt{eval(expr)} & \texttt{eval(expr)} & \texttt{eval(expr[, globals[, locals]])} &  \texttt{eval(expr[, envir[, enclos]])}  \tabularnewline
		\hline
		Environments &Global  & Local & Global or local  & Any \tabularnewline
		\hline
	\end{tabularx}
	\caption{Relative power of \eval throughout languages: in which environment it can evaluate its expression.}
\end{table}


\section{Conclusion}



\bibliography{bib/bibliography,bib/jv}

\end{document}
