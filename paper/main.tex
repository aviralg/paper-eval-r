\documentclass[review,screen,acmsmall,anonymous=true]{acmart}
\settopmatter{printfolios=false,printccs=false,printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}


\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}   %% For author/year citations
\usepackage{listings,hyperref,multirow,paralist,xspace,url,wrapfig}
\usepackage{tabularx}
\input{macro}
\newcommand{\mypara}[1]{\medskip\noindent\emph{#1}\xspace}
\newcommand{\NOTE}[1]{{\it Note: #1}\xspace}
\newcommand{\authorcomment}[3]{\xspace\textcolor{#1}{{\bf #2} #3}\xspace}
\newcommand{\todo}[1]{\authorcomment{red}{TODO}{#1}}

% cf. https://tex.stackexchange.com/a/144640
\makeatletter\let\expandableinput\@@input\makeatother

\begin{document}
\title{What We Eval in the Shadows}
\subtitle{A large-scale study of {\sf eval} usage in R programs}

\author{Author }
\authornote{Both authors contributed equally to this research.}
\email{a@b.com}\orcid{}\author{Author}\authornotemark[1]
\email{r@s.com}
\affiliation{\institution{Institute} \country{Country}}

\begin{abstract}
  \noindent Most dynamic languages allow users to turn text into code using
  various functions, often named \eval, with language-dependent semantics. The
  widespread use of these reflective functions hinders static analysis and
  prevents compilers from performing optimizations. This paper aims to provide a
  better sense of why programmers use \eval. Understanding why \eval is used in
  practice is key to finding ways to mitigate its negative impact. We have
  reasons to believe that reflective feature usage is language and application
  domain-specific; we focus on data science code written in R and compare our
  results to previous work that analyzed web programming in JavaScript. This
  paper studied \CranRunnableScripts scripts extracted from \CranPackages R
  packages, for a total of \packageAllcalls calls to \eval. We find that \eval
  is indeed in widespread use; R's \eval is more pervasive and arguably
  dangerous than what was previously reported for JavaScript.
\end{abstract}

\maketitle

\section{Introduction}

Most dynamic languages provide their users with a facility to
transform unstructured text into executable code and evaluate that
code. We refer to this reflective facility as \eval bowing to its
origins in LISP, all the way back in 1956~\cite{lisp}. \Eval has been
much maligned over the years. In computing lore, it is as close to a
boogeyman as it gets. Yet, for McCarthy, \eval was simply the way to
write down the definition of LISP; he was surprised that someone coded
it up and offered it to end-users. Since then, reflective facilities
have been used to parameterize programs over code patterns that can be
provided after the program is written. The presence of such a feature
in a language is a hallmark of dynamism; it is a form of delayed
binding as the behavior of any particular call to \eval will only be
known when the program is run, and that particular call site is
evaluated.

\vspace{2mm}\noindent\emph{Trouble in Paradise.} Reflective facilities
hinder most attempts to reason about or apply meaning-preserving
transformations to the code using them. In practice, \eval causes
static analysis techniques to lose so much precision as to become
pointless. For compilers, anything but the most trivial, local
optimizations are unsound after the use of \eval. Furthermore, the
addition of arbitrary code --- code that could have been obtained from
a network connection --- as a program is running is a security
vulnerability waiting to happen. To illustrate these challenges,
consider the interaction of a static analysis tool with a dynamic
language. An abstract interpretation-based program analyzer computes
an over-approximation of the set of possible behaviors exhibited by
the program under study~\cite{cc77}. A reflective facility may have \emph{any}
behavior that can be expressed in the target language, \ie any legal sequence of instructions can replace \eval. As dynamic
languages tend to be permissive, the analysis has to, for example,
assume that many (or all) functions in scope may have been redefined,
\eg that \texttt{`+`} now opens a network connection or something
equally surprising. A single occurrence of \eval causes the static
analyzer to lose all information about the program state and
meaning of identifiers. This loss of precision can sometimes be
mitigated by analyzing the string argument~\cite{moller03} to bound its
possible behavior, but when the string comes from outside the program, 
not much can be done. A frustrated group of researchers argued giving up
on soundness and, instead, under-approximating dynamic features
(soundiness)~\cite{soundy}. In their words, ``a practical analysis,
therefore, may pretend that \eval does nothing unless it can
precisely resolve its string argument at compile time.'' Alas,
assuming that \eval does not have side-effects or that side-effects
will not affect the results of the analysis may be unduly optimistic.

\vspace{2mm}\noindent\emph{Is Past Prologue?} Previous work investigated how
\eval is used in web programming, specifically in websites that use
JavaScript~\cite{pldi10a}. In 2010, 17 of the largest websites used the feature.
In 2011, 82\% of the 10,000 most accessed sites used \eval~\cite{ecoop11}. Yet,
the strings passed to \eval, and their behaviors, when executed, are far from
random; it was shown that when one can observe several calls to \eval, the
``shape'' of future calls can be predicted with 97\% accuracy~\cite{oopsla12b}.
Overall, practical usage suggested that most reflective calls were relatively
harmless. While this backs up the soundiness squad's approach, does it
generalize to other application domains than web programming and to other
languages?

\vspace{2mm}\noindent\emph{The Here and Now.} In this study, we investigate the
usage of \eval in programs written in the R programming language. R is a language
designed by statisticians for applications in data science~\cite{r,R96}. What
makes looking at R after JavaScript interesting is that, while both languages
are dynamic, they are quite different. While one can program in an
object-oriented style in R like in JavaScript, R is primarily a lazy, untyped,
functional language. JavaScript was designed to run untrusted code in a browser,
while R is used for statistical computing on desktops. JavaScript is a general-purpose language used by a vast community of programmers, while R is used for
scientific computing by data scientists and domain experts with, often, limited
programming experience. One can distinguish between library implementers,
developers with some programming experience and a working knowledge of R, and
end-users, who are typically not expert programmers and often have only a
cursory knowledge of the language.\footnote{Consider that R is lazy like
Haskell. We informally surveyed end-users and did not find a single user aware
of this fact. Library developers, on the other hand, know and program
defensively around laziness.} Thus, our goal is to highlight the differences in
usage between JavaScript and R and try to explain those differences in terms of
language features, application domain and programmer experience. Hopefully, some
of our observations will generalize to other languages.

\vspace{2mm}\noindent\emph{The What and How.} One significant benefit of
choosing R is that every package in the CRAN repository is curated and comes
with examples of typical usage. This gives us a large codebase that we can
analyze dynamically. To observe \eval, we built a two-level monitoring
infrastructure:\footnote{Our infrastructure is open source and publicly
available and  will be submitted to the artifact evaluation committee. } we can
monitor R programs by instrumentation --- this gives us access to many
user-visible properties of R programs --- but we can also monitor the
inner workings of the R interpreter --- this allows us to capture details not
exposed at the source level. Dynamic analysis is limited; it can only observe
behaviors triggered by the particular inputs passed to a program. Luckily, CRAN
libraries come with many tests and use-cases. The choice of the corpus is crucial.
Our corpus has been constructed to reflect the levels of sophistication of the R
community. We distinguish between \emph{CRAN packages} (\CranPackages curated
packages that pass stringent quality checks and are equipped with tests and
sample data) and \emph{Kaggle scripts} (\KaggleUnique end-user written programs
that perform a particular data analysis task). It is reasonable to expect that
\eval usage differs between these datasets: the libraries represent a lively
ecosystem with new libraries added each day, while end-user code is often thrown
together, run once, and never revisited.


\vspace{2mm}\noindent\emph{Why do we Eval?} {\bf A short summary of the
results should go here.}


\newpage
\section{Background and Previous work}

This section provides a short introduction to R and the reflective features of
the language; then looks at the semantics of \eval in R and discusses design
choices; lastly, this work is put in context.

\subsection{R, Briefly}

\citet{ecoop12} gave a programming language-centric overview of the R language.
They characterized it as a lazy, vectorized, functional language with a rich
complement of dynamic features expressive enough to layer several object systems
on top of the core language. Most data types are sequences of primitive values
constructed by calling the combination function \c{c}. For instance,
\c{c("Ha","bye")} evaluates to a vector of two strings, constants such as \c{42}
are vectors of length one. To enable equational reasoning, values accessible
through multiple aliases are copied when written to. Furthermore, values can be
tagged by attributes; these are key-value pairs. For instance, the attribute
\c{dim}-\c{c(2,2)} can be attached to the value at \c{x} by the call
\c{attr(x,"dim")$\leftarrow$c(2,2)}. In this case, the addition of this
attribute turns \c x into a matrix. The \c{class} attribute gives a 'class', in
the object-oriented sense, to a value. So, \c{class(x)<-"human"} sets the class
of \c{x} to \c{human}; classes are used for method dispatch. Every linguistic
construct is desugared to a function call, even control flow statements,
assignments, and bracketing. All functions can be shadowed or redefined, making
the language at the same time remarkably flexible and exceedingly challenging to
compile statically as vividly detailed by~\citet{dls19}. R uses a relaxed
call-by-need convention for passing arguments to functions. Each argument is a
thunk composed of an expression, its environment, and a slot for the result;
these are called \emph{promises}. To get the value of an argument, the
corresponding promise must be forced. Once forced, the promise's result is cached
for future use.


\subsection{On the Expressive Power of Eval}

While a data-to-code facility is available in many languages, some design
choices affect the expressive power of \eval. The key choices are the input
format, the environment in which generated code is evaluated, and the reflective
operations available to that code. Fig.~\ref{comp} summarizes designs.

\begin{figure}[!h]\center\small\begin{tabular}{r@{~}l|l|l|l}\hline
\tiny\sc Language&&\sc\tiny Input&\sc\tiny Scope&\tiny\sc Reflective operations\\\hline
\bf Julia&\cite{julia}     & expression& toplevel         & data\\
\bf Java&\cite{cl}  & bytecode  & classloader       & data\\
\bf JavaScript&\cite{ecoop11}& text      & current, toplevel& data\\
\bf R&\cite{R96}  & expression& programmatic      & data, stack, environment\\\hline
\end{tabular}\caption{Design space of \eval}\label{comp}
\end{figure}

The input to \eval can be in any format convertible to code. JavaScript allows
arbitrary strings to be used. Both Julia and R are more restrictive as they
require expressions (or abstract syntax trees). Finally, Java is the most
restrictive as its classloader only accepts complete classes in bytecode form.

The choice of the environment of \eval is essential as it determines how much of
a program \eval can observe as well as the reach of potential side-effects
performed by that operation. The most restrictive semantics is that of Java, 
where newly loaded code evaluates in the environment defined by the classes
visible from the current classloader. Julia and JavaScript's strict mode limit
\eval to the global environment. Finally, R is the most flexible as any
accessible environment can be selected and passed to \eval.

The last degree of freedom is the expressive power of the code executed by
\eval. The main difference between languages lies in how much of the state of a
program is accessible through reflective operations. Julia, Java and, JavaScript
all allow some form of introspection on the data. R is more flexible as it is
possible to inspect the program's call stack. Thus, any environment in the
program can be inspected and modified.

Given the above, the claim that R is amongst the languages with the most
powerful \eval seems plausible. The rationale for R's design seems to have been
to expose as much of the language and its internals as possible in order to
maximize expressivity. In R; \eval is a key tool to extend the language and
implement DSLs, it is also a replacement for macros. By contrast, the designers
of Julia chose to limit \eval. In Julia, only global variables can be
side-effected, and environments cannot be readily manipulated. This is designed
to shield optimized code from some of the most pernicious uses of the
facility~\cite{oopsla18a}. Furthermore, Julia provides a versioning
mechanism, called world age, to ensure that any methods defined within an \eval
only become visible at well-defined program points and thus that recompilation
does not have to occur when optimized code is running~\cite{oopsla20a}.

\subsection{Eval in R}\label{sec:eval-in-r}

R exposes a rich reflective interface with functions, \c{eval},
\c{evalq}, \c{eval.parent}, and \c{local}. The following simplifies details not
relevant to this paper, the interested reader should consult \citet{hadley}.

\begin{lstlisting}
 eval <- function(e, envir = parent.frame(),
                   encl = if(is.list(envir)) parent.frame() else baseenv()) ...
\end{lstlisting}
The most general is \eval. Parameter \c e is the \c{expression} to evaluate,
\c{envir} is the evaluation environment, and \c{encl} is used to look up
variables not found in \c{envir}. The default value of the latter is either the
caller's environment or the base environment \Eval can take several types for \c e, for
our purposes on the \c{expression}. These can be thought of as the abstract
syntax trees returned by the parser. An \c{expression} can be obtained by
calling \c{quote} or \c{parse} with a string:
\begin{lstlisting}
 > quote(a + b)
 # a + b
 > parse(text="a+b")
 # expression(a+b)
\end{lstlisting}
Usually, expressions are obtained from promises using \c{substitute}. Consider a
function called with \c{x = a+b}, then \c f returns an expression object.
\begin{lstlisting}
 > f <- function(x) substitute(x)
 > f(a+b)
 # a+b
 > substitute(a+b,list(a=1,b=2))
 # 1+2
\end{lstlisting}
The call to \c{substitute(x)} extracts the unevaluated expression from the
promise \c x. Function \c{substitute(e,envir)} takes two arguments, the second
is used to substitute free variables in \c e.
\begin{lstlisting}
 > environment()
 # <environment: R_GlobalEnv>
\end{lstlisting}
The \c{environment} function returns the current environment. Environments nest,
each has a parent. When creating a new environment with \c{new.env}, the parent
is the current environment. Environment chains can be traversed with
\c{parent.env}, until \c{emptyenv} is reached. The top-level environment is
\c{.GlobalEnv}, it has parents that represent the packages that have been
loaded. One can also directly read, modify or create new bindings, given any
environment:
\begin{itemize}
\item \c{envir\$v} and \c{get("v",envir=envir)}: read  \c{v} from \c{envir};
\item \c{envir\$v<-2} and \c{assign("v",2,envir=envir)}: store 2 in \c{v}, if
  not found, \c{v} is created in \c{envir}.
\end{itemize}
\noindent Environments are used as hash maps as they have reference semantics
and a built-in string lookup. Functions \c{parent.frame} and \c{sys.frame}
return environments further up the call stack.
 \begin{lstlisting}
  evalq <- function(e,envir,encl) eval(quote(e),envir,encl)
  eval.parent <- function(e,n) eval(e,parent.frame(n))
  local <- function(e,envir,encl) evalq(e,new.env())
\end{lstlisting}
The three variants can be expressed with \eval: \c{evalq} quotes its
argument to prevent evaluation in the current environment,
\c{eval.parent(e,n)} evaluates \c{e} in the environment of the {\tt n}-th
caller of this function; finally, \c{local} evaluates \c{e} in a new environment
to avoid polluting the current one.

\subsection{Previous Work}

\citet{ecoop11} provided the first large-scale study of the runtime behavior of
\eval in JavaScript. They dynamically analyzed a corpus of the 10,000 most
popular websites with an instrumented web browser to gather execution traces.
They show that \eval is pervasive, with 82\% of the most popular websites using
it. The reasons for its use include the desire to load code on demand,
deserialization of JSON data, and lightweight meta-programming to customize web
pages. While many uses were legitimate, just as many were unnecessary and could
be replaced with equivalent and safer code. They categorized inputs to \eval so
as to cover the vast majority of input strings. Restricting themselves to \eval
in which all named variables refer to the global scope, many patterns could be
replaced by more disciplined code~\cite{oopsla12b, moller12}. The work did not
measure code coverage, so the numbers presented are a lower bound on possible
behaviors. Furthermore, JavaScript usage in 2011 is likely different from today,
\eg Node.js was not covered. More details about dynamic analysis of JavaScript
can be found in~\cite{liang}.

\citet{wang} analyzed use of dynamic features in 18 Python programs to find if
they affect file change-proneness. Files with dynamic features are significantly
more likely to be the subject of changes than other files. Chen et al. looked
at the correlation between code changes and dynamic features, including \eval,
in 17 Python programs~\cite{chen}. They did not observe many uses of \eval.
\citet{oscar} performed an empirical study of the usage of dynamic features in
1,000 Smalltalk projects. While \eval itself is not present, Smalltalk has a
rich reflective interface. The authors found that reflective methods are used in less
than 2\% of methods. The most common reflective method is \c{perform:}; it send
a message that is specified by a string. These features are primarily used in the
core libraries.

\citet{bodden} looked at the usage of reflection in the Java DaCapo benchmark suite.
They found that dynamic loading was triggered by the benchmark harness. The
harness then executes methods via reflection. This caused static analysis tools
to generate an incorrect call graph for the programs in DaCapo.

\citet{Arceri21} study \eval in JavaScript from a software security point of
view. The authors report that 53\% of the malware they studied used \eval as a
means to obfuscate attack code or mount attacks. They propose an abstract
interpretation-based approach to analyzing dynamic languages. One must construct
a static approximation of the argument to \eval and then analyze possible
behaviors of the interpreter when evaluating the generated code.

\citet{ecoop12} had a short section on the usage of \eval in R. They found the
that \eval  is widely used in R code with 8500 call sites in CRAN and 2.2 million dynamic calls. The 15 most frequent call sites account for 88\% of those. The
\c{match.arg} function is the highest used one with 54\% of all calls. In the
other call sites, they saw two use cases. The most common is the evaluation of
the source code of a promise retrieved by \c{substitute} in a new environment,
\eg as done in the \c{with} function. The other use case is the invocation of a
function whose name or arguments are determined dynamically. For this purpose, R provides \c{do.call} and thus \eval is overkill.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

This section explains how we selected our corpus and how we obtained the
reported results.

\subsection{Corpus}

Our corpus is assembled from three sources: the \emph{Base} libraries 
bundled with the language implementation, packages hosted on \emph{CRAN}, and scripts from \emph{Kaggle}.

\mypara{Base.} The dataset contains \BasePackages libraries performing basic
arithmetics, statistics, and operating system functionalities. These libraries
are bundled with R and executed pervasively. Together they contain
\BaseEvalCallSites call sites to \eval in \BaseFunsWithEvals functions. Some of these
functions provide basic functionalities such as package loading, and thus, there
is hardly any code that does not invoke one of these.

\mypara{CRAN.} The Comprehensive R Archive Network
({\small \url{cran.r-project.org}}) is the largest curated repository of R
packages. It hosts \CranAvailablePackagesRnd packages with 6 new ones submitted
daily~\cite{Ligges2017}. Packages are authored by experienced developers and
abide by well-formedness rules automatically checked on each commit.
Each package comes with sample data. There are three sources of
runnable code in a package: \emph{tests, examples}, and \emph{vignettes} --
respectively, unit tests, code snippets from the documentation, and long-form
use-cases written in RMarkdown. Examples and vignettes can be turned into
scripts by extracting the relevant code. From \CranPackages packages we
extracted \CranRunnableScripts scripts that contain \CranRunnableCode lines of
code (\CranRunnableCodeExamplesRnd in examples, \CranRunnableCodeVignettesRnd in
vignettes and \CranRunnableCodeTestsRnd in tests).

\mypara{Kaggle.} The Kaggle website ({\small \url{kaggle.com}}) is an online
platform for data science. It allows users to submit problems and compete to
solve them. Solutions, called \emph{kernels}, are uploaded as plain scripts or
notebooks. The quality of code on Kaggle is not uniform. Each kernel is
runnable, and thus there is one script per kernel. Input data is provided. We
obtained \KaggleKernels kernels. Since Kaggle is not curated, we used SHA-1 hashes
to identify and remove \KaggleDuplicates duplicate entries. The remaining
\KaggleUnique kernels are unique solutions to \KaggleCompetitions competitions.
They contain \KaggleCode lines of R code. Only \KaggleWithEvals kernels call
\eval.

\subsection{Pipeline}

The analysis presented in this has been automated by a pipeline that acquires
packages, extracts scripts, executes them, traces their behavior, and summarizes observations. Figure~\ref{fig:pipeline} shows the main steps along
 with their running time, data size, and  number of elements manipulated.
  Timings are from a cluster of three 2.3GHz Intel Xeon 6140 servers, with 72 cores and 256GB of RAM, and shared OCFS network storage. The pipeline steps are:


\begin{figure}[!h]\hspace{-5mm}
  \includegraphics[width=.95\linewidth]{pipeline.pdf}
  \caption{Pipeline}\label{fig:pipeline}
\end{figure}

\medskip
\begin{compactenum}
\item \emph{Download.} Packages are downloaded from CRAN. For scripts, a web
  crawler retrieves code, and the Kaggle command-line tool gets data.
  Installation is complicated by native dependencies, which are not adequately
  documented and thus hard to resolve automatically.
\item \emph{Extract.} Given installed packages and scripts, the next step is to
  create runnable programs. The \genthat tool helpfully extracts all runnable
  code snippets from a package and turns each of these into a self-standing
  program~\cite{issta18}. Some Kaggle kernels are already scripts. For those,
  nothing more needs to be done. Others kernels are packages as notebooks, either   as Rmarkdown or Jupyter. For those, we use \c{knitr} to extract
   runnable code.
  The body of each extracted program is instrumented with calls to our dynamic
  analyzer to ensure that we only record calls to \eval from the code of
  interest and not from bootstrapping or execution harness operations.
\item \emph{Trace.} Each program executes using our dynamic analysis tool, a heavily instrumented interpreter that captures calls to \eval and
  many other fine-grained runtime events. Packages are run twice, once to
  capture \eval calls originating from package code and a second time to
  capture calls coming from the base libraries. To avoid any interference, each
  program is run in its own process.
  As the GNU R bytecode compiler is written in R, we turn it off
  to avoid recording \eval in its code, and fallback on the interpreter.
\item \emph{Analyze.} Finally, the analysis output is merged, cleaned, and summarized
  in a post-processing phase driven by series of R scripts. The summarized data
  is then analyzed in RMarkdown notebooks to gather insights. All figures and
  numbers appearing in the paper are generated automatically. Figures are
  produced in PDF by \c{ggplot2}; numbers are exported as \LaTeX macros.
\end{compactenum}

\medskip\noindent The code extraction and tracing steps of the pipeline are run
in parallel~\cite{GNUparallel} orchestrated by a Makefile. Servers have
identical environments thanks to docker images with all dependencies installed.

\subsection{Dynamic Analysis}

The dynamic analysis is performed by \rdyntrace, a modified R virtual machine
based on GNU R 4.0.2 that exposes low-level callbacks for a variety of runtime
events~\cite{oopsla19b}.

The tracer registers callbacks to all \eval functions as well as a few other
ones that allow us to identify better where the expressions come from. For
example, we taint the results of calls to \c{parse} (turns strings into
expressions) and \c{match.call} (reflects on current call). We also capture calls
to R API for dynamic code loading (\eg\xspace \c{library}, \c{require}, and
\c{source}). Next, we subscribe to the events related to a variable definition
and assignment, allowing us to record side effects that happen in environments
while evaluating code in \eval. One challenge we encountered during the analysis
was that essentially R only provides source references for top-level calls in a
function body or in the block surrounded by braces. That is to say, a method
whose body would \c{eval(x)} would not have debug information that we can use to
identify that particular \eval. The same expression surrounded by braces would
have a line number and source reference. This is unfortunately not easily fixed
in the R implementation. Instead, we extended the dynamic analysis tool to
attach synthetic source code references to all \eval call sites by traversing
ASTs. The tracer is implemented as an R package in 3.2K C++ and 1.3K of R code.
For performance reasons, most of the tracing is done in C++. While in theory, the
implementation is relatively straightforward, it is not so in practice:
%
\begin{compactitem}[---]

\item The lazy evaluation makes it challenging to analyze function arguments
  while tracing as prematurely forcing a promise might have dangerous
  consequences.

\item While the R interpreter is implemented in C, a lot of the core
  functionality is done in R. For example, package loading is implemented in R
  using \eval or the S4 and R6 object systems. This makes it hard in the tracer
  to separate the \eval that are essential to user programs from the
  accidental ones that are products of the way R implements its basic
  operations. This is even more so for the side-effects analysis.

\item In the dynamic analysis, we run all the \emph{runnable} code we obtained
  from CRAN and Kaggle---\ie actual code written by people with highly varying
  expertise in R and programming in general. This exercises a lot of the corner
  cases of the highly underspecified R behavior.

\end{compactitem}


\mypara{Limitations.} The analysis has some limitations. Even with the extension
described above, there are still \PkgUndefinedRnd \eval calls without source
references. This occurs when a reference to the \eval function is passed as an 
argument to a higher-order function or when the \eval call originates from
native code. However, these missing source references account for a meager
\PkgUndefinedRatio of all calls; they are unlikely to affect our results. Other
limitations are that we ignore calls to the native \eval function and to the
alternate \c{rlang::tidy\_eval} function, which uses native \eval internally.
The \c{rlang} package introduces a new kind of promise called \c{quosure} which
is evaluated by \c{tidy\_eval}. None of these limitations should invalidate our
conclusions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Usage Metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{wrapfigure}{r}{5cm} \hspace*{-12mm}
  \centering
  \includegraphics[width=72mm]{pkgs-eval-callsites-hist.pdf} \caption{CRAN
  \eval call sites}%
  \label{fig:pkgs-eval-callsites-hist}
\end{wrapfigure}
%
This section focuses on CRAN packages and reports statistics about the usage of
\eval. We use the word \emph{site} to an occurrence of a call site to the \eval
function in the source code and \emph{call} to denote an observed invocation of
the \eval function.

\subsection{CRAN}

There are \PkgEvalCallSites \eval sites in \PkgPackages packages. The proportion
of packages calling \eval is \PkgPackagesRatio. Over half of these packages
have fewer than 3 sites, and with the exception \MaxEvalCallSitesPackage, which
has \MaxEvalCallSitesCount sites, all packages contain fewer than
\MaxEvalCallSitesRest sites (\cf Fig~\ref{fig:pkgs-eval-callsites-hist}, which
shows a histogram of sites per package). These sites appear in \PkgFunsWithEval
functions (\CranFunsWithEvalRatio of all functions in CRAN).


\begin{figure}[!b]
\small
\begin{tabular}{@{}l@{\hspace{1.5cm}}l@{}}
\begin{minipage} {5cm}
  \begin{tabular}{|r@{\,}r@{\,}l@{}r|r@{\,}r@{\,}l@{}r|} \hline
    \multicolumn{3}{|c}{\small\#calls} &\small \#pck
&     \multicolumn{3}{c}{\small\#calls} &\small\#pck \\\hline
\tt 1 &--& \tt 10      & \packageBina  & \tt 10K &--&\tt 100K  & \packageBine\\
\tt 11 &--& \tt 100    & \packageBinb  & \tt 100K &--&\tt 1M  & \packageBinf\\
\tt 101 &--& \tt 1K    & \packageBinc  & \tt 1M &--&\tt 10M   & \packageBing\\
\tt 1K &--& \tt 10K    & \packageBind  & \tt 10M &--& \tt 100M & \packageBinh\\\hline
\end{tabular}
\caption{Call frequency}\label{freq}
\end{minipage}
&
\begin{minipage}{7cm}
\begin{tabular}{|@{\,}r|rrrr|}\hline
  &\eval & \c{evalq} & \c{eval} & \c{local}\\[-1.5mm]
           & & & \c{.parent} &\\\hline
\small Static sites &\packageStaticeval&\packageStaticevalq&\packageStaticevalparent&\packageStaticlocal \\
\small Exercised sites&\packageTriggeredeval&\packageTriggeredevalq&\packageTriggeredevalparent&\packageTriggeredlocal\\
\small Invocations&\packageEvalsRnd&\packageEvalqsRnd&\packageEparentsRnd&\packageLocalsRnd\\\hline
\end{tabular}~\\[2mm]\caption{Variants}\label{tab:variantseval}
\end{minipage}\end{tabular}

%\end{figure}

%\begin{figure}[b] \centering
  \includegraphics[width=.78\textwidth]{traced-eval-callsites.pdf} \centering
  \caption{\eval call sites coverage of the \PkgPackages packages.}%
  \label{fig:traced-eval-callsites}
\end{figure}


For dynamic analysis, we run \CranRunnableScripts programs extracted from
\CranPackages packages. Any run that does not exercise \eval is discarded. This
left \packageNbruns runs from \packageCorpus packages. There were
\packageAllcalls calls in \packageTriggeredpkgs packages originating from
\PkgHitEvalCallSites unique sites. In terms of coverage, the data exercised
\PkgHitEvalCallSitesAvgRatio of sites, a coverage similar to the code coverage
metric for the packages, which is \PkgCodeCoverage. The fact that not all sites
are exercised can be chalked down to incomplete tests and occasional analysis
failures (\PkgFailedProgramsRatio of programs crashed or timed out).
Fig~\ref{fig:traced-eval-callsites} shows the number of sites that were
exercised; coverage is unequal. Figure~\ref{freq} summarizes the frequency of
dynamic calls to \eval, with the left column being the number of calls and the
right, the number of packages in that range. There are \packageFewcalls packages
with low \eval frequency, fewer than 100 calls, and a smaller, but still
significant, number of packages, \packageManycalls to be precise, that use \eval
more than 1,000 times. Package \packageMaxcallspack makes \packageMaxcalls calls
and thus accounts for over half of the observations.
Figure~\ref{tab:variantseval} summarizes the use of variants of \eval. For each
of the four variants, the first row shows the number of sites in the corpus
(\emph{static}), the second row is the number of sites that were encountered
during analysis (\emph{exercised}), the last row is the number of calls
(\emph{invocations}). The overwhelming majority of sites and calls are to \eval
itself, \c{eval.parent} is rare, and both \c{evalq} and \c{local} are barely
used at all. The difference between sites and exercised sites underscores the
limitations due to code coverage.

\begin{wrapfigure}{4}{5.8cm}
  \small
  \vspace*{-2mm}
\centering
  \begin{tabular}{|r@{\,}r@{\,}l@{\,}r|r@{\,}r@{\,}l@{}r|} \hline
\multicolumn{3}{|c}{\#calls} & \#sites &
\multicolumn{3}{c}{\#calls} & \#sites \\\hline
\tt 0 &--& \tt 50    & \packageRunbina & \tt 501 &--& \tt 1000   & \packageRunbine\\
\tt 51 &--& \tt 100  & \packageRunbinb & \tt 1001 &--& \tt 1500  & \packageRunbinf\\
\tt 101 &--& \tt 250 & \packageRunbinc & \tt 1501 &--& \tt 2000  & \packageRunbing\\
\tt 251 &--& \tt 500 & \packageRunbind & \tt 2001 &--& \tt 3000 & \packageRunbinh\\\hline
\end{tabular}

  \medskip  (a) All  \medskip  \medskip

  \vspace*{-1mm}
  \includegraphics[width=5.6cm, trim=5.5cm 0 5cm 0, clip]{package_calls_per_run_per_call_site}

  (b) Small

\caption{Normalized calls} \label{cn}\vspace{-2mm}

\medskip
\medskip

\begin{tabular}{c}
  \vspace*{-1mm}
  {\hspace{-25mm}\includegraphics[width=0.7\textwidth]{package_size_loaded_distribution}}
\end{tabular}
\caption{Loaded code} \label{fig:sizedistribution}
\end{wrapfigure}

Figure~\ref{cn}(a) shows normalized call counts per site; on the left are
the average number of calls from a given site and given run, on the right are the counts of sites that fall in that range. For instance, \packageRunbinh sites are
invoked 2,000+ times per run. Larger numbers suggest loops or recursive contexts
-- this seems to be the exception as most \evals, \packageRunbina, are exercised
50 times or less. Figure~\ref{cn}(b) zooms in on low frequency sites. The x-axis
shows normalized calls, and the y-axis is the number of sites for that value. Most
low-frequency \evals are invoked only once; about half as many are invoked
twice; after that, the frequency quickly drops.

\Eval takes any value, but if its argument is not an expression, \eval returns
it unchanged. Expressions account for \packageCodepercent of arguments in our
corpus. More specifically, \packageSymbolpercent are \texttt{symbol}s (single
variables such as \c{x}), \packageLanguagepercent are \c{language} objects
(function calls such as \c{f(x)} or \c{x+1}), and \packageExpressionpercent are
\c{expression} objects (lists of expressions such as function calls or
symbols). Further inspection reveals that most symbols,
\packageGgplotsymbolpercent to be exact, come from a single site in the
\c{ggplot2} package and have the value \c{\_inherit}.\footnote{This models
inheritance in \c{ggproto}, one of the many object-oriented systems in
R, used exclusively in the \c{ggplot2} graphics library.}

To estimate how much executable code is injected through \eval, we measure the number of nodes in the expressions; for example, \c{x+1} counts as 3. We
also measured string lengths of unparsed expressions, but these measurements
were dominated by the size of data objects which could range in the MBs. The
median argument size is \packageMedianszeval (due to the symbols), and the
average is \packageAvgszeval nodes. The largest \eval input observed is
\packageMaxszeval\,-- a significant chunk of code.
Figure~\ref{fig:sizedistribution} shows the distribution of sizes for arguments
of fewer or equal to 25 nodes. The x-axis is the size of arguments in number of
nodes, and the y-axis is the count of arguments with that size. The size drops
rapidly, with few observations larger than 15 nodes. The long tail is omitted
for legibility.

To estimate the work performed in \evals, we count instructions executed by the
interpreter. Most invocations perform relatively little work, with
\packageSmalleventspct of \evals executing 50 or fewer instructions. The violin
plot of Figure~\ref{ev}(a) corresponds to \evals executing $\leq$ 50
instructions; it is dominated by trivial symbol lookups. Figure~\ref{ev}(b) has
the work-intensive \evals, which go all the way to \packageMaxeventsRnd
instructions.

\begin{figure}[tb!]
\begin{tabular}{@{}c@{}c@{}}
\begin{minipage}{7.5cm}
 \includegraphics[width=\textwidth]{package_events_per_pack_small}
\end{minipage}&\begin{minipage}{7.5cm}
  \includegraphics[width=\textwidth]{package_events_per_pack_large}
\end{minipage}\\[-3mm]
\small (a) Small & \small (b) Large
\end{tabular}
 \caption{Instructions per call} \label{ev}
\end{figure}

\subsection{Base}\label{sec:usage-base}

We recorded \baseAllcalls \eval calls in the \BasePackages base libraries. This
comes from running 10\% randomly selected programs from the
\CranRunnableScriptsRnd extracted programs from CRAN packages, covering
\baseTriggeredevalpct of the \BaseEvalCallSites sites. Most of the calls
(\baseEvalsratio) are to the \c{eval} function, and most arguments
(\baseCodepercent) are expressions. However, unlike CRAN, the majority
(\baseLanguagepercent) of the arguments are language objects. The median
argument size is \baseMedianszeval, which is more than for CRAN and the maximum size is \baseMaxszeval, less than for CRAN. Small instruction counts ($<50$) amount to \baseSmalleventspct of calls. A single site in the \c{match.arg}
function is responsible for \baseTopFuncPercent of all recorded calls. This
function provides a convenient way for argument verification using partial
matching. It is heavily relied on in R functions that use string arguments to
parameterize function behavior, a popular API mechanism in R. For example, in a
body of \c{center <- function(x, type = c("mean", "median", "trimmed"))} one
can get the value of \c{type} parameter using \c{type <- match.arg(type)}. A
call \c{center(x, "trim")} will match \c{type} to \c{"trimmed"}.

The challenge dealing with Base is that every program uses it. It can thus
generate extreme amounts of data, but the data is quite predictable. For the
rest of the paper, we only mention Base when there are surprising
observations.

\medskip

\subsection{Kaggle}

In total, \kaggleAllcalls \eval were recorded, all to \eval function. Out of
\kaggleStaticeval sites from \KaggleWithEvals scripts, only \kaggleTriggeredeval sites
in \kaggleNbruns scripts were hit; \KaggleFailedScripts scripts failed to run,
and the others did not exercise the \eval sites. This is partially expected
as Kaggle code does not need to abide by any checks. Upon a manual inspection,
we observed that indeed, the failing scripts were of poor quality, often not
finished, using misspelled package names, hard-coded file paths, or accessing
missing files.

Most call sites are invoked only once. Only one site is called \kaggleMaxcalls times.
Expressions amount for \kaggleCodepercent of arguments. Unlike the CRAN corpus,
there are very few symbols (\kaggleSymbolpercent). Most expressions result from
calls to \c{parse}; thus, most \evals start with strings. The median argument
size is \kaggleMedianszeval, which makes sense, as few arguments are only
symbols. The largest argument is \kaggleMaxszeval. The distribution of
instructions per eval is similar to CRAN; \kaggleSmalleventspct of \evals
execute fewer than 50 instructions. Manual inspection of \eval usage in Kaggle
suggests that it is consistent with the data obtained for CRAN. We do not
discuss it further.


\medskip

\subsection{Discussion}
These results show that \eval is central to the language implementation as it is
omnipresent in the relatively small Base library. The Base library is, in turn,
used by every single package. To avoid having Base dominate the result, we have
excluded calls occurring there from the data we report when analyzing packages.
CRAN Packages, which are typically developed by experienced programmers, make
regular and varied use of \eval. They represent our most interesting data set as
these packages result from over 20 years of contributions by thousands of
authors. The code in packages is well maintained and relatively well tested.
Finally, Kaggle scripts are often written by less sophisticated users and
likely perform more straightforward tasks, thus having a lesser need for \eval, and its uses originate from strings as these are easier to manipulate for end-users. Due to the relative paucity of data in Kaggle, we do not pursue the data set further.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Taxonomy of Eval}

The previous section gave a quantitative view of \eval usage; we now try to
elucidate \emph{what} it does.

\subsection{The expression in \eval} \label{sec:minimized}


The expressions passed to \eval vary widely. In order to categorize them, let us
use a minimization function $min(e)$ which for a given expression $e$ returns a
normal form that abstracts incidental details allowing the reader to focus on
the structure of the evaluated code. The minimization function performs constant
folding of arithmetic and string expressions for base operators, e.g.
$min(\c{1+1})=\c{V}$, value simplification, $min(\c{c(1,2,3+2)})=\c{V}$,
variable absorption, $min(\c{x+y})=\c{X}$, function absorption,
$min(\c{g(f(x),h(z))})=\c{F(F(X))}$ and a number of other simplifications.
Table~\ref{tab:minimizedexpressions} gives the 10 most frequent forms; \#sites
and \%sites are, respectively, the number and ratio of sites receiving arguments
of that form; \#packages is the number of packages with that form; \#operations
is the median number of instructions performed by the interpreter; and \%envir
is the ratio of sites that evaluate in a function environment. The example
column shows one sample expression $e$ that normalizes to the particular form.

\begin{table}[h]\small
\begin{tabular}{|c|r|r|r|r|r|c|}\hline
  $min(e)$& \#sites & \%sites & \#packages & \#operations & \%envir & example\\\hline
\c X&\packageMinimizedcallsitesa &\packageMinimizedpropsitesa &\packageMinimizedpackagea &\packageMinimizedmedianoperationsaRnd &\packageMinimizedpercentparentframesa & \c{y+1}\\\hline
\c{F(F(X))} & \packageMinimizedcallsitesb  & \packageMinimizedpropsitesb & \packageMinimizedpackageb  & \packageMinimizedmedianoperationsbRnd & \packageMinimizedpercentparentframesb & \c{gbov( mean(x), a-1)}\\\hline
\c{V}&\packageMinimizedcallsitesc &\packageMinimizedpropsitesc &\packageMinimizedpackagec &\packageMinimizedmedianoperationscRnd &\packageMinimizedpercentparentframesc& \c{c(42,21,0)}\\\hline
\c{F(X)}& \packageMinimizedcallsitesd & \packageMinimizedpropsitesd & \packageMinimizedpackaged & \packageMinimizedmedianoperationsdRnd & \packageMinimizedpercentparentframesd & \c{seq\_len(iters)} \\\hline
\c{\$} & \packageMinimizedcallsitese & \packageMinimizedpropsitese & \packageMinimizedpackagee & \packageMinimizedmedianoperationseRnd & \packageMinimizedpercentparentframese & \c{DF\$B}\\\hline
\c{model.frame}& \packageMinimizedcallsitesf & \packageMinimizedpropsitesf & \packageMinimizedpackagef & \packageMinimizedmedianoperationsfRnd & \packageMinimizedpercentparentframesf &  \c{model.frame(formula = Z $\sim$ U)}   \\\hline
\c{F()}& \packageMinimizedcallsitesg & \packageMinimizedpropsitesg & \packageMinimizedpackageg & \packageMinimizedmedianoperationsgRnd & \packageMinimizedpercentparentframesg & \c{rgamma(3, 2, n = 10L)} \\\hline
\c{FUN} & \packageMinimizedcallsitesh & \packageMinimizedpropsitesh & \packageMinimizedpackageh & \packageMinimizedmedianoperationshRnd & \packageMinimizedpercentparentframesh & \c{function(x, y) x + 3 * y} \\\hline
\c{<-} & \packageMinimizedcallsitesi  & \packageMinimizedpropsitesi & \packageMinimizedpackagei & \packageMinimizedmedianoperationsiRnd & \packageMinimizedpercentparentframesi & \c{x[1, 2:3, 2:3] <- value}\\\hline
\c{BLOCK} & \packageMinimizedcallsitesj & \packageMinimizedpropsitesj & \packageMinimizedpackagej & \packageMinimizedmedianoperationsjRnd & \packageMinimizedpercentparentframesj & \c{\{ write.csv(iris, tf) ; file.size(tf) \}} \\\hline
\end{tabular}
\caption{Minimized expressions} \label{tab:minimizedexpressions}
\end{table}

\medskip\noindent  We detail these forms and discuss their implication for the behavior of \eval.

\newcommand{\EE}[1]{{{\emph{\framebox{#1}}}}\\[1mm]}

\medskip\noindent\EE{$min(e)=\c{V}$} Expressions that represent values are
frequently passed to \eval -- they occur in 17\% of sites. The majority of
those, \packageValOneNodePercent, are inline constants (integer or double
vectors). The rest trivially evaluate to a value, \eg~\c{1+1}.\footnote{True as
long as base functions such as \c{+} are not redefined. They typically are not,
but this is a limitation of this categorization.} In our corpus,
\packageNbCallSitesUniqueActualValue call sites only ever see a simple value.
Manual inspection reveals cases such as the following
\begin{lstlisting}
 f1 <- eval(paste("A~",paste(paste(names(X[,-1])),collapse="+")))
\end{lstlisting}
where the argument to \eval is a string which \eval simply returns. Of twenty
randomly selected \evals, \packageUsefulValueEvalPercent of them  need % There are a majority which needs eval
\eval for some inputs. Either we are dealing with a value that needs to be
constructed dynamically, or the value is a default case that sometimes is
replaced by a more interesting expression. This form is usually evaluated in few
interpreter steps; in fact, the median is only
\packageMinimizedmedianoperationscRnd. The environment in which they evaluate is
mostly irrelevant (unless a built-in operator is redefined).

\medskip\noindent\EE{$min(e)=\c{X}$} Variable lookups are the most common form; they are found in 28\% of the sites. This form includes simple variable reads,
\eg~\c{x}, which are \packageNbSymbolVarSitePercent of \c X. The form also
subsumes \c{V}, so it includes a mixture of arithmetic expressions,
\eg~\c{x+y+1}. The operations allowed are limited to built-in arithmetics. It is
noteworthy that, while most \c{X}\!s evaluate in a single step, the variable can
be bound to a promise, and accessing it may trigger evaluation of that promise,
thus resulting in an arbitrary amount of computation. The median number of
interpreted operations is \packageMinimizedmedianoperationsaRnd, suggesting that
it is not the common case. Lookups are often evaluated in constructed
environments; as few as \packageMinimizedpercentparentframesa of these
expressions are evaluated in a function environment.

\medskip\noindent\EE{$min(e)=\c{\$}$} This form extends \c X to include lookup
with the dollar operator, \eg~\c{x\$f}, and vector indexing, \eg~\c{x[42]} or
\c{x[[24]]}. As with \c X, we allow arithmetics and values in this form. Lookup
occurs in 10\% of the sites. The interpreter evaluates
\packageMinimizedmedianoperationsgRnd operations on average; the minimum is 3
operations. This is typically used in a function environment,
\packageMinimizedpercentparentframese of the time to be precise.

\medskip\noindent\EE{$min(e)=$~\c{<-}} This form includes both assignment
operators, the direct assignment {\tt <-}, and assignment to the parent
environment {\tt <\,\!<-}, the \c{assign} function and the \c{\$} form. Assignments occur in 5\% of the
sites. They represent the most obvious source of side effects. The median number
of operations is \packageMinimizedmedianoperationsiRnd; the minimum is 3.

\ \\[5mm] % To push the box on the next page
\medskip\noindent\EE{$min(e)=\c{F()}$} This form captures simple function calls,
\eg~\c{f(2)}, with neither variables nor assignments. More specifically, it
allows for variables in the function position but not in arguments. Usually,
looking up function names does not trigger computation, but that is not a given
if the function is returned by a promise or if the function name is shadowed by
a promise containing a value, then the computation will occur. This form occurs in
6\% of sites and typically does not perform much work in the interpreter.

\medskip\noindent\framebox{$min(e)=\c{F(X)}$}~\EE{$min(e)=\c{F(F(X))}$} These
forms allow for function calls whose arguments may include variable references.
The latter allows nested calls. Assignments are excluded from this form. They
occur in, respectively, 14\% and 20\% of the sites in the corpus. Together they
are the most frequent forms. The median numbers of interpreter steps are,
respectively, \packageMinimizedmedianoperationsdRnd and
\packageMinimizedmedianoperationsbRnd. Also,  \packageMinimizedpercentparentframesd and \packageMinimizedpercentparentframesb
of these expressions run in function environments.

\medskip\noindent\EE{$min(e)=\c{FUN}$} This form captures expressions that define functions, \eg~\c{function(x) x+1}, and do nothing else. \c{FUN} occurs in
\packageFunctionDefinitionSitesPercent of sites. Evaluating a function
definition is done in 2 interpreter steps; the data does not record the work
performed by the interpreter when the generated functions are eventually run. In
addition to \c{FUN}, \packageGeneralizedFunctionDefinitionSitesPercent of sites
have function definitions nested in other expressions. This gives an idea of the
use of higher-order functions.

\medskip\noindent\EE{$min(e)=\c{BLOCK}$} This form captures multi-statement
code blocks, which occur in only 4\% of sites. These are huge expressions; we
do not inspect the contents of the blocks. The median number of executed operations
is \packageMinimizedmedianoperationsjRnd. They typically run in function
environments.

\medskip\noindent\EE{$min(e)=\c{model.frame}$} The \c{model.frame} function
returns a dataframe resulting from fitting the model described in a given
formula. This form subsumes \c{F(F(X))}, \c{FUN}, and assignments. It is the
single most popular function invoked from \eval; it occurs in 7\% of the sites.
Each call does quite a lot of work with a 2K instructions median.

\paragraph{Consistency} It is interesting to consider how many different
forms any given site sees. The more forms, the harder it will be to characterize
the program's behavior at that site. Luckily, \packageNbOneMinimizedPercent
of sites only see a single form.  In other words, a JIT compiler could correctly predict the \emph{shape} of an expression by looking just at the first call in \packageNbOneMinimizedPercent of the sites.  Few sites are highly polymorphic (8 or more different forms). These include the pipe operator of the \c{magrittr} package, which is used to compose functions.

\paragraph{Discussion} The variety of uses of \eval is evidenced by the number
of different forms observed in CRAN. In comparison, JavaScript \eval usage was more straightforward and more predictable as reported by \citet{oopsla12b}, with 98.7\% of the sites with only one form. Nevertheless, simple forms dominate in R also, and there are many cases where \eval could be replaced by less powerful constructs.


\subsection{The environments of \eval}\label{sec:env}

Contrary to the Javascript \eval, which does not precise the scope in which to evaluate,the second argument of \eval in R, \c{envir}, specifically is dedicated to that. 
The environment in which evaluation happens determines what is visible to the
computation started by \eval and the potential reach of its side-effects. The
second argument to \eval specifies that environment. Environments of \eval can be
classified into the following four kinds:

\begin{compactitem}[---]
\item \emph{Function:} environments for the local variables of some function
  currently active on the call stack. Obtained by calling \c{parent.frame()} or
  \c{sys.frame()}.
\item \emph{Synthetic:} environments built from data structures such as lists,
  dataframes, or constructed explicitly with \c{new.env}, \c{list2env}, or
  \c{as.environment}. It also includes the empty environment.
\item \emph{Global:}  environments in which scripts or interactive commands
  are evaluated.
\item \emph{Package:}  environments of a loaded library.
\end{compactitem}

\noindent
As shown in Table~\ref{tab:highlevelenvironments}, most calls evaluate in a
function environment with the \c{global} kind as a distant second. This means
that most variable lookups and most side effects either read/update existing
local variables or introduce new ones. But for which function? From the point of
view of the function that called \eval, Table~\ref{tab:funoffset} gives an
offset on the call stack. Thus 0 is the direct caller and corresponds to local variables; 1 is its parent, and so on. The data suggests that in 81\% of cases, \eval accesses its caller's environment -- this means  the variables of the function where \eval textually
occurs are read and written to. It is interesting that some 1.5\% of sites
evaluate a code in an environment that is three frames or above from \eval. This
distance implies that, in general, modular reasoning is impossible. Understanding the behavior of any piece of code entails fully understanding the
behavior of all functions that the code may call as the actions of \eval may
happen at a distance. Finally, note that any particular site may have several
kinds of environments, but in \packageNbOneCategoryEnvirSitePercent of the cases,  a site has a single kind. It means that a JIT compiler can predict the \emph{kind} of  expression passed to \eval in \packageNbOneCategoryEnvirSitePercent of the sites by just examining at the first invocation of a site.

\begin{table}[h]
  \centering\small\hspace{-.5cm}
\begin{minipage}{3.7cm}
  \begin{tabular}{@{}r|r|r@{}}\hline
 Kind & \#sites & \%sites \\\hline
 Function & \packageNbFunctionEnvSites &  \packageNbFunctionEnvSitePercent\\
 Synthetic & \packageNbSyntheticEnvSites & \packageNbSyntheticEnvSitePercent \\
 Global &  \packageNbStrictGlobalEnvSites & \packageNbStrictGlobalEnvSitePercent \\
 Package & \packageNbPackageNamespaceEnvSites & \packageNbPackageNamespaceEnvSitePercent \\\hline
\end{tabular}
\caption{Kinds per site} \label{tab:highlevelenvironments}
\end{minipage}\hspace{-.2cm}
\begin{minipage}{3.7cm}\centering
\begin{tabular}{@{}r|r|r@{}}\hline
 Offset & \#sites & \%sites \\\hline
  \packageCallerEnvHierarchyNamea & \packageCallerEnvHierarchySitesaRnd & \packageCallerEnvHierarchySitePercenta \\
  \packageCallerEnvHierarchyNameb& \packageCallerEnvHierarchySitesbRnd & \packageCallerEnvHierarchySitePercentb \\
  \packageCallerEnvHierarchyNamec& \packageCallerEnvHierarchySitescRnd & \packageCallerEnvHierarchySitePercentc  \\
$\ge 3$& \packageNbFarAwayCallerSites &  \packageNbFarAwayCallerSitePercent \\\hline
 \end{tabular}
\caption{Function offset}\label{tab:funoffset}
\end{minipage}\hspace{-.2cm}
\begin{minipage}{3.7cm}
\begin{tabular}{@{}r|r|r@{}} \hline
Parent & \#sites & \%sites \\\hline
Function & \packageNewEnvCategorySitesa & \packageNewEnvCategorySitePercenta \\
Package & \packageNewEnvCategorySitesb &  \packageNewEnvCategorySitePercentb\\
Global & \packageNewEnvCategorySitesc & \packageNewEnvCategorySitePercentc \\
Empty & \packageNewEnvCategorySitesd & \packageNewEnvCategorySitePercentd \\\hline
\end{tabular}
\caption{Wrapper envs.} \label{tab:newenvs}
\end{minipage}\hspace{-.2cm}
\begin{minipage}{3.7cm}\centering
 \begin{tabular}{@{}c|c|c@{}} \hline
 \#kinds & \#sites &  \%sites \\ \hline
 \packageNbCategoryEnvira & \packageNbCategoryEnvirSitesaRnd &  \packageNbCategoryEnvirPercenta\\
 \packageNbCategoryEnvirb &  \packageNbCategoryEnvirSitesbRnd & \packageNbCategoryEnvirPercentb \\
 \packageNbCategoryEnvirc & \packageNbCategoryEnvirSitescRnd &  \packageNbCategoryEnvirPercentc\\
 \packageNbCategoryEnvird & \packageNbCategoryEnvirSitesdRnd & \packageNbCategoryEnvirPercentd\\\hline
\end{tabular}\caption{Multiplicities}\label{tab:polyenvir}
\end{minipage}\hspace{-1cm}
\end{table}

\noindent
Sites that evaluate in the global environment are likely split between
intentional and accidental use. Direct references to the top-level, using
\c{globalenv()} or \c{.GlobalEnv}, are rare; they occur in only
\packageNbExplicitGlobalSites sites. We suspect that the majority of uses of the
global environment come from the fact that our corpus consists of scripts loaded at the top level. Thus, that environment may often be the caller of
\eval or close to it. The reason we make this point is that values stored in the
global environment are visible to all functions and are not reclaimed by the
garbage collector. So accidental uses may pollute that namespace.

Synthetic environments need a parent. It is specified as an argument of the \c{new.env} function, or when \c{envir} is a list or a data frame, \eval uses its third argument
(\c{enclos}) as the parent.  The parent is used to lookup variables not found in its
child (side effects go to the child only). Table~\ref{tab:newenvs} shows the
kinds of the parents of synthetic environments. Most of them are functions, then
come global and package. 


\paragraph{Discussion}
The data presented in this section is what one could expect. ({\it Function})
The majority of \eval call sites access the environment of some function on the
call stack. Usually, the current function, but sometimes frames arbitrarily far
up the call stack. One thing the data does not say is how that environment was
obtained. The expected case is that the environment was the one obtained from a
promise, as promises combine code and the environment in which that code
originates. Likely less frequent are cases where \eval is provided the results
of programmatically selecting some call stack. ({\it Synthetic}) The relatively
high frequency of synthetic environments corresponds to cases when one wants to
evaluate an expression in either a restricted environment, use a data
structure as an environment. ({\it Global}) The fact that the global environment
shows up so frequently is likely an artifact of how the code is run. ({\it
  Package}) There are only \packageNbPackageNamespaceEnvSites sites that use a %77 sites
 package environment in \eval. This is probably best as it is an improper form of adding bindings or modifying functions of a loaded package. Many more sites instead wrap a package environment in a new
environment to create singletons in a package.

%% The \c{imchange} function of package \c{imager} makes it possible to modify
%% images using a dedicated formula syntax.\footnote{Inspired by {map} in
%% package \emph{purr}.} Here, \eval is evaluated in \c{newenv}, which creates a
%% new environment that inherits from \c{parent.frame()} by default (classified
%% as 1+).
%% \begin{lstlisting}
%%  newenv <- new.env()
%%  ...
%%  fo <- parse(text=as.character(fo)[2])
%%  im[where] <- eval(fo,envir=newenv,enclos=env)
%% \end{lstlisting}

%% \c{adjCoef} in package \emph{actuar} find the root of an equation defined by
%% a function \c{h} whose arguments must be named \c{x} and \c{y}. \c{h} is
%% transformed into an auxiliary function \c{h2} that can be optimized. Here,
%% the list used for \c{envir} ensures the correspondance between the textual
%% arguments of \c{h} and the arguments of \c{h2}.
%% \begin{lstlisting}
%%  sh <- substitute(h)
%%  fcall <- paste(sh, "(x, y)")
%%  ...
%%  h2 <- function(x, y)
%%  eval(parse(text = fcall),
%%  envir = list(x = x, y = y),
%%  enclos = parent.frame(2))
%% \end{lstlisting}

\subsection{The origins of \eval}

Where does the expression passed to \eval come from? There are various means of
creating that expression, which are associated with particular use cases. We
classify them into three categories:

\begin{compactitem}[---]
\item {\it Constructed:} Expressions can be constructed by invoking the
  \c{quote}, \c{enquote}, \c{expression} functions. Function arguments are passed as
  promises, and \c{substitute} is used to retrieve the source expression
  associated with the promise.
  \item {\it Reflection:} This group corresponds to uses of the \c{match.call}
  function to reflectively capture the expression that invoked the current
  function.
\item {\it String:} Finally, expressions can be created from strings by invoking
   \c{parse}, \c{str2expression}, or \c{str2lang}.
\end{compactitem}

\noindent
Table~\ref{tab:provenance} summarizes the expression provenance in our corpus.
This data is obtained by dynamically tainting values as they are produced by the
various sources. Due to technical reasons, there is some imprecision in the
results. In particular, we are not able to classify all sites. Manual inspection
of numerous examples suggests that the classified results are accurate.

\begin{wraptable}{r}{5cm}\small\centering
\begin{tabular}{r|r|r} \hline
Origin  & \#sites & \%sites \\\hline
Constructed & \packageNbConstructedSites & \packageNbConstructedSitePercent \\
Reflection &  \packageNbMatchCallExprsSites & \packageMatchCallExprsSitePercent\\
String & \packageNbStringSites & \packageNbStringSitePercent \\\hline
\end{tabular}
\caption{Provenance}\label{tab:provenance}
\end{wraptable}

Strings could correlate with dynamic code loading. This is what the base
functions \c{source} and \c{sys.source} do. We observed few calls
(\packageNbParseFromFileSites in total) that consume the result of calling
\c{parse} on a file. Most of the calls build strings programmatically. We also
identified one function \c{invokeRestartInteractively} that prompts the user for
input, parses it, and passes it to \eval. The use of strings seems to correlate
with less sophisticated programmers; in our Kaggle corpus,
\kaggleParseExprsSitePercent of sites use strings.

\paragraph{Discussion}

% Compare to JS ? Everything comes from a string in JS anyway
% Composite (concatenation of strings), DOM and constant are the most common
% ...
The origin data suggests that constructing expressions from strings is a
minority of the use cases. Instead, the constructed category shows that most \evals comes from code that was processed by the compiler, and may be
slightly modified by the programmer before invoking \eval. Both constructed and
reflection categories roughly correspond to meta-programming. Some of these use
cases could likely be replaced by macros if the R designers could be convinced
to overcome their distaste for those.

\subsection{Side effects in \eval}

The code executed by \eval can do side effects. From the compiler perspective,
we care about the observable side effects in environments---\ie variable
definitions, updates, and removals that are visible after a call to \eval
finishes. Knowing where---\ie in which environments do these side-effects
happen, can help us determine how much of the compiler knowledge about the
program will be potentially invalidated.

The \rdyntrace contains low-level hooks capturing the life-cycle of an
environment and variables defined in it. We register callbacks in the tracer
that records each environment creation and removal with updates to its
bindings. From the recorded data, we ignore side effects coming from \eval
sites from unit testing frameworks.\footnote{In the corpus, we have \c{RUnit,
testthat, tinytest} and \c{unitizer} unit testing frameworks.} They run the
testing code via \eval, and thus the results would be biased because of the
high number of code in these tests (\CranRunnableCodeTestsRnd lines).

From the \packageNbrunsRnd programs, we capture \SEAllRnd side effects from
\SEAllCallsRnd \eval calls (\SEUserCallsToAllRatio) in \SEAllSites sites
(\SEUserSitesToAllRatio). Again, the challenge is to remove the accidental
side effects caused by the R virtual machine implementation that are not
related to the user code. For example, the \c{.Random.seed} variable, which
contains the state of the random number generator, is saved and restored from
and to the global environment every time the user calls one of the base
routines that use random numbers.
%
% \begin{wrapfigure}{r}{5cm} \hspace*{-20mm} \centering
% \includegraphics[width=90mm]{se-types.pdf} \caption{Distribution of R types
% in target environments for side effects.} \label{fig:se-types}
% \end{wrapfigure}
%
Removing them leaves us with \SEUserRnd side effects (\SEUserRatio) from
\SEUserCallsRnd \eval calls (\SEUserCallsRatio) in \SEUserSites sites
(\SEUserSitesRatio). These sites are part of \SEUserFunctions R functions
(\SEUserFunctionsToAllRatio of all functions doing \eval) in \SEUserPackages
packages. From these functions, \SEFunsNighty are responsible for 90\% of side
effects. Half of the side-effects come just from three functions:
\c{plyr::allocate\_column} (allocates space for a new data frame column),
\c{withr::execute\_handlers} (executes deferred expressions), and
\c{foreach::doSEQ} (executes an expression on each element in a collection,
possibly in parallel). Most of the \eval sites (\SESitesInEnvirRatio) do side
effects in the environment specified by the \c{envir} parameter (\cf
Section~\ref{sec:eval-in-r}), \SESitesNotInEnvirRatio modifies other
environments, and finally, \SESitesBothEnvirRatio does both.
Table~\ref{tab:se-env} shows the class of the environment where \eval side
effects happen (\emph{target environment}). It follows the classification
described in Section~\ref{sec:env} with two extra classes: \emph{Object},
which represents a side-effect that happens in S4 or R6 object environments, and
\emph{Local} that corresponds to a side effect in local variables of the \eval
caller (offset 0 in Section~\ref{sec:env}).  The table shows data for both \eval sites and functions. For a
function to have a given target environment class, all of its \eval sites that
do side effects must have the same target environment class The majority of the
\eval sites (\SESitesInOneClass) do all side effects consistently in one
environment class. The same happens at the function level. Almost half of the
sites (over a third of the functions) do side effects in either \emph{Local} or
\emph{Object} environments. This gives a ray of hope for the compiler. Even
though it is possible to do anything anywhere, the data suggest that most
side-effects are sane.

\begin{wraptable}{r}{7cm}\small\centering
  \small
  \centering
  \begin{tabular}{l|r|r|r|r}\hline
    Environments & \#sites & \%sites & \#funs. & \%funs. \\%
    \expandableinput tag/table-se-target-envs.tex
  \end{tabular}
  \caption{Target environments for \eval side-effects} \label{tab:se-env}
\end{wraptable}

Table~\ref{tab:se-types} shows the proportion of the different side effects
that were recorded. In terms of calls, we see assignments primarily and in
terms of site definitions. This is expected. A subsequent \eval call will turn
a definition into an update. The most dangerous side effect is variable removal,
as it means that after a call to \eval some binding in some environments will
disappear. While rare, this happens, but the vast majority comes from a single
site called by the already mentioned \c{withr::execute\_handlers} function.
This makes sense as it is used to defer evaluation of an expression to after
the function exit and thus used for clean up. In the corpus, it is used almost
exclusively by the \c{tidyselect} package for removing the reference to the
current quosure environment while interpreting a data frame column
selectors.\footnote{\cf \url{https://tidyselect.r-lib.org/}}

Looking closely at the value types in variable updates, we observe that the
majority of \eval sites involve basic R vectors (\SEBasicTypeRatio) and lists
(\SEListTypeRatio). From the compiler perspective, we would like to know how
many sites changes function bindings. In the corpus, this happens in only
\SEClosureType sites; \SEClosureTypeLocal of them do that in the local
environment. We have manually inspected a few of these sites, but except for
manual injection of parameters into a \c{model.frame} execution environment, we
did not find a common use case.

% TODO do assignments change types?

\begin{table}[h]
  \small
  \centering
  \begin{tabular}{l|r|r|r|r|r|r}\hline
    Side effect & \#events & \%events & \#calls & \%calls & \#sites & \%sites \\%
    \expandableinput tag/table-se-types.tex
  \end{tabular}
  \caption{Types of \eval side-effects} \label{tab:se-types}
\end{table}

\mypara{Discussion} In JavaScript assignments in \eval can happen in either
local scope or less often, when called through an alias, in the global scope.
In R, given the support for first-class environments, it can happen anywhere, 
making it \eval more dangerous than it already is. However, the data  suggest
that first, side-effects from \eval are not as widespread as in the case of
JavaScript, \footnote{\citep{ecoop11} shows that in the \emph{Interactive} scenario, \eval in Javascript performs, store events can reach up to 40\% of the events, and 7\% to 8\% of the \eval do side effects in the global scope. } and that over half of them happen in a predictable environment.

\section{Usage of \eval}

The R language was intended to be extensible. The combination of lazy
evaluation, \c{substitute} and \c{eval} are the tools given to developers to
this end.
%This API is slightly more complex than just passing a string, it is
%conceivable that this may discourage some casual users. \Eval is also being
%used to reduce boilerplate code and provide convenience features for
%programmers. 
In this section we focus on the \emph{why}, giving real-world examples of the
\eval usage in the wild. We start the discussion with the examples for the
minimized expressions from Section~\ref{sec:minimized}. They we look into
high-level \eval design patterns. All of the examples are based on code from
our corpus often simplified to fit the space and increase clarity. For each
example, we indicate the package and the function where it is located.

\subsection{Shapes of \eval}

In Table~\ref{tab:minimizedexpressions} we have shown the most common shapes of expressions passed to \eval.
Here we illustrate them on concrete instances.

\begin{compactitem}[---] 

  \item \emph{Variable lookup and values and indexing \c{(X, V, \$)}}
    constitute one of the most common use case for \eval. Let's consider the
    \c{subset} function from R base library which allows, among others, to
    select variables in a data frame. For example, given a data frame \c{df}
    with three columns \c{A,B,C}, \c{subset(df, sel=A)} returns a subset of
    \c{df} with just the \c{A} column. The same would happen with \c{subset(df,
    sel=1)}. However, thanks to \eval it can also be also called with
    \c{subset(df, sel=c(1,B,x))} returning a subset of the first column, column
    \c{B} and whatever index the variable \c{x} holds. It is implemented using:
    \lstinline|eval(substitute(sel), nl, parent.frame())| The first argument
    returns the expression passed to \c{sel}, the second is the data frame in
    which this expression will be evaluated and finally the last indicates
    which environment should be used to look up bindings not found in \c{nl}.

    Another example is the most often called \eval site in CRAN:
    \lstinline|eval(`_inherit`, env)| defined in the \c{find\_super} function
    in \c{ggproto} object system. It looks up the variable \c{`\_inherit`} to
    find the parent class. In the vast majority of cases the variable is symbol
    and so the \eval seems superfluous. The \eval is however necessary so one
    can specify parent classes using \c{pkg\_name::class\_name} as the \c{::}
    operator is a function call in R.

  \item \emph{Assignments \c{(<-)}} appears in the cases where a developer wants
    to create or update a biding in a specific environment where either the
    name of the value come from some expression. Most cases come from trivial
    code generation where the assign expression is assembled using \c{parse} or
    \c{substitute}, often in a loop: \lstinline|eval(parse(text=paste(capture.output(default_args[[i]]), "<-", i)))|. 

    There are also more sophisticated cases. The \c{plyr::allocate\_column}
    function uses \eval for a deferred assignment to fill missing values in a
    column. Package \c{overture} repeatedly evaluates R expressions saving any
    assignments that happen in it, as samples of Markov chains.
    
  \item \emph{Function definition \c{(FUN)}} is mostly used in conjunction with
    code generation where new functions are synthesized using either \c{parse}
    or \c{substitute}. Some FFI frameworks use this to automatically generate R
    binding to native code. For example, \c{Rcpp}, which bridges R with C++,
    uses \eval to generate R function for C++ methods:\\
    \lstinline|eval(substitute(function(...) .CppObj$M(...), list(M=...)), env)|.

  \item \emph{Function calls \c{(F(), F(X), F(F(X)))}} 


  \item \emph{Block \c{(BLOCK)}} can appear anywhere, but it is almost always
    the case that the block is only passed to \eval rather than being directly
    part of in as in \lstinline|eval({...})|. Essentially, the block denotes a fragment
    of a program to be evaluated in a special way. There is a number of use
    cases: unit testing frameworks (\eg \c{testthat}, \c{testit}), code
    benchmarking (\eg \c{rbenchmark}, \c{microbenchmark}), running code in
    parallel (\eg \c{foreach}, \c{doParallel}), or deferring code execution
    (\eg \c{withr}). 


\end{compactitem}

\subsection{High-level patterns.}

We present here high-level patterns that we identified after manual inspections of hundred of \eval sites. Some of the patterns are also described in the R documentation.

\subsubsection{Domain Specific Language.} R expressions are built and transformed symbolically and then evalled to a concrete value. For instance,   base R packages \emph{stats} provides two operators for \emph{symbolic differentiation}, \c{D} and \c{deriv}. They support arithmetic operations and functions on real numbers such as \c{sin}.
In \emph{MCMCglmm} (Monte Carl Markov Chain Generalised Linear Mixed Models), a \c{Dtensor} object is created, which contains expressions derived twice (with \c{DD}) and then typically in a list or a data frame.
\begin{lstlisting}
 expr<-expression(beta_1 + time*beta_2+u)
 mu = data.frame(beta_1=0.5, beta_2=1, time=3, u=2.3)
 D[i]<-eval(DD(expr, name[unlist(comb.pos[i,])]), mu)
\end{lstlisting}
% See https://github.com/cran/MCMCglmm/blob/cfac9e67ec73a3db2142826faae35e5d2318da31/man/evalDtensor.Rd

Package \c{tidyselect} is the backend of several \c{dplyr} and \c{tidyr} functions to select columns. It offers a DSL that makes it possible to select a set of columns, over a range of columns, a complement, an intersection, an union, with a regular expression and so on. \eval is used to traverse selectors stored as slots of a dataframe.


\subsubsection{Capturing a call.}  A common use case for \eval is to be combined with \c{match.call}.
\c{match.call} walks up the call stack, captures the code that invoked the
currently executing function, and returns it as an unevaluated expression that includes the call function and its unevaluated arguments. The pattern is to transform a call to some function \c{f} into a call to \c{g} with some arguments retained and others modified. This pattern is recognizable
by the fact that the expression is a call and the target environment is that of
the parent.

% We have quantitative data on that if needed...
This pattern is very often (more than half of the cases) used in relation with statistical models, to build a new model, \c{model.frame} to build a new generic model, or a more specialized model like \c{glm} or, \c{lm}. The R documentation of \c{match.call} explicitly describes this usage.

In package \emph{survival}\footnote{It is a \emph{recommended package}, so recommended by the CRAN maintainers, thoroughly tested, and following R best practices.}, in function \c{coxph}, the surrounding call is first captured, then \c{model.frame} is injected as the function name in the call, and finally, the new call is evaluated in the parent frame, to ensure that the rewritten call
is evaluated in the same environment the original call was.

\begin{lstlisting}
Call <- match.call()
tform <- Call[c(1,indx)]  # only keep the arguments we wanted
tform[[1L]] <- quote(stats::model.frame)  # change the function called
mf <- eval(tform, parent.frame())
\end{lstlisting}

A related pattern is not to change the call but rather to evaluate the arguments in the parent frame. This could also be achieved with \c{substitute}, but \c{match.call} does it in bulk.
In package \emph{MedDietCalc},  in function \c{computeMDS95}, \c{match.call} retrieves the argument expressions of the call, which are evaluated in a dataframe \c{data} which contains the numeric data. Each argument of function \c{computeMDS95} is also a column in dataframe \c{data}. The advantage of using \c{eval} here is to be able to express an argument as a combination of several columns in the dataframe.
\begin{lstlisting}
computeMDS95 <- function (data, Vegetables, ...) {
	arguments <- as.list( match.call())
	Vegetables <- eval(arguments$Vegetables, data) 
\end{lstlisting}

\c{substitute} can be used to capture only a part of the call, \ie one of the arguments. For instance, in package \emph{xyplot}, the expression for argument \c{subset} is captured with \c{substitute} and then evaluated in a dataframe \c{data} and in the definition environment of \c{x}.
\begin{lstlisting}
xyplot.formula <-  function(x,    data = NULL, ..., subset = TRUE)
subset <- eval(substitute(subset), data, environment(x))
\end{lstlisting}


In the R community, the call capturing pattern with evaluation in a non-local environment is often referred~\cite{hadley} as \emph{Non-Standard Evaluation}. This pattern in relation with \c{model.frame} is also described explicitly in the R documentation of \c{match.call}.

\subsubsection{Injecting code in package environments.} R distinguishes two kinds of environments for packages: the \emph{package} environment itself, which contains every publicly accessible functions, and the \emph{namespace} environment, which contains all functions including private ones.

 A call to \c{library(packageName)}, which loads libraries in R, for instance, will
populate environment \c{package:packageName} using \eval. It is a \emph{base} function
though, so we do not see it in the CRAN dataset. However, R still makes it
possible to access any package environments, which we see in
\packageNbPackageEnvPackages different packages in our corpus. Those packages perform
introspection, to represent and search any environment by name, in package
\emph{envnames}, to containerize tests, as with \emph{testthat} and
\emph{unitizer}, to create a custom OOP system with \emph{R.oo}, or to bypass the visibility of functions in \emph{USSCXenaTools}. It is also used to
implement a custom caching mechanism in packages \emph{g.data} and \emph{SOAR}.

\subsubsection{Code Generation.} The more traditional use of \eval is to execute code
that was assembled by the programmer into a string. Here we show the method
\c{plot} for class \c{gmap} in package \c{gpmaps}, simplified for
explanatory purposes.   \c{paste} is used to dispatch to a specialized plot function, among \c{plot2\_orig},  \c{plot3\_orig}, \c{plot3\_dec} and so on.
\begin{lstlisting}
eval(parse(text=paste('plot',gpmap$nloci,'_orig(gpmap)',sep="")))
\end{lstlisting} 

In Kaggle, plots are often built with \eval, \c{paste} and \c{ggplot}. In this kernel for the \emph{digit-recognizer} challenge, \c{paste0} is used to generate \c{ggplot} commands featuring unique y labels to build 9 different graphs.

\begin{lstlisting}
p <- lapply(paste0('PC', 1:9), function(pc){
	eval(parse(text=paste0('ggplot(ld, aes(x=label, y=',pc,')) + geom_boxplot()')))
})
\end{lstlisting}

The \parse and \eval combination is
used by the \source function in the base library to load R code from a file
in the current workspace. 

 Code generation is often used as a means to reduce boilerplate code;
simple and repetitive code can easily be replaced with judicious use of \eval.
For example, the \c{data.table} package uses \eval to calls the \c{options}
function with named arguments taken from a vector of strings. While the benefits
are limited in this example, it is an attractive tool for programmers.
\begin{lstlisting}
  opts = c("datatable.verbose"="FALSE", # ...many others
  for (i in names(opts))
    eval(parse(text=paste0("options(",i,"=",opts[i],")")))
\end{lstlisting}
This pattern is a special case of code generation, recognizable by the
fact that \eval is executed in a loop. 

Using \c{parse}, \ie converting strings to R expressions, opens to a more flexible code generation, as the building blocks which are going to be concatenated together, do not need to be valid R expressions. On the contrary, combining expression created with \c{substitute} or \c{quote} requires more discipline. In Kaggle, which features less experienced programmers,  code generation for plots is widespread, and uses \c{parse} on strings with \eval.

\todo{There is an example in package comprenhr that does code generation with substitute to create list comprehensions, but it is quite difficult to grasp. But at the same type, we lack here an example of code generation using quite, enquote or substitute.}

\todo{Add a pattern about Library Loading? \c{library(packageName)} in R uses \eval This is one of the patterns in Javascript...}

\subsubsection{Unnecessary use of \eval}

Similarly to JavaScript, there are also unnecessary uses of \eval. For example,
the \c{PerformanceAnalytics} package contains a function \c{chart.QQPlot} that
uses \eval to resolve a string into function and another to call it and assign
its results into a variable:
\begin{lstlisting}
function (R, d="norm", dp, ...) {
	q.f <- eval(parse(text=paste("q",d,sep="")))
	z <- NULL
	eval(parse(text=paste("z<-q.f(",dp,",...)")))
}
\end{lstlisting}
  In both cases, there is no need for \eval:
\begin{lstlisting}
function (R, d="norm", dp, ...) {
	q.f <- get(paste0("q",d))
	z <- q.f(dp, ...)
}
\end{lstlisting}
or even to a oneliner \c{do.call(paste0("q",d), as.list(dp, ...))}.

% \todo{Aviral's story when he contacted a developer about his use of eval}

    A peculiar use of \eval is to bypass some checks performed by the \c{R CMD
      CHECK} tool allowing one to submit a package to CRAN that would otherwise
      be refused. For example, the tool checks that packages do not call
      \c{unlockBinding} function.\footnote{It allows one to open and modify
      loaded package name space.} Packages such as \c{data.table} circumvent
      such restrictions by using \eval as the check is only performed
      statically.

\mypara{Discussion.} \Eval in R is mostly used for meta-programming purposes and to access environments other than the local one. In Javascript, patterns are ad-hoc, such as json loading and parsing, and can often be rewritten without \eval, such as function or method call, or assignment to a local variable or an object property. Compared to Javascript, \eval patterns in R we observed are much more legitimate, although this is biased by the fact that our main corpus CRAN features more experienced developers than  websites found at large on the web. \todo{can we really claim that? SHould we talk about Kaggle there?}

\eval is a powerful tool and can be sometimes replaced with functions with more specialized goals. For instance, variable lookup in any environment can be performed with \c{get}, and assignment in any environment, with \c{assign}. Building a call and executing can be done with \c{do.call}. Nevertheless, \eval shines when expressions passed to it can be of many different types. % And it is shorter to type, and no need to remember all the specialized functions, so it is convenient.

\section{Conclusion}

In this paper, we provide a large-scale study of the use of the \eval function
in the R programming language. It is based primarily on a corpus of
\CranRunnableScripts scripts extracted from \CranPackages R packages. As
expected, \eval is used mainly in libraries. We have found just a few \eval
call sites in scripts coming from large corpus of Kaggle that represents code
written by R practitioners. We used an instrumented version of the R virtual
machine, \rdyntrace, to capture all \eval calls traces including the relevant
side effects that happen within these calls. Several insights can be learnt
from the data.
%
First, \eval is widely used in the R core packages that come bundled with the
language. Because of that, there is hardly any R code that would not use it.
\eval is used to implement some basic functionality such as package loading.
Putting the core packages aside, in CRAN, \eval is used by \PkgPackagesRatio of
the available \CranPackages packages.
%
The code executed by R \eval has unusual expressive power. It can essentially
modify anything anywhere which is wary for both R developers and R compiler. The shapes of expressions passed to \eval are extremely diverse, ranging from simple variable lookups to complex blocks or creation of new functions. Luckily, most site only see a single shape. \eval unlashes its power and shows up its singularity compared to \eval in other languages with the diversity of environments in which it can evaluate its expressions. While most of the time, the \eval environment is the local environment, it is also often the global environment, a synthetic environment, or even a package environment. Luckily, as for the expressions passed to \eval, the environment passed to \eval for one given site is easily predictable with the first call to the site.

The expressive power of \eval shines even more  so in the case of side effects that can quite literally happen anywhere. The data however suggest that this is not the case. The majority of eval sites do not do any observable side effects and when they do, it happens in a predictable environment.

The origins of the expression passed to \eval, either constructed from language expressions, or stemming from reflection, or built by parsing strings. Reflection and constructed expressions dominate, which suggest the main use of \eval is meta-programming. % macros...

% how much can we say about the threat model?
Similarly to \citep{ecoop11}, we started looking into \evals in R with the
ambition to replace them by other features that are more amenable to static
analysis. The data unfortunately suggests that replacing \eval is going to be
a lengthy journey. Even if we disregard core packages as they are part of the
language and the \eval use is stable, in CRAN \eval is used widely and in more
sophisticated way than was the case in JavaScript. There the vast majority of
sites fitted an extremely simple categorization based on simple regular
expressions. Not so in R. While some usage, especially the one which calls
\eval with an expression parsed from string, can be categorized and often
replaced by equivalent yet more disciplined and safer code, the case of \eval
coming from reflection is much harder. 

Even though we reckon that \eval has more expressive power than macros, as it can access runtime information such as its call stack, adding a macro system to R to replace \eval for all meta-programming purposes would help static analysis as macros can be expanded at analysis-time.

% It's dubious whether R programmers would develop with macros though.
% Macros and R
% The Thomas Lumley's article, Programmer's Niche: Macros in R, (https://www.r-project.org/doc/Rnews/Rnews_2001-3.pdf) describes how to write a `defmacro` function that would like as a macro creator, using `substitute` and `eval`.  There are no local macro variables in this implementation though.  

\bibliography{bib/bibliography,bib/jv}

\end{document}
