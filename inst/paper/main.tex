\documentclass[USenglish,cleveref, autoref, thm-restate]{lipics-v2019}
\usepackage{listings,hyperref,multirow,paralist,xspace,url}
\input{macro}

\title{Why do we Eval?\\[2mm]\Large A large-scale study of Eval usage in R}
\titlerunning{Why Eval (in R)?}

\begin{document}\maketitle

\begin{abstract}
  \noindent Most dynamic languages allow users to turn text into code
  using various functions, often named \eval, with language-dependent
  semantics. The widespread use of these reflective functions hinders
  static analysis and prevents compilers from performing many
  optimizations. This paper aims to provide a better sense of why
  programmers use \eval.  Understanding how \eval is used in practice
  is key to finding ways to mitigate its negative impact. We have
  reasons to believe that reflective feature usage is language and
  application domain specific. In this paper, we focus on data science
  code written in R, and compare our results to previous work that
  analyzed web programming in JavaScript.  We conduct a large-scale
  study of a corpus of \CorpusAllCodeRnd lines of libraries and
  end-user code on which we performed dynamic analysis to confirm that
  \eval is indeed in widespread use. We found that \eval is more
  dangerous in some ways, and safer in others, than what was
  previously reported for JavaScript.
\end{abstract}


\section{Introduction}

Most dynamic languages provide their users with a facility to
transform unstructured text into executable code, and to evaluate that
code. In this paper we refer to this reflective facility of a language
as \eval bowing to its origins in LISP in 1956~\cite{lisp}. \Eval has
been much maligned over the years. In computing lore, it is as close
to a boogeyman as it gets. Yet, for McCarthy, \eval was simply the way
to write down the definition of LISP, he was surprised that someone
coded it up and offered it to end users.  Since then, reflective
facilities have been used to parameterize programs over code patterns
that can be provided after the program is written.  The presence of
such a feature in a language is a hallmark of dynamism as it is a form
of delayed binding as what any particular call to \eval will do is
only know at program run-time when that particular call site is
evaluated.

\vspace{2mm}\noindent\emph{Trouble in Paradise.} Reflective facilities
hinder most attempt to reason about the code using them, as well as to
apply meaning-preserving program transformations. In practice this
means that static analysis techniques will loose so much precision as
to become pointless and that anything but the most trivial, local,
compiler optimizations will be unsound. Furthermore the addition of
new code --- code that could have been obtained from a network
connection --- as the program is running is a security vulnerability
waiting to happen.

To further illustrate the challenges at hand, consider the interaction
between a static analysis tool and a dynamic language. An abstract
interpretation-based program analyzer computes an over-approximation
of the set of possible behaviors exhibited by the system under
study~\cite{cc77}. The presence of a reflective operation entails that
the operation may have \emph{any} behavior that can be expressed in
the target language; i.e. as if we replaced it by any legal sequence
of instructions. As dynamic languages tend to be permissive, the
analysis has to, for example, assume that many (and sometimes all)
functions in scope may have been redefined, e.g. that \texttt{`+`} now
opens an Internet connection or something equally surprising. This
means that a single occurrence of \eval causes the static analyzer to
loose all information about the program state and the meaning of
identifiers. This loss of precision can sometimes be mitigated by
analyzing the string passed to \eval~\cite{} to bound its possible
behavior but when the string comes from outside the program not much
can be done. A frustrated group researchers argued giving up on
soundness and, instead, under-approximating dynamic features
(soundiness)~\cite{soundy}. In their words ``a practical analysis,
therefore, may pretend that \eval does nothing, unless it can
precisely resolve its string argument at compile time.'' Alas,
assuming that \eval does not have side-effects, or that side-effects
will not affect the results of the analysis, may be unduly optimistic.

\vspace{2mm}\noindent\emph{Is Past Prologue?} Previous work
investigated how \eval is used in web programming, specifically in
websites that use JavaScript~\cite{pldi10a}. In 2010, 17 of the
largest website used the feature. In 2011, 82\% of the 10,000 most
accessed sites used \eval~\cite{ecoop11}.  Yet, the strings passed to
\eval, and their behaviors when executed, are far from random; it was
shown that when one can observe several calls to \eval, the ``shape''
of future calls can be predicted with 97\% accuracy~\cite{oopsla12b}.
Overall, practical usage suggested that most reflective calls were
relatively harmless. While this backs up the soundiness squad's
approach, does it generalize to other application domains than web
programming and to other languages?

\vspace{2mm}\noindent\emph{The here and now.}  In this study, we
investigate the usage of \eval in programs written in the R
programming language. R is language designed by statisticians for
applications in data science~\cite{r,R96}. What makes looking at R after
JavaScript interesting is that, while both languages are dynamic, they
are quite different. While one can program in an object-oriented style
in R, like in JavaScript, R is primarily a lazy, untyped, functional
language.  JavaScript was designed to run untrusted code in browser,
while R is used for statistical computing on desktops. JavaScript is a
general purpose language used by a wide community of programmers;
while R is used for scientific computing by data scientists and domain
experts with, often, limited programming experience. One can
distinguish between library implementers, developers with some
programming experience and a working knowledge of R, and end-users,
who are typically not expert programmers and often have only a cursory
knowledge of the language.\footnote{Consider that R evaluates function
argument lazily, just like Haskell. We informally surveyed end-users,
including computer scientists, and did not find a single user aware of
this fact. Library developers, on the other hand, know about laziness
and program defensively around it.} Our goal is thus to highlight the
differences in usage between JavaScript and R, and try to explain
those differences in terms of language features, application domain
and programmer experience.

\vspace{2mm}\noindent\emph{The What and How.}  One significant benefit
of choosing R is that every package in the CRAN repository is curated
and comes with examples of typical usage. This gives us a large code
base that we can analyze dynamically.  To observe \eval we built a
two-level monitoring infrastructure: we can monitor R programs by
instrumentation --- this gives us access to many user-visible properties
of R programs --- but we can also monitor the inner working of the R
interpreter --- this allows us to capture details of execution that are
not exposed at the source level. Dynamic analysis is of course limited
in that it can only observe behaviors that are triggered by the
particular inputs passed to the program. Luckily, CRAN libraries come
with multiple input datasets and use-cases.

The choice of corpus is crucial for analyses like ours.  Our corpus
has been constructed to reflect the levels of sophistication of the R
community.  We distinguish between \emph{core packages} (there are
\CorpusCorePackages such packages that are distributed together with
the R virtual machine), \emph{CRAN packages} (\CorpusPackages curated
packages that pass stringent quality checks and are equipped with
tests and sample data), \emph{Kaggle scripts} (\CorpusFinishedKaggle
end-user written programs that performs a particular data analysis
task). It is reasonable to expect that \eval usage differ between
these datasets. The core R code is extremely stable, maintained for
the last 20 years with very few far reaching changes. The libraries
represent a more lively ecosystem with new libraries added each day.
Finally, end user code is often one-use scripts thrown together and
sometimes never revisited.

Our infrastructure is publicly available, released in open source, and
our code, and data will be submitted for artifact evaluation.

\section{Background and Previous work}

This section provides a short introduction to R as it has a few
surprising features that impact the use of reflective features of the
language. We then look at the semantics of \eval in R and discuss some
design choices. Lastly, we put this paper in context of previous work.

\subsection{R in a nutshell}

R is a lazy functional programming language~\cite{ecoop12} with
dynamic features that allow to write object-oriented code. Most data
types are vectorized. Values are constructed by the \c{c(...)}
function, e.g. \c{c("a","bc")} creates a vector of two strings. To
enable equational reasoning, R copies values accessible through
multiple variables when they are written to. Values can be tagged by
user-defined attributes. For instance, one can attach the attribute
\c{dim} to the value \c{x<-c(1,2,3,4)} with \c{attr(x,"dim")<-c(2,2)}.
This causes arithmetic functions to treat \c x as a $2 \times 2$
matrix. Another attribute is \c{class} which can be bound to a list of
names, e.g., \c{class(x)<-"human"}. This sets the class of \c{x} to
\c{human}; classes are used for object-oriented dispatch. Every R
linguistic construct is desugared to a function call, even control
flow statements, assignments, and bracketing. Furthermore, all
functions can be redefined in user code. This makes R both flexible
and challenging to compile~\cite{dls19}. Arguments to user-defined
functions are bundled into thunks called \emph{promises}. Logically, a
promise combines an expression's code, its environment, and its value.
To access the value of a promise, one must force it. Forcing a promise
triggers evaluation and the computed value is captured for future
reference.

\textit{Environments.} In R, environments can be accessed, manipulated and created at will. Each function has a corresponding environment, which inherits from its calling function. In the scope of the function, it can be accessed with \c{environment()}.  A new environment, which by default inherits from the current environment, is created with \c{new.env}. Contrary to other values, environments in R follow a \emph{reference semantics}, not a \emph{copy semantics}. The chain of inheriting environments can be climbed up with \c{parent.env}, and ultimately, one arrives at \c{emptyenv}, the empty environment. Functions such as \c{parent.frame} and \c{sys.frame} also make it possible to access environments further up in the calling stack, up to the global environment \c{.GlobalEnv}, enabling \emph{dynamic scoping}.
One can also directly read, modify or create new bindings in any environments. Given an environment \c{e}:

\begin{itemize}
	\item  \c{e$v} or \c{get("v", envir = e)} reads variable \c{v} in \c{e};
	\item \c{e$v <- 2} or \c{assign("v", 2, envir = e)} writes value 2 in variable \c{v} in \c{e}. \c{v} is created if it did not exist in \c{e}. %if argument inherits = TRUE, then if v exists in a parent environment, that binding is modified, instead of creating a new one
\end{itemize}

It means that for instance, one can directly modify a variable in the scope of its calling function, and there is even a special assignment operator for that purpose, \c{<<-}, \ie \c{v <<- 2} will search for a definition of \c{v} in all the parent environments, redefine the value, or create a new definition in the global environment.
Explicit environments are used in R as hashtables, or to maintain state across function calls, or to create package namespaces.

\subsection{Eval Semantics}

The expressive power of \eval depends on a number of choices made by
the programming language designers. The two axes that are the most
relevant here are:
\begin{itemize}
\item {\bf Scoping:} What environment does the string passed to \eval
  execute in? JavaScript and R evaluate the expression in the current
  scope. This means that local variables and parameters are exposed to
  \eval, breaking the abstraction boundary of the enclosing function.
  Julia takes a more restrictive approach by running \eval at the
  ``top level'', i.e. in the global environment, outside of any
  function's environment. JavaScript has a global \eval that behaves
  like Julia, it also has a ``strict mode'' in which \eval is allowed
  to access local variables, but fobidden from injecting new variables
  in the local environment. In Java, one can implement \eval but the
  expression will execute in new environment.\footnote{While \eval
  used to be available only in interpreted languages, the advent of
  just-in-time compilation gradually lifted this restriction. To
  implement \eval in Java requires wrapping its input in a static
  method of an anonymous class, generating bytecode for that class,
  invoking the class loader to install the code, and then using
  reflection to call the new method. }
\item {\bf Reflective API:} What other reflective operations are
  exposed to user code? JavaScript does provides few reflective
  functions (other than being able to enumerate the properties of an
  object and to access properties with strings), the string passed to
  \eval is thus limited to what can be expressed in JavaScript. R, on
  the other hand, has a rich reflective interface. For example, R
  allows user code to walk the call stack and arbitrarily add and
  delete local variables in any environment.
\end{itemize}

\noindent
These design choices impact our ability to reason about code. More
expressive power for \eval translates directly into additional
restriction for program analysis and transformation tools. The reason
Julia has restricts \eval to execute in top level environment is that
they wanted to preserve their compiler's ability to optimize
functions~\cite{oopsla18a}. What they wanted to prevent was the case
where \eval changed the type of local variables as this would require
recompilation of the method (or prevent optimization). Julia even adds
a versoning mechanism, called world age, to ensure that code added
during an \eval does not invalidate optimizations such as
inlining~\cite{oopsla20a}. More permissive languages such as R are
much harder to deal with from a compiler's perspective.

\medskip\noindent{\it Eval in R.} There are four reflective functions
that take abstract syntax trees (called expressions), parse it as
code, and evaluate it. These functions are \c{eval}, \c{evalq},
\c{eval.parent}, and \c{local}. Unlike other languages, R's \eval does
not take a string, but rather an expression. As this is slightly more
involved, it may discourage some casual users. There are two ways to
obtain an expression. Given a string, the \c{parse()} function retuns
an unevaluated expression. The other way is to use the function
\c{substitute(e,envir)}, it yields the parse tree of the expression
\c{e} after performing substitutions defined by the bindings in the
\c{envir} parameter.

\begin{lstlisting}
 > substitute(expression(a+b), list(a=1)))
 expression(1+b)
\end{lstlisting}

\noindent
This function is widely used in conjunction with \eval, as it can
leverage R's laziness for metaprogramming. R allows programmatic
manipulation of parse trees, which are themselves first class objects.
Expressions are evaluated using \eval (or one its variants). The \eval
function takes three arguments:

\begin{lstlisting}
 eval(expr,
      envir = parent.frame(),
      enclos = if(is.list(envir) || is.pairlist(envir))
               parent.frame() else baseenv())
\end{lstlisting}

\noindent
This code is part of the definition of \eval, it specifies that the
function takes an \c{expr} which is the value to be evaluated,
\c{envir} is the environment in which evaluation happes, and
\c{enclos} is relevant when \c{envir} is a list or a data frame, then
it specifies where R looks for objects not found in \c{envir}. This
can be \c{NULL} (interpreted as the base package environment,
\c{baseenv()}) or an environment. Thus, the expression \c{expr}
evaluates in the environment \c{envir} and returns the computed value.
If \c{envir} is missing, then the default is \c{parent.frame()} (the
environment where the call to \eval was made). Values to be evaluated
can be of types \c{call} (function calls) or \c{expression} (arbitrary
expressions), \c{name} (when the symbol is looked up in the current
scope and its binding is evaluated), promise, or any of the basic
types such as vectors and environments (which are returned unchanged).

The \c{evalq} form is equivalent to \c{eval(quote(expr), ...)}.
Quoting that argument prevents if from being evaluated in the current
environment. \c{eval.parent(expr, n)} is a shorthand for \c{eval(expr,
  parent.frame(n))}. \c{local} evaluates an expression in a local
environment. It is equivalent to evalq except that its default
argument creates a new, empty environment. This is useful to create
anonymous recursive functions and as a kind of limited namespace
feature since variables defined in the environment are hidden.



The second substantial difference is the control that R gives over the
choice of environment in which the AST is evaluated in. Beside
traditional choices such as the top level or current lexical
environment, R allows programmers to choose a fresh, empty,
environment or any environment on the call stack. This is a
significant increase in expressive power as functions that are
otherwise not aware of the use of \eval may be deeply impacted.


\subsection{Previous Work}

Richards et al.~\cite{ecoop11} provided the first large-scale study of
the runtime behavior of \eval in JavaScript. They dynamically analyzed
a corpus of the 10,000 most popular websites with an instrumented web
browser to gather execution traces.  They show that \eval is pervasive
with 82\% of the most popular websites using it. The reasons for its
use include the desire to load code on demand, deserialization of JSON
dataq and lightweight meta-programming to customize web pages.  While
many uses were legitimate, just as many were unnecessary and could be
replaced with equivalent and safer code.  They categorized inputs to
\eval so as to cover the vast majority of input strings.  Restricting
themselves to \eval in which all named variables refer to the global
scope, many patterns could be replaced by more disciplined
code~\cite{oopsla12b}.  The work did not measure code coverage, so the
numbers presented are a lower bound on the possible behaviors.
Furthermore, JavaScript usage in 2011 is likely different from today,
e.g. Node.js was not covered by Richards.  More details about dynamic
analysis of JavaScript can be found in~\cite{liang}.

Wang et al.~\cite{wang} analyzed use of dynamic features in 18 Python
programs to find if they affect file change-proneness.  Files with
dynamic features are significantly more likely to be the subject of
changes, than other files.  Chen et al. looked at the correlation
between code changes and dynamic features, including \eval, in 17
Python programs~\cite{chen}. They did not observe many uses of \eval.
Callau et al.~\cite{oscar} performed an empirical study of the usage
of dynamic features in 1,000 Smalltalk projects. While \eval itself is
not present, Smalltalk has a rich reflective interface that gives
programmer great flexibility. Their work was based on a static
analysis.  %% TODO:: Say someting meaningfull about OSCAR

Morandat et al.~\cite{ecoop12} had a short section on the usage of
\eval in R. They found the it widely used in R code with 8500 call
sites in CRAN and 2.2 million dynamic calls. The 15 most frequent call
sites account for 88\% of those. The \c{match.arg} function is the
highest used one with 54\% of all calls. In the other call sites, they
saw two uses cases. The most common is the evaluation of the source
code of a promise retrieved by \c{substitute} in a new environment;
e.g. as done in the \c{with} function. The other use case is the
invocation of a function whose name or arguments are determined
dynamically. For this purpose, R provides \c{do.call} and thus \eval
is overkill.



\begin{figure}[!t]\hspace{-5mm}\includegraphics[width=1.05\linewidth]
{pipeline.pdf}\caption{Analysis Pipeline}\label{fig:pipeline}
\end{figure}

\section{Methodology}

\subsection{Infrastructure}

Our infrastructure consists of a dynamic analysis pipeline to record the
calls to \eval and RMarkdown notebooks for data analysis and visualization.
Figure~\ref{fig:pipeline} shows the main steps of the pipeline. We provide
the runtime, input and output data size for each step. All runs are done on
a single server Intel Xeon 6140, 2.30GHz with 72 cores and 256GB of RAM.

The pipeline begins with downloading and installing open source R packages
from CRAN for the corpus along with their dependencies. The Kaggle programs
are downloaded from \href{http://www.kaggle.com}{Kaggle} separately by a
crawler.  Next, code size and coverage metrics from the installed packages
are extracted using the \href{ https://github.com/r-lib/covr}{\covr}
package. This is followed by extraction of runnable programs from R
packages: tests, examples and vignettes as well as from Kaggle. The code of
each extracted program is wrapped into a call to dynamic analyzer
\emph{evil} (\emph{Ev}al \emph{I}nspection \emph{L}ibrary) that collects eval
usage. The reason for this is that we only want to record eval usage for the
extracted code. Without this, the data would include eval calls from the
unit testing frameworks as well as from bootstrapping R virtual machine
itself. To avoid any interference, each program is run in its own R
instance. The \emph{evil} framework is implemented as a R package in 2K lines of
R and 400 lines of C++ code.

The dynamic analyzer builds upon the dynamic analysis framework, \instrumentr
that we have implemented to enable us to write dynamic analysis logic in R. It
is an R package implemented in 2.5K lines of R and 6K lines of C++ code. It
internally uses a modified R interpreter, \rdyntrace~\cite{oopsla19a}, that
exposes hooks from within the interpreter implementation for events of interest.
\instrumentr serves as an intermediary between \rdyntrace and \evil, it
intercepts the hooks exposed by \rdyntrace and attaches R functions exported by
\evil as callbacks. The \evil callbacks execute on corresponding interpreter
events.

All steps of this pipeline are parallelized using GNU
parallel~\cite{GNUparallel} and orchestrated by GNU make. To schedule and
parallelize extraction and analysis of programs, we use the \runr
package. Furthermore, \runr gracefully handles and reports failures across
large-scale program runs which greatly aids debugging of the analysis
pipeline.  The data extracted by \evil from each program is concatenated,
cleaned and summarized in the post-processing phase by custom R
scripts. Finally, the summarized data is analyzed in RMarkdown notebooks to
gather insights. Apart from the figures, the data points included in the
paper are also generated by RMarkdown notebooks as latex macros.

\subsection{Corpus}

In this study, we report on a corpus of \CorpusCorePackages R core and
\CorpusPackages CRAN packages with \CorpusAllCodeRnd lines of source code
(native + R).  We run \CorpusAllProgramsRnd programs extracted from those
packages as well as from the user-written data-analyses shared on the Kaggle
platform. In this section we describe this corpus and methodology used to
assemble it.

\begin{figure}[!h]\centering\includegraphics[width=.7\linewidth]
  {corpus.pdf}\caption{CRAN packages}\label{fig:corpus}
\end{figure}

\subsubsection{CRAN Packages}

We have selected the TOP \CorpusPackages packages based on their reverse
dependencies, \ie how many other packages depend on them.\footnote{Extracted
  from package metadata using a builtin function.}  This helps us find
packages which should be better written and have better code coverage.  The
resulting set contains \CorpusRCodeRnd lines of R code and
\CorpusNativeCodeRnd lines of native code.\footnote{Excluding comments and
  blank lines using \url{github.com/AlDanial/cloc}.}  For each package we
use its runnable code to compute the package code coverage.\footnote{Computed
  using \url{ https://github.com/r-lib/covr}.} On average, across the
\CorpusPackages packages, it is \CorpusMeanExprCoverage.
Figure~\ref{fig:corpus} shows these packages, the size of the dots reflects
the project's size in lines of code.  The x-axis indicates code coverage in
percents and the y-axis gives the number of call sites to \eval that were traced, in log
scale. Dotted lines indicate means. Packages with over
\CorpusEvalsPackageTreshold eval call sites are named.

All included packages come from the Comprehensive R Archive Network
(CRAN\footnote{\url{http://cran.r-project.org}}), the largest repository of R
code with over \CorpusAllCranRnd packages.\footnote{CRAN receives about 6 new
  package submissions a day~\cite{Ligges2017}.} Unlike other open code
repositories such as GitHub, CRAN is a curated repository. Each submitted
package must abide to a number of well-formedness rules that are automatically
checked asserting certain quality. Most relevant for this work is that all of
the \emph{runnable code} is tested and only a successfully running package is
admitted in the archive. This is important as we use this runnable code for the
dynamic analysis of eval calls.

There are three sources of runnable code in a R package. Next to the package
\emph{tests},\footnote{We had to exclude most tests because the {testthat}
  harness uses \eval and thus causes the entire test to register as an \eval
  call.} there are also \emph{examples} and \emph{vignettes}. Examples are R
code snippets extracted from package documentation into scripts files.
Vignettes are long-form description of package functionality written using
literate-programming style in \LaTeX\xspace or Rmarkdown with chunks of R
code that can be similarly extracted into R scripts. R provides builtin
functions for tangling the embedded code into files.  The selected packages
contain \CorpusPackagePrograms programs with \CorpusPackageProgramsCodeRnd
lines of R code: \CorpusExamplesProgramsRnd examples with
\CorpusExamplesCodeRnd lines of code, and \CorpusVignettesProgramsRnd
vignettes.

\subsubsection{Kaggle Scripts}

To represent end-user code in the corpus, we turned to Kaggle, an online
platform for data-science and machine-learning. The website allows people to
share data-science competitions, a data-analysis problem together with data,
for which users try to find the best solution. The solutions, called
{kernels}, are then shared back to the platform in the form of either plain
scripts or as notebooks.  One of the most popular competition is about
predicting passenger survival on
Titanic\footnote{\url{https://www.kaggle.com/c/titanic}} with \CorpusKaggle
kernels in R (over 1/4 of all available R kernels) which we used for our
corpus.

Unlike CRAN, Kaggle is not a curated repository and therefore there are no
guarantees about the quality of the code. After downloading all of the
\CorpusKaggle kernels and extracting the R code from the various
formats,\footnote{We use {\sf rmarkdown} to convert from notebooks to R.} we
found that \CorpusDuplicatedKaggle were whole-file duplicates. From
\CorpusRunnableKaggle kernels, \CorpusFailedKaggle failed to execute. Next
to various runtime exceptions, common problems were parse errors and
misspelled package names.  The final set contains \CorpusFinishedKaggle
kernels with \CorpusFinishedKaggleCodeRnd lines of R code.

\subsection{Threats to Validity}
The primary threat to validity is the issue of
code coverage, a cause of concern for any dynamic approach. We mitigate this
by including focusing on R packages that have high code coverage. We turn
off the bytecode compiler for this study. The bytecode compiler can also
call \eval. We do not get source locations for \UndefinedEvalsRnd eval
calls. In these cases \eval is either passed as an argument to a
higher-order functions or is defined in a function returned by a
higher-order function and the R parser does not retain location information
for \eval. However, this is a meager \PercentUndefinedEval of all eval calls
and is unlikely to affect our analysis. We ignore calls to the native \eval
function exposed by R. We also ignore the \c{rlang::tidy_eval} function
which uses native \eval internally because \c{rlang} is used to implement a
DSL for data analysis in R. It introduces a new first-class promise object
called \c{quosure} for which it implements special evaluation support in
\c{tidy_eval}.

\section{Evaluation}

This section presents the results of our empirical study of \eval in R in terms
of four research questions.

\subsection{Use of Eval}

The first research question we address is how and why \eval is used in R code.

\begin{center}{\bfseries RQ1: What are the patterns of usage of \eval in the corpus?}\end{center}

This section presents an overview of the usage of \eval in our corpus. Richards
et al. reported 82\% of JavaScript-enabled web pages using \eval~\cite{ecoop11}.
The case for R is more striking with 100\% of the programs call the \eval
function.\footnote{We combine calls to {\sf eval}, {\sf eval.parent}, {\sf
    evalq} and {\sf local}.} This high number is explained by the fact that the
R implementation, its core packages, use \eval extensively. We differentiate
three types of eval based on their origin:
%
\begin{compactitem}[$-$]
  \item a {\bf core eval} is one that occurs in the code of a core package,
  \item a {\bf dependent eval} occurs in a package imported by the target
  package, and
  \item an {\bf own eval} is one that occurs in the source code of the target
  package.
\end{compactitem}

Over all, we observe \AllAllCallCountRnd calls to \eval in our corpus.
Table~\ref{A} details this run-time information. The format is T (m/M/x)
where T is the total number of calls observed for all runs of the corpus, m
and x are the minimum and maximum number of calls for any given run, and M
is the mean number of calls per run. An average CRAN package run triggers
581 {\eval} calls in core package, 532 in dependent packages and 292 in the
CRAN package itself.  The Kaggle data set has small programs which only
trigger \eval in core and dependent packages. Thus, an average Kaggle run
has 412 core calls, 96 dependent calls, and no own calls.


%% SORRY  I HAD TO REFORMAT
\begin{table}[!h] \centering
 \begin{tabular}{r|c|c|c} \hline
Source & Core    & Dependent                      & Own \\\hline
CRAN   & 5.6M &  867K    & 259K \\
       & (1 / 337 / 46K)& (0 / 52 / 58K ) & (0 / 15 / 15K) \\\hline
Kaggle &  1.2M &  470K   & 0\\
 &  (0 / 721 / 18K) & (0 / 290 / 20K) & (0 / 0 / 0)\\\hline
\end{tabular}\caption{Eval calls in CRAN and Kaggle}\label{A}\end{table}


Table~\ref{table:site-package-summary} shows the number of call sites in
CRAN packages. \OneCallSitesEnrich{} packages have a single call site, and one
package, \emph{metafor}, has \MaxCallSitesEnrich{} call sites of
\eval. \NoCallSitesEnrich{} packages have no calls to \eval. The core packages
have \NbCoreEvalCallSites{} call sites of \eval and Kaggle has none. These
numbers are under-approximations as \eval may be called reflectively,
through aliases, or passed to a higher-order function.


\begin{table}[ht]\resizebox{\columnwidth}{!}{%
\begin{tabular}{rr|rr|rr}\hline
 Callsites & Packages & Callsites & Packages & Callsites & Packages \\\hline
\input{tag/table-site-package-summary.tex}
\end{tabular}}
\caption{Distribution of callsites in CRAN}
\label{table:site-package-summary}
\end{table}

We compare the number of events intercepted during execution of \eval across
the whole corpus. For this, we use the number of interpreter steps as a
proxy for events. We observe \AllEventCountRnd events all told, and
\EvalEventCountRnd events occur inside eval (\EvalEventAllPerc of all
events). Table~\ref{table:event-distribution} shows the distribution of
events caused by \eval calls.

\begin{table}[ht]\resizebox{\columnwidth}{!}{%
\begin{tabular}{lll}\hline
Events & Core Eval Call \% & Package Eval Call \% \\ \hline
\EventsMinRangeA--\EventsMaxRangeA & \EventsCoreEvalPercA & \EventsPackageEvalPercA \\
\EventsMinRangeB--\EventsMaxRangeB & \EventsCoreEvalPercB & \EventsPackageEvalPercB \\
\EventsMinRangeC+ & \EventsCoreEvalPercC & \EventsPackageEvalPercC \\ \hline
\end{tabular}} \label{table:event-distribution}
  \caption{Events in \eval calls}
\end{table}

We observe a very wide spread in the number of events generated by \eval
calls. The largest number of events generated by an \eval call is
\EventsMaxCountRnd.  However, a majority of \eval calls performs up to
\EventsMaxRangeA events suggesting that most expressions passed to \eval are
small.  It is interesting to note that a higher proportion of Core R \eval
calls perform side effects in the \EventsMinRangeB--\EventsMaxRangeB range.
Only \EventsCoreEvalCountC Core R \eval calls and \EventsPackageEvalCountC
package \eval call generate \EventsMinRangeC to \EventsMaxRangeC events.
These \eval calls originate from statistical modeling packages such as
\mlogit, \mboost, \metafor, \lavaan, \mclust and \gamlss.


\subsubsection{A taxonomy of eval}

% Following the high-level overview of the usage of eval in the previous section,
% we now look into categorization of the eval behavior. Similarly to the JavaScript paper, we look at:
% %
% \begin{inparaenum}[(A)]
%   \item \textbf{mix} of operations performed by the expressions passed to eval,
%   \item \textbf{}
% \end{inparaenum}

In this section we turn our attention to the expression passed for evaluation to
\eval. The \eval function accepts all types of R objects as inputs, but in the
majority of cases (\AllExpressionInputEvalCallPerc), it is called with
expressions. The remaining \AllValueInputEvalCallPerc are values, \ie, they
evaluate to themselves.
%
Passing a value into an eval is effectively an no-op so we look closely at the
expression types the call sites pass to eval across all the runs.
\AllValueInputEvalSitePerc of all call sites always pass values,
\AllExpressionInputEvalSitePerc pass expressions and a very large proportion,
\AllPolymorphicInputEvalSitePerc, pass both expressions and values as inputs
across all program runs. We inspected the eval callsites that accept only values
or both values and expression as inputs. The top three of these call sites
contribute to over ~80\% of all calls to evals. The first one is inside
\c{match.arg} from core. Given a vector of names as strings, this function finds
the default values for parameters with those names in the caller function. Since
the default value for a function parameter can be a complex expression or a
benign value such as \c{NULL}, the \c{match.arg} function evaluates it through a
call to \eval. This makes the \eval inside \c{match.arg} ``polymorhpic''. The
second such \eval call originates from \ggplot, a popular R package for making
plots. \ggplot has it's own object system, \ggproto, which uses \eval inside the
\ggproto function to determine the super-class from which it inherits (the
default value is \c{NULL} but in general it could be an arbitrarily expression).
The third \eval call is in core R \c{str} function used to print a summary of R
objects. It internally maps \eval function on the bindings of an environment
which results in inputs of multiples types being passed to \eval.

We further investigate the type of input passed to \eval in
Figure~\ref{fig:eval-expression-kind} by the type of input and the source of
\eval. In this figure, \emph{Expression} is the type of program text,
\emph{Environmment} is the type of scopes or environments used for binding,
\emph{Null} is the type of the unique \c{NULL} object, \emph{Vector} is the
type assigned to vectors of strings, booleans, integers, reals and bytes;
\emph{Symbol} is the type of unevaluated symbols, and \emph{Function} is the
type of functions.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\columnwidth]{eval-expression-kind}
  \caption{Type of Expression passed to \eval} \label{fig:eval-expression-kind}
\end{figure}

From the figure, we observe that majority of inputs to Core \eval functions
are expressions in contrast to package \eval calls which receive mostly
environments as inputs. In fact, \PackageEnvironmentInputEvalCallPerc of
\eval calls in packages receive environments as input expressions. There are
only four callsites in package evals that receive environments as
expressions.
\begin{compactitem}[$-$]
\item \c{ggplot2::ggproto} contributes to 99.9\% of package \eval calls that
  receive environments as input.
\item \c{R6::generator_funs} function contributes to only 936 \eval calls and
implements the same functionality as \c{ggplot2::ggproto} but for the \emph{R6}
OOP system.
\item \c{future::backtrace} function applies \eval to a future object which
  is implemented as an environment. This is called only once.
\item \c{RModel::str.RMmodel} function overloads the core \c{str} method for its
\c{RModel} objects and maps \eval exactly like \c{str}. This is called only
once.
\end{compactitem}

We looked at the top ten expressions passed to core and package \eval
calls. The most frequent ten expressions to eval calls from core R
contribute to 85\% of all \eval calls.

\begin{table}[!h] \centering
\begin{tabular}{@{}l|rr@{}} \hline
Expression & Eval Call &  \% \\\hline
\c{c("auto", "shell", "radix")} & 1,987,105 & 29\%\\
\c{c("auto", "shell", "quick", "radix")} & 1,593,169  & 23\%\\
\c{\{info <- loadingNamespaceInfo(...} & 1,008,632 &       14\%\\
\c{c("onLoad", "attach", "detach", "onUnload")}   & 470,566 &      6.9\%\\
\c{c("append", "prepend", "replace")} &              261,587&       3.9\% \\
\c{c("left", "right", "centre", "none")} & 162,086     & 2.4\%\\
\c{c("no", "ifany", "always")}   &                71,580 &       1.1\%\\
\c{c("pearson", "kendall", "spearman")}  & 72,962 &      1.1\%\\
\c{NULL}& 75,330  &      1.1\% \\
\c{Symbol}&                 66,279&       1\%\\\hline
\end{tabular}
\caption{Top ten eval calls in Core}\label{B}
\end{table}

The expression \c{\{info <- loadingNamespaceInfo(...} is added by core R
to a package directory during installation. To load the package, this code is
executed. It creates a namespace for the package, injects the package bindings,
and attaches the namespace to the program search path. The \c{NULL} comes from a
call to \c{substitute(subset)} in \c{stats::model.frame.default} function which
has a default value of \c{subset} as \c{NULL}. The \c{Symbol} arises from a
call to \c{as.name} in \c{base::str} function that returns a symbol that is
looked up by evaluating it in a specific environment. The remaining cases arise
from calls to \c{match.arg} which is used to look up the default choices for a
variable and match against the choice passed by the caller.

The most frequent ten expressions to eval calls from CRAN packages
contribute to 77.1\% of all \eval calls.

\begin{table}[!h]  \centering
\begin{tabular}{@{}l@{~}|@{~}r@{~}r@{}} \hline
Expression & Eval Call &  \% \\\hline
\c{Environment} &                                  989302   & 61\%\\
\c{column[rows] <<- what} &                        55677    & 3.5\%\\
\c{function(value) freduce(value, `_function_list`)} & 37251& 2.3\%\\
\c{NULL} &                         32005    & 2\%\\
\c{List} &                         22293    & 1.4\%\\
\c{c("default", "default2012", "default2011" ...}& 20610    & 1.3\%\\
\c{force(..1)}            &                        20461    & 1.3\%\\
\c{alist(`_spec`)}       &                         18532    & 1.2\%\\
\c{inner}               &                          18530    & 1.2\%\\
\c{String Vector}      &                           17487     & 1.1\%\\
\end{tabular}\caption{Top ten eval calls in CRAN} \label{C}
\end{table}

The expression \c{Environment} occurs because of the four callsites
explained above, \c{ggplot2::ggproto}, \c{R6::generator_funs},
\c{future::backtrace} and \c{RModel::str.RMmodel}. The next expression,
\c{column[rows] <<- what}, is used inside the \c{plyr::rbind.fill} function
to merge data frames by assigning concatenated vectors to rows. The \c{<<-}
operator is interesting in that it skips the current scope and assigns in a
parent scope in which the variable is already present. In our corpus, all
these \eval calls contribute to a single side-effect. The expression
\c{function(value) freduce(value, `_function_list`)} arises from the
\c{magrittr::\%>\%} function which is a pipe operator that pipes the output
of previous command to the next one. The expression is evaluated in a custom
environment to create a function binding for evaluating the components of
the pipe.\c{String Vector} and \c{List} also arise from the same function
when a string or a list is piped using the \c{\%>\%} function into the next
expression.  The \c{NULL} arises from \c{R6::generator_funs} function when
the \eval is passed a \c{NULL} argument by the \c{DataMask_generator}
package. The \c{c("default", "default2012", "default2011" ...} pattern
arises from \c{copula::polyG} where it reflectively access the default
expression for its formal parameter and evaluates it. The \c{force(..1)} and
\c{alist(`_spec`)} patterns occur in \c{glue::glue_data} function which
concatenates and interpolates strings. The two patterns occur because the
function captures unevaluated unnamed arguments and maps the evaluation of
\c{force(..1)} on them.  The \c{force} function forces promises and returns
the result of evaluation. The \c{inner} pattern arises from
\c{glue::identity_transformer} which enables the creation of custom
transformation functions for affecting the interpolation and concatenation
of input by the \c{glue} package.


\subsubsection{Operation Mix}

The JavaScript paper reported on the operation mix within \evals. This means
the proportion of read, write, and call bytecodes performed by the
JavaScript interpreter. They found that some web pages were creating objects
more frequently and writing into their fields.  In R, nearly every operation
performed in the interpreter is a function call. We observe 2.9G calls and,
of those 125M happen inside \eval (there are 463M native calls, of which
only 31M are in \eval).

{\bf there is no good reason for not counting reads and writes, we can...}


%% ...The following is fluff ...

%% Qualitatively, we observe all kinds of operations being executed inside
%% \eval calls, from numerical computing for statistical modeling to reflection
%% and metaprogramming for implementing DSLs. Even testing frameworks such
%% \c{genthat} heavily rely upon \eval to execute test code snippets. The core
%% R implementation is rife with uses of \eval for mundane things like
%% accessing default values of formal parameters and in statistical packages
%% for computation.


\subsubsection{Usage Patterns}

Richards et al.~\cite{ecoop11} identified a number of frequently occurring \eval
patterns which were detected using simple regular expressions on the \eval
strings. In the case of R we match the patterns on the recorded AST.

Table~\ref{table:js-pattens-in-r} shows the ratio of \eval in each of these
patterns. We included all the original patterns except for \emph{JSON} as R uses
its own serialization format and for \emph{empty} as there were no empty expression
passed to \eval:

\begin{table}[ht]%\resizebox{\columnwidth}{!}{%
  \centering
\begin{tabular}{r|r|r|r|r}\hline
\multirow{2}{*}{Pattern} & \multicolumn{2}{c|}{Core} & \multicolumn{2}{c}{Packages} \\
                         & Calls & Ratio            & Calls & Ratio \\\hline
\input{tag/table-js-patterns-in-r.tex}
\end{tabular}
\caption{JavaScript \eval Patterns in R}
\label{table:js-pattens-in-r}
\end{table}


\noindent \emph{Library}, \emph{Typeof} and \emph{Try} are all barely used
mostly because the language provides dedicated features for it. The few calls in
Library pattern come mostly from file loading functions such as \c{source} and
\c{sys.source}.
%
\noindent \emph{Read} pattern in JavaScript represented mostly an incorrect

access to an object's property or local variable both replaceable by
JavaScript core functions. In the case of R there are \PatternReadRnd read
\eval calls (\PatternReadRatio). They come from calls like
\c{subset(airquality, select = Temp)} where the variable \c{Temp} will be
evaluated in the \c{airquality} data frame.

%
\noindent \emph{Assign} category comprises of assignments to variables
within an \eval call. We have seen \PatternAssignRnd such calls
(\PatternAssignRatio).  \PatternAssignArrowRatio uses the simple assignment
(\c{<-}). The majority of cases comes from the \c{magrittr}'s \c{\%<>\%}
pipe operator (\PatternAssignArrowMagrittrRatio) which forwards an object to
a function on rhs and update lhs with the result. Far more calls used the
super assignment (\c{<<-}) operator, but almost all come from the \c{plyr}
package which provides a convenient API for common data manipulation
problems.
%
\noindent \emph{Call} and \emph{Other} together represents the vast majority
of eval calls in R. The \emph{Other} pattern represents all the calls where
eval was called with a value instead of with an expression. The \emph{Call}
pattern contains all the calls that did not fall in the previous
categories. Without a context it is usually difficult to further categorize
the intent of these calls.  Therefore we have randomly chosen
\PatternManualPackages packages and manually looked at
\PatternManualCallsites call sites from which we have observed the following
categories:

\subsubsection{Recording the current call} R provides a \c{match.call} function
which can return the current call. This is heavily used by the statistical
model-fitting functions that records the current call that is then
reevaluated in the different environment. Another pattern is to match the
current call, adjust some arguments and pass it to another function. For
example the \c{write.csv} function changes the \c{sep} argument in the
matched call and reevaluates itself as \c{write.table}:
%
\begin{lstlisting}
    Call <- match.call(expand.dots = TRUE)
    Call$sep <- ","; Call[[1L]] <- as.name("write.table")
    eval.parent(Call)
\end{lstlisting}

\subsubsection{Sandboxing} The combination of \eval and first class
environments can be used to construct a sandbox for evaluating
side-effecting expressions. The newly constructed environment can use the
current environment for its lexical scope. This provides the evaluated
expression access to all the bindings in the current scope but restricts the
side-effect to the newly constructed throwaway environment. However, this
sandboxing is limited because the expression can reflectively access any
environment in the call stack or use the super assignment operator(\c{<<-})
to write to parent scope. The \c{local} function which is part of Core R is
an example of this usage pattern.

\subsubsection{Dynamic Code Loading} The combination of \parse and \eval is
used to parse text from string or file and evaluate in a custom environment.
This pattern is used by the \source Core R function to load R code from a
file in the current workspace. This is quite useful for loading functions
for interactive data analysis.

\subsubsection{Metaprogramming} The combination of \eval and \substitute is
 used to do metaprogramming in R. \substitute can be used to synthesize ASTs
 by stitching together user inputs. \substitute walks over its argument AST
 and replaces the symbols with their bindings in the specified environment
 and returns this new AST which can then be evaluated in custom environments
 using \eval.

  \subsubsection{Convenience} Eval is used to make some programming tasks easier (and/or shorter). For example, in \c{data.table}, it is used to set R options from a character vector
  \begin{lstlisting}
opts = c("datatable.verbose"="FALSE", ...)
for (i in names(opts)) eval(parse(text=paste0("options(",i,"=",opts[i],")")))

  \end{lstlisting}
  %
  instead of manually calling \c{options(datatable.verbose=FALSE)}

  \subsubsection{Implementation}
  The package installation process adds an extra file to the package directory
  that contains the code to load the package code inside a \local block. It
  creates a namespace object and injects the package bindings.

  \subsubsection{Miscellaneous} \eval is also used in R packages to get around
  various limitations. For example, in \datatable package, \eval is used to
  modify a binding in the \base package of R. Bindings in the \base package are
  locked for modification and require explicit unlocking using
  function \unlockBinding. However, directly using this function triggers a
  restricted-function-use warning during package build, preventing the package
  to be accepted by CRAN. The \datatable gets around this limitation by
  enclosing the call to \unlockBinding inside \eval. From the same reason, other
  packages use \eval to access private variables and functions from their
  dependencies.

\subsection{Necessity of Eval}

\subsubsection{Unnecessary use of eval}
  Similarly to JavaScript, there are also unnecessary uses of \eval.
  For example, the \c{PerformanceAnalytics} package contains a function
  \c{chart.QQPlot} that uses \eval to resolve a string into function and
  another to call it and assign its results into a variable:
\begin{lstlisting}
function (R, d="norm", dp, ...) {
q.f <- eval(parse(text=paste("q",d,sep="")))
z <- NULL
eval(parse(text=paste("z<-q.f(",dp,",...)")))
}
\end{lstlisting}
  In both cases, there is no need for \eval:
\begin{lstlisting}
function (R, d="norm", dp, ...) {
q.f <- get(paste0("q",d))
z <- q.f(dp, ...)
}
\end{lstlisting}
  or even to a oneliner \c{do.call(paste0("q",d), as.list(dp, ...))}.

\subsection{Power of Eval}

\subsubsection{How much code is executed inside eval}
- measure number of operations inside eval and outside eval.
- this is useful because if analyzers treat eval as a nop, they may miss X\% of the code.

\subsubsection{Does eval create observable side-effects}
- measure number of reads and writes by eval outside of its ``bubble''.
- Since R is lazy, even reads can cause promise evaluation and hence, side-effects.
- Look at writes to non-local scopes. Compare with total number of writes for
reference?
- Categorize side-effects between env passed to eval and other envs.

\subsubsection{Does eval do reflection}
\eval can access parent scopes via reflection. Find number of times it does that
and the depth to which it access parent frames.

\subsubsection{Does eval introduce new bindings}
Find number of times, eval introduces new bindings. There are many ways -- library,
load, attach, source, and explicitly introduce bindings using super-assign, assign and define.

\subsubsection{Does eval call C code}
Find number of times, eval makes call to native code using - .Call, .External,
etc. Find out if there are native functions that are called only from within eval.

\subsubsection{Does eval add finalizers}
Eval can attach code to be executed when the object goes out of scope or program
exits. Find if eval does that.

\subsubsection{Does eval do non-local returns}
Eval can do non-local returns effective bypassing evaluation of the rest of the
function. This can be useful for static analyzers.

Conclude with number of evals that are ``pure'', i.e. evals which
could be ignored by a static analyzer without any problem.

\subsubsection{Provenance}

In JavaScript, the provenance of the string passed into \eval is a matter of
concern as \eval can be used to breach the security of websites. The main
danger comes from string that were obtained from external sources. If they
are allowed to flow into an \eval, then an attacker could hijack the
program.

{\bf what have they found...}

There are various ways to obtain expressions:
\begin{itemize}
  \item \c{substitute} synthesizes ASTs from expressions by replacing
    symbols with their bindings in the specified environment.
  \item \c{expression}  creates a vector of expression
    objects from text.
  \item \c{parse}, \c{str2expression} and \c{str2lang} turn strings into
    expressions.
\end{itemize}

We observe 5.2\% cases where \eval directly evaluates the output of
\c{substitute}, 0.7\% cases where output of \c{parse} is read directly and
only 704 cases for \c{expression}. Most expressions consumed by \eval are
generated by other functions.

\c{eval(parse(...))} can be used for dynamic code loading. This forms the
core of \c{source} and \c{sys.source} functions in R that are commonly used
for loading code in R files in interactive settings. We investigated the
number of cases in which the output of \c{parse} and its variants is passed
to \eval, directly or transitively by tainting their output. This
corresponds to \PercentParsedCallSites of the total \eval call sites and
\PercentParsedEvals of the eval calls. We observed that very few of the eval
call (\NbParseFilesRnd in total) consume the result of calling \c{parse} on
a file. Most of the eval calls consume the result of calling \c{parse} on a
string.  We also identified one function in core R,
\c{invokeRestartInteractively} that prompts the user for input, parses it,
and passes it to \eval.

\subsubsection{Side-Effects}

In the corpus we observe \AllWritesRnd writes to variables of which
\EvalWritesRnd writes happen inside \eval. However, all writes are not
dangerous. Only writes to environments not local to the computation spawned
by \eval are side-effecting. These writes outlive the computation and hence
are visible outside it. The remaining writes are local to the
computation. We observe that \EvalSideEffectingWritesRnd writes inside evals
are side-effecting. This is only \EvalSideEffectingWritesEvalPerc of all
variable writes inside \eval and \EvalSideEffectingWritesAllPerc of all
variable writes in the corpus.

An \eval is considered side-effecting if it performs a side-effecting write to a
variable, directly or indirectly. Only \SideEffectingCoreCallPerc \eval calls in
Core R are side-effecting and \SideEffectingPackageCallPerc \eval calls in CRAN
packages are side-effecting.

\subsubsection{Scope}

One of the main differences of the R \eval with the javascript one is how it
interacts with \emph{scope}, or environments in R. In Javascript, \eval can
access the local scope and the global scope. In R, \eval has an argument
\c{envir} to indicate in which environment its first argument must be evaluated.
By default, it is equal to \c{parent.frame()},
which is the parent environment of \eval call. This default argument
happens in \DefaultEnvirExprPercent of the eval calls.

This argument can also be \c{NULL}, a \c{List} or a data frame. This happens
in 3.5\% of the cases: \eval copies the fields of the list or data frame
and creates bindings for them in a new environment. This pattern is used to
evaluate formulas which can directly refer to the fields of the data.  The
\c{envir} argument can also be a number $n$. It means that the environment
in which the expressions is evaluated will be the result of \c{sys.call(n)}
where $n$ refers to the $n$-th stack frame.

The top-level environment in R is called the global environment. New
environments can be created using \c{new.env}. They can be provided a parent
environment which becomes the enclosing scope of the new environment.

We looked at the environments passed to all the \eval calls in our corpus.
Table~\ref{tab:environments} summarizes the results. A numeric environment
class \c{n} denotes the environment of the $n$-th call stack frame from the
current function. \c{global} denotes the top-level environment and \c{list}
denotes a list passed for evaluation of formulas. Environment classes of the
form $n+$ denote the $n$-th environment extended with a new environment. The
new environment provides a limited form of sandboxing. All assignments using
the \c{\<-} function occur inside it and prevent the extended environment
from mutation. However, it is still possible to mutate the extended
environment using the \c{\<\<-} or \c{assign} functions; but, that happens
rarely.

\begin{table}[htbp] \resizebox{\columnwidth}{!}{%
    \centering
    \begin{tabular}{c|c|c|c}
        \hline
        \multicolumn{2}{c|}{Core} & \multicolumn{2}{|c}{Packages}  \\
        \hline
        Environment Class & Eval Calls \% & Environment Class & Eval Calls \% \\
        \hline
        \CoreEnvClassA & \CoreProportionA & \PackagesEnvClassA &  \PackagesProportionA\\
        \CoreEnvClassB & \CoreProportionB & \PackagesEnvClassB &  \PackagesProportionB\\
        \CoreEnvClassC & \CoreProportionC & \PackagesEnvClassC &  \PackagesProportionC\\
        \CoreEnvClassD & \CoreProportionD & \PackagesEnvClassD &  \PackagesProportionD\\
        \CoreEnvClassE & \CoreProportionE & \PackagesEnvClassE &  \PackagesProportionE\\
        \hline
    \end{tabular}} \label{tab:environments}
\caption{Environments in terms of eval calls.}
\end{table}

We observe that a disproportionately high number of core R \eval calls
access the caller's caller's environment. This is because many core R
functions call functions that pass the result of \c{parent.frame()} to an
\eval.  A disproportionately high number of calls to \eval happens in an
extended top-level environment. This can be explained by the fact that many
packages evaluate code passed from the user's workspace in the top-level
environment to access the bindings.


The \c{imchange} function of package \c{imager} makes it possible to modify
images using a dedicated formula syntax using \c{\~}.\footnote{Inspired by
 {map} in package \emph{purr}.}  Here, \eval is evaluated in \c{newenv},
which creates a new environment that inherits from \c{parent.frame()} by
default (classified as 1+).


\begin{lstlisting}
newenv <- new.env()
...
fo <- parse(text=as.character(fo)[2])
im[where] <- eval(fo,envir=newenv,enclos=env)
\end{lstlisting}

\c{adjCoef} in package \emph{actuar} find the root of an equation defined by
a function \c{h} whose arguments must be named \c{x} and \c{y}.  \c{h} is
transformed into an auxiliary function \c{h2} that can be optimized. Here,
the list used for \c{envir} ensures the correspondance between the textual
arguments of \c{h} and the arguments of \c{h2}.


\begin{lstlisting}
sh <- substitute(h)
fcall <- paste(sh, "(x, y)")
...
h2 <- function(x, y)
    eval(parse(text = fcall),
    envir = list(x = x, y = y),
    enclos = parent.frame(2))
\end{lstlisting}

\section{Discussion}

\subsection{Comparison with Javascript}

\medskip\noindent\emph{Discussion:} Comparing the use of \eval in R and
JavaScript is instructive. The use of \eval is more widespread in data
science code than it was in JavaScript. Any run of an R program seems to
trigger \eval; moreover package programmers use \eval liberally. On the
other hand, the authors of Kaggle packages do not use the feature at all.
This point to a bifurcated user community: experts who write package know
how to use \eval while end-users do not. An alternative explanation is that
end-users do not need \eval for simple data analysis tasks. This bifurcation
was not observed in the JavaScript community, but this may have changed with
the advent of Node.js ecosystem.  The distribution of input sizes is similar
to that found in JavaScript, the majority of inputs being rather small.


\begin{figure}[!b]
	\centering
  \includegraphics[width=\columnwidth]{ast_sizes}
  \caption{Input sizes} \label{fig:ast-size}
\end{figure}

To roughly estimate the complexity of the code being evaluated,
Fig.~\ref{fig:ast-size} shows the distribution of the abstract syntax tree
size of the \c{expr} argument to \eval. The majority of inputs across core
and packages (\NbAstOnePercent of \eval calls and \NbAstOneCallSitePercent
of call sites) have size 1. That is to say, the code being evaluated
consists of a single variable name or constant. A function call needs at
least three nodes and a list such as \c{c(1, 2, 3)} counts for 4
nodes. Inputs smaller than \AstSizeNineFive nodes account for 95\% of all
calls. Sizes in core are larger than in packages. Most large ASTs originate
from library \c{VGAM}, which is used for statistical modeling, and also calls functions using \eval from core. It uses
\eval to evaluate large matrices with values in a given environment.

\subsubsection{Consistency}

We look at how \emph{consistent} the \c{expr} argument of \eval can be, \ie
how many different types of the resolved \c{expr} there are per call
sites. Most of the call sites, \ie \PercentMonomorphic, are
\emph{consistent}, and this is similar to javascript. However, a few ones
are highly \emph{polymorphic} (10 different types). They are the pipe
operators \c{\%>\%}, \c{\%<>\%} and \c{\%\$\%} in package \emph{magrittr}. It
is effectively used to compose functions on their first argument, which can
be of any type.


\section{Conclusion}

\bibliography{bib/bibliography,bib/jv}

\end{document}
