\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{listings}
\newcommand{\missingNumber}{\textcolor{red}{XX}\xspace}
\newcommand{\missingPercentage}{\textcolor{red}{XX\%}\xspace}
\newcommand{\missingTable}{\textcolor{red}{XXTable}\xspace}
\newcommand{\missingGraph}{\textcolor{red}{XXGraph}\xspace}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{paralist}
\usepackage{xspace}
\usepackage{url}

\input{tag/counts.tex}
\input{tag/corpus.tex}
\input{tag/analysisStable.tex}

\graphicspath{{img/}}

\lstset{ 
 language=R,              % the language of the code
 basicstyle=\small\ttfamily,    % the size of the fonts
% numbers=left,            % where to put the line-numbers
 numberstyle=\color{Blue},% the style that is used for the line-numbers
 stepnumber=1,            % the step between two line-numbers. If it is 1,
                          % each line will be numbered
 numbersep=2pt,           % how far the line-numbers are from the code
 backgroundcolor=\color{white},  % choose the background color.
 showspaces=false,        % show spaces adding particular underscores
 showstringspaces=false,  % underline spaces within strings
 showtabs=false,          % show tabs within strings adding particular 
%frame=single,            % adds a frame around the code
 rulecolor=\color{black}, % if not set, the frame-color may be changed on
                          % line-breaks within not-black text (e.g. commens
                          % (green here))
 tabsize=2,               % sets default tabsize to 2 spaces
 captionpos=b,            % sets the caption-position to bottom
 breaklines=true,         % sets automatic line breaking
 breakatwhitespace=false, % sets if automatic breaks should only happen at 
 keywordstyle=\color{Blue},    % keyword style
 commentstyle=\color{YellowGreen},  % comment style
 stringstyle=\color{ForestGreen}    % string literal style
} 
\begin{document}

\title{A Large-Scale Study of the Use of Eval in R}
\author{\vspace{-.8cm}\IEEEauthorblockN{~}} %\IEEEauthorblockA{ dept} \\ \and
\maketitle

\newcommand{\eg}{\emph{e.g.},\xspace}
\newcommand{\ie}{\emph{i.e.},\xspace}
\newcommand{\cf}{\emph{cf.}\xspace}
\newcommand{\summary}[1]{({\csname #1Min\endcsname} / {\csname #1Mean\endcsname} / {\csname #1Max\endcsname})}
\newcommand{\summaryrnd}[1]{({\csname #1MinRnd\endcsname} / {\csname #1MeanRnd\endcsname} / {\csname #1MaxRnd\endcsname})}

\newcommand{\eval}{\texttt{eval}\xspace}
\renewcommand{\c}[1]{\lstinline{#1}\xspace}
\newcommand{\miss}[1]{{\textcolor{red}{#1}}\xspace}

\begin{abstract}
  The \eval function turns text into code. From the point of view of
  reasoning about software system, uses of \eval turn the program's code
  from a statically known quantity that can be reasoned about and analyzed
  into one of the inputs to the program. Thus, widespread use of \eval
  hinders our ability to provide safety and security guarantees for
  software. Understanding how \eval is used in practice is key to finding
  ways to mitigate its damage. One question that has not been studied is
  whether the usage of \eval is dependent on the application domain or other
  features of the language. This paper is a large-scale study of a corpus of
  \CorpusAllCodeRnd lines of data science libraries and end-user written in
  the R language. We performed a simple static analysis and more
  sophisticated dynamic analysis to confirm that \eval is indeed in wide
  spread use, and to categorized patters of usage. We found that \eval in
  this code base is more dangerous in some ways, and safer in others, than
  what was previously reported for JavaScript.
\end{abstract}

\section{Introduction}

Traditional program analysis techniques are based on sound reasoning about
program behavior, a program analyzer computes an over-approximation of the
set of possible operations performed by the system being
analyzed~\cite{cc77}.  This is predicated on knowing the program's code, and
computing that code's behavior for all possible inputs. The presence of
\eval entails that the program's input, any string variable, may be
reflected into its code giving rise to \emph{any} behavior allowed by the
language semantics. In many dynamic languages, \eval can redefine most
user-defined and built-in functions resulting in a complete loss of
precision for any operation following \eval. A group of influential
resaerchers argued to give up on soundness and, instead, to
under-approximate dynamic features~\cite{soundy} In their words ``a
practical analysis, therefore, may pretend that \eval does nothing, unless
it can precisely resolve its string argument at compile time.''  Assuming
that \eval does not have side-effects may be too optimistic. We rather
propose to study what \eval does in practice, in real-world code, so as to
give researchers data about how to better under-approximate uses of that
feature.

While there has been previous work investigating the usage of \eval in web
programming and JavaScript~\cite{ecoop11}, there are no other studies that
shed light on the usage of \eval in other programming languages and
application domains.  It is thus reasonable to wonder if the results from
previous work can be carried over to other contexts. For this paper, we have
picked the data science language R~\cite{r} as our object of study. R is
both different from JavaScript in linguistic terms and in terms of its user
base. JavaScript was designed to run untrusted code in browser, while R was
design for statistical computing on a desktop. JavaScript is used as a
general purpose language by a wide community of programmers; while R is used
for scientific computing by data scientist and domain experts with, often,
limited programming experience.

This paper sets out to highlight the differences in usage patterns of \eval
between these two languages and user communities. Our results are broadly
applicable as they increase our understanding of the range of uses of \eval,
present new use-cases, and, hopefully, provide insights into reasonable
under-approximations. More narrowly, for researchers interested in static
analysis of data science code, we provide a publicly available corpus of
programs and a toolchain that can be customized to deliver fine-grained
information about the usage and impact of \eval.

Our methodology for this work bows to the fact that static analysis of R is
difficult, instead we focus on dynamic analysis. To observe \eval we have
built a two-level monitoring infrastructure: we are able to monitor R
programs by instrumentation -- this gives us access to many user-visible
properties of R programs -- and we also can monitor the inner working of the
R interpreter -- this allows us to capture details of execution that are not
exposed at the source level.

Our corpus has been constructed to reflect the different level of
sophistication in the R community. We distinguish between the core
implementers of the language, developers with both extensive programming
experience and keen understanding of the language semantics, library
implementers, developers with some programming experience and a working
knowledge of R, and end-users, who are typically not expert programmers and
often have only a cursory knowledge of the language.\footnote{To illustrate
  the above consider that R uses lazy evaluation like Haskell. The authors
  have informally surveyed end-users, including computer scientists, and did
  not find a single one aware of this fact. Library developers, on the other
  hand, know about laziness and program defensively around it.}  Thus, our
corpus distinguished between \emph{core packages} (there are \CorpusCorePackages such
packages that are distributed together with the R virtual machine), \emph{CRAN
  packages} (we selected \CorpusPackages curated packages that pass stringent
community quality checks and are equipped with tests and sample data),
\emph{Kaggle scripts} (we used \CorpusFinishedKaggle end-user written programs that performs a
particular data analysis task). There are reasons to believe that \eval
usage patterns may differ between these datasets. The core R code is
extremely stable, it has only been maintained for the last 20 years with
very few far reaching changes. The libraries represent a more lively
ecosystem with new libraries added each day.  Finally, end user code is
often one-use scripts thrown together and sometimes never revisited.


To understand the behavior of eval we perform a combination of static
analysis, dynamic analysis and manual inspection. The results presented
here summarize our findings.

Our infrastructure is publicly available, released in open source, and our
data, and code will be submitted to artifact evaluation.


\section{Background and Related Work}

The origins of \eval go back to the first implementation of the Lisp
programming language~\cite{lisp}. Over the years, \eval has been included in
many dynamic languages, with some variability in expressive power and
nomenclature. In all of its incarnation \eval allow developpers to
dynamically extend the code of their program at run-time.  Originally, \eval
was limited to interpreted languages, but with the advent of just-in-time
compilation this restriction was gradually lifted.

Some languages have chosen to restrict \eval. The essential difference
between languages relates to scoping of variables that can be accessed from
code executed by \eval.  Java and Julia are among the more restrictive
languages. In Java, \eval must be synthesized as a combination dynamic
loading and reflection.\footnote{While Java does not have \eval built in,
  one can take the input to \eval, wrap it in a static method of an
  anonymous class, generate bytecode for that class, invoke the Java class
  loader to dynamically install it, and then use reflection to call the new
  method.} Julia provides \eval more directly but limits its scope to top
level variables and methods. JavaScript support both a, so-called, global
\eval, and a local \eval. Where the latter has acces to local variables of
the function being executed (as well as variables of lexically enclosing
scopes). Finally languages such as R allow \eval to manipulate variables
defined in the caller environment of the function in which \eval is invoked.

The difference between these design choices has an impact on the ability of
compilers to optimize, and of analysis tools to reason about, code. The
impact of a top level \eval is that any global definition may be affected
but the current function can continue to execute unchanged. A local \eval
may change the value of local variables, their types, or even their
presence. This is a much more invasive change for reasoning about code and
may invalidate common optimizations such as constant folding or copy
propagation. The designers of Julia were aware of this difference and chose
to restrict \eval to preserve their ability to compile
code~\cite{oopsla18a}. They even added a versioning mechanism to function
definitions to ensure that new code added during an \eval would not
invalidate optimizations such as inlining.  R's ability to affect any one of
the callers of a function is even more invasive as the impact of \eval is
non-local. Such unfettered power is difficult to deal with.

The intuition behind the soundiness manifesto~\cite{soundy} is that while
the use of a single \eval may invalidate every single invariant computed by
a static analysis or optimizer, this worst case usually does not happen.  In
many cases, \eval is used carefully and has limited effect. This is a
hypothesis that must be validated and, to be useful, we should have the
means to provide some reasonable guesses as to which side-effects will
result from any particular \eval call.



Previous work looked at eval in JavaScript

Othere languages

Dynamic analysis

\section{The Design of Eval}

There are four different kinds of \eval in the \emph{base} package:
\c{eval}, \c{evalq}, \c{eval.parent}, \c{local}.  The most important one is
\eval:

\begin{lstlisting}
  eval <- function(expr, envir=parent.frame(),
                   enclos=if (is.list(envir)||is.pairlist(envir)) parent.frame() else baseenv())  
	.Internal(eval(expr, envir, enclos))
\end{lstlisting}




The three other functions are defined using \lstinline|eval|.

\begin{lstlisting}
evalq <- function (expr, envir = parent.frame(), enclos = if (is.list(envir) || is.pairlist(envir)) parent.frame() else baseenv())  
	.Internal(eval(substitute(expr), envir, enclos))
	
eval.parent <- function (expr, n = 1) 
{
	p <- parent.frame(n + 1)
	eval(expr, p)
}

local <- function (expr, envir = new.env()) eval.parent(substitute(eval(quote(expr), envir)))
\end{lstlisting}


\c{expr} is the expression to be evaluated. It is not a string as in javascript. In order to evaluate a string, it should be first parsed using \c(parse(text = str)) to get an expression.

\eval makes it possible to specify the environment in which to evaluate the expression. \c{enclos} is an additional environment used for lookup when \c{envir} is a list, a pairlist or a dataframe. In that case, \c{envir} is copied into a temporary environment enclosed in \c{enclos}.

The \c{expr} argument of \eval is first evaluated in the current scope, then passed to the \eval. Quoting that argument prevents if from being evaluated in the current environment. \c{evalq} is a shortcut for \c{eval(quote(expr), ...)}. In the following code snippet, variable \c{r} is first looked up in the current scope with \eval whereas it is looked up in \c{env} with \c{evalq}.

\begin{lstlisting}
> r <- 5
> env <- new.env()
> env$r <- 9
> eval(r, envir = env)
[1] 5
> evalq(r, envir = env)
[1] 9
\end{lstlisting}

\eval will return unchanged basic types such as vectors, functions, strings and environments. Promises are forced by \eval. Symbols are looked up in the current environment. Calls and expressions are evaluated. 


\section{Infrastructure}

This section describes the infrastructure we used to run the dynamic program
analysis to record the calls to evals. Our analysis pipeline starts with
downloading and installing the open source R packages from CRAN. The Kaggle
kernels are downloaded from Kaggle.com by a crawler and handled separately.
Next, we extract code size and coverage metrics from the installed packages
using the covr package.
This is followed by extraction of runnable programs from R packages: tests,
examples and vignettes. The extracted programs are then executed through our
dynamic analyzer, instrumentr, which obtains relevant information from the
program runs. instrumentr is an R package that enables us to write our tracing
logic in R. It internally uses R-dyntrace which is a modified R interpreter that
exposes hooks for events of interest. instrumentr provides an API to execute R
callbacks from these hooks.
Finally, we use RMarkdown notebooks to analyze and visualize this data.
Our infrastructure contains three
components:
Figure~\ref{fig:pipeline} shows the main steps of our pipeline.

\begin{figure*}[!tb]\centering\includegraphics[width=\linewidth]
  {pipeline.pdf}\caption{Pipeline}\label{fig:pipeline}
\end{figure*}

\section{Corpus}

In this study, we report on eval usage in a corpus of \CorpusCorePackages R core
and \CorpusPackages popular CRAN packages. To quantify the eval use, we run
\CorpusAllProgramsRnd of R programs that are extracted form those packages as
well as from the user-written data-analyses shared on the Kaggle platform. In
this section we describe this corpus and methodology used to assemble it.

\subsection{CRAN Packages}

We have selected the TOP \CorpusPackages packages based on the number of reverse
dependencies, \ie how many other packages depend on them\footnote{Extracted from
  package metadata using builtin function}. Using this criterion helps us to
choose popular packages which in general should be of a better quality and have
better code coverage and therefore more code to run. The resulting set has
\summary{CorpusRevdeps}\footnote{Such triples represent ($min$ / $mean$ /
  $max$)} reverse dependencies and consists of \CorpusRCodeRnd lines of R code
and \CorpusNativeCodeRnd lines of native code (C/C++/Fortran)\footnote{Excluding
  comments and blank lines using \url{github.com/AlDanial/cloc}}.
Figure~\ref{fig:corpus} shows these packages, the size of the dots reflects the
project's size in lines of code including both R and native code, the x-axis
indicates the expression code coverage in percents and the y-axis gives the
number of reverse dependencies in log scale. Dotted lines indicate means.
Packages with over \CorpusEvalsPackageTreshold eval call sites are annotated.

\begin{figure*}[!tb]\centering\includegraphics[width=\linewidth]
  {corpus.pdf}\caption{CRAN packages}\label{fig:corpus}
\end{figure*}

All included packages come from the Comprehensive R Archive Network
(CRAN\footnote{\url{http://cran.r-project.org}}), the largest repository of R
code with over \CorpusAllCranRnd packages\footnote{CRAN receives about 6 new
  package submissions a day~\cite{Ligges2017}}. Unlike other open code
repositories such as GitHub, CRAN is a curated repository. Each submitted
package must abide to a number of well-formedness rules that are automatically
checked asserting certain quality. Most relevant for this work is that all of
the \emph{runnable code} is tested and only a successfully running package is
admitted in the archive. This is important as we use this runnable code for the
dynamic analysis of eval calls.

There are three sources of runnable code in a R package. Next to the package
\emph{tests}, there are also \emph{examples} and \emph{vignettes}. Examples are
R code snippets extracted from package documentation into scripts file.
Vignettes are long-form description of package functionality written using
literate-programming style in \LaTeX\xspace or Rmarkdown with chunks of R code
that can be similarly extracted into R scripts. R provides builtin functions for
tangling the embedded code into files.

The selected packages contain \CorpusPackagePrograms programs with
\CorpusPackageProgramsCodeRnd lines of R code:
%
\begin{compactitem}[$-$]
  \item \CorpusExamplesProgramsRnd examples with \CorpusExamplesCodeRnd
  \summaryrnd{CorpusExamplesCode} lines of code,
  \item \CorpusTestsProgramsRnd tests with \CorpusTestsCodeRnd
  \summaryrnd{CorpusTestsCode} lines of code, and
  \item \CorpusVignettesProgramsRnd vignettes with \CorpusVignettesCodeRnd
  \summaryrnd{CorpusVignettesCode} lines of code.
\end{compactitem}

For each package we use its runnable code to compute the package code
coverage\footnote{Computed using \url{ https://github.com/r-lib/covr}}. On
average, across the \CorpusPackages packages, it is \CorpusMeanExprCoverage.

\subsection{Kaggle Scripts}

To represent user-written code in the corpus, we turned to
Kaggle\footnote{\url{https://www.kaggle.com}}, an online platform for
data-science and machine-learning practitioner. Among others, the website allow
people to share data-science competitions, a data-analysis problem together with
data, for which users try to find the best solution. The solutions, called
\emph{kernels}, are then shared back to the platform in the form of either plain
scripts or as Rmarkdown / Jupyter notebooks written in either Julia, Python, R
or SQLite.

One of the most popular competition is about predicting passenger survival on
Titanic\footnote{\url{https://www.kaggle.com/c/titanic}} with \CorpusKaggle
kernels in R (over 1/4 of all available R kernels) which we used for our corpus.

Unlike CRAN, Kaggle is not a curated repository and therefore there are no
guarantees about the quality of the code. After downloading all of the
\CorpusKaggle kernels and extracting the R code from the various
formats\footnote{We use \texttt{rmarkdown} package to convert from Jupyer
  notebook to Rmarkdown as well as to convert from Rmarkdown to plain R}, we
found that \CorpusDuplicatedKaggle were whole-file duplicates. From
\CorpusRunnableKaggle kernels, \CorpusFailedKaggle failed to execute. Next to
various runtime exceptions, common problems were parse errors and misspelled
package names.

The final set contains \CorpusFinishedKaggle kernels with
\CorpusFinishedKaggleCodeRnd \summaryrnd{CorpusFinishedKaggleCode} lines of R
code.

\subsection{Summary}

Together, we have assembled a corpus of \CorpusPackages R packages and
\CorpusFinishedKaggle user-written data-analysis scripts. Together it contains
\CorpusAllCodeRnd lines of source code (both and R and native code). Out of
this, \CorpusAllRunnbaleCode lines R code spread across \CorpusAllPrograms R
scripts is the code that we run during the dynamic analysis.

\section{Usage Metrics}

This section presents an overiew of the usage of \eval in our corpus.
Previous work reported that 82\% of JavaScript-enabled web pages used
\eval~\cite{ecoop11}. The case for R is more striking with 100\% of the
programs using that \eval.\footnote{In this section, we combine calls to
  \c{eval}, \c{eval.parent}, \c{evalq} and \c{local}.}  This high number is
explained by the fact that the R implementation, its core packages, use
\eval extensively. In our analysis, we differentiate uses of \eval based on
where they originate from. A {\bf core} \eval is one that occurs in the code
of a core package.  A {\bf dependent} \eval is one that occurs in a package
that was included by the target package. An {\bf own} \eval is one that
occurs in the source code of the target package.

Over all, we observed \AllAllCallCountRnd calls to \eval in the
\CorpusAllProgramsRnd programs in our corpus.  Table~\ref{A} details this
run-time information. The format is T (m/M/x) where T is the total number of
calls observed for all runs of the corpus, m and x are the minimum and
maximum number of calls for any given run, and M is the mean number of calls
per run. We can observe that an average run of CRAN package will trigger 581
{\eval}s in core package, 532 in dependent packages and 292 in the package
itself.  The Kaggle data set has small programs, and, these programs only
trigger \eval in core and dependent packages. Thus, an average Kaggle run
has 412 core calls, 96 dependent calls, and no own calls.


\begin{table}[ht]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}r|ccc@{}} \hline
Source & Core & Dependent & Own \\\hline
      %      CRAN & \CranCoreTotalEvalCallsRnd (\CranCoreMinEvalCallsRnd/\CranCoreAvgEvalCallsRnd/\CranCoreMaxEvalCallsRnd) & \CranDependentTotalEvalCallsRnd (\CranDependentMinEvalCallsRnd/\CranDependentAvgEvalCallsRnd/\CranDependentMaxEvalCallsRnd) & \CranOwnTotalEvalCallsRnd (\CranOwnMinEvalCallsRnd/\CranOwnAvgEvalCallsRnd/\CranOwnMaxEvalCallsRnd) \\
CRAN & \CranCoreTotalEvalCallsRnd (1 / 581 / 416K)  & \CranDependentTotalEvalCallsRnd (1 / 532 / 183K) & \CranOwnTotalEvalCallsRnd (1 / 292 / 144K)\\
Kaggle & \KaggleCoreTotalEvalCallsRnd (1 / 412 / 2.8K) & \KaggleDependentTotalEvalCallsRnd (3 / 96 / 1.1K) & 0 (0 / 0 / 0) \\
%      Kaggle & \KaggleCoreTotalEvalCallsRnd (\KaggleCoreMinEvalCallsRnd/\KaggleCoreAvgEvalCallsRnd/\KaggleCoreMaxEvalCallsRnd) & \KaggleDependentTotalEvalCallsRnd (\KaggleDependentMinEvalCallsRnd/\KaggleDependentAvgEvalCallsRnd/\KaggleDependentMaxEvalCallsRnd) & 0(0/0/0) \\
      \hline
    \end{tabular}
  }
  \caption{Eval calls in CRAN and Kaggle}
  \label{A}
\end{table}



%% The following table will be replaced. {\bf <At this point separating between
%%   the different kinds of evals is distracting. We have not argued why it is
%%   important.>}

%% \begin{table}[ht]\resizebox{\columnwidth}{!}{\begin{tabular}{l|rr|r||rr|r}\hline
%% \multicolumn{1}{c}{Function}& \multicolumn{3}{c}{Calls} & \multicolumn{3}{c}{Callsites}\\
%% \cline{1-1} \cline{2-4} \cline{5-7}
%% & Core & Packages & Total & Core & Packages & Total\\\hline
%% \c{eval} & \CoreEvalCallCountRnd & \PackageEvalCallCountRnd  & \AllEvalCallCountRnd & \CoreEvalSiteCountRnd & \PackageEvalSiteCountRnd &  \AllEvalSiteCountRnd\\
%% \c{eval.parent} & \CoreEvalParentCallCountRnd & \PackageEvalParentCallCountRnd  & \AllEvalParentCallCountRnd & \CoreEvalParentSiteCountRnd & \PackageEvalParentSiteCountRnd & \AllEvalParentSiteCountRnd\\
%% \c{evalq} & \CoreEvalqCallCountRnd & \PackageEvalqCallCountRnd  & \AllEvalqCallCountRnd & \CoreEvalqSiteCountRnd & \PackageEvalqSiteCountRnd & \AllEvalqSiteCountRnd \\
%% \c{local} & \CoreLocalCallCountRnd & \PackageLocalCallCountRnd  & \AllLocalCallCountRnd & \CoreLocalSiteCountRnd & \PackageLocalSiteCountRnd & \AllLocalSiteCountRnd \\\hline
%% Total & \CoreAllCallCountRnd & \PackageAllCallCountRnd  & \AllAllCallCountRnd & \CoreAllSiteCountRnd & \PackageAllSiteCountRnd & \AllAllSiteCountRnd \\  \hline
%% \end{tabular} } \label{table:eval-count-summary}
%%  \caption{Distribution of eval calls and callsites in the corpus}
%% \end{table}

%% \ref{table:top-ten-package-summary} shows the number of calls to eval
%% made by the ten most frequent callers to eval. These packages together have
%% \missingNumber callsites and account for \missingPercentage calls to eval.
%% Furthermore, the top ten callsites account for \missingPercentage of package
%% eval calls. A single callsite of \TopTenPackageNameA accounts for over
%% \missingPercentage calls to eval.

%% \begin{table}[ht]
%%   \resizebox{\columnwidth}{!}{%
%%     \begin{tabular}{l|rrr||l|rrr}
%%       \hline
%%       \multicolumn{4}{c}{}& \multicolumn{4}{c}{} \\
%%       \cline{1-4} \cline{5-8}
%%       Package & Callsites & Calls & Calls \% & Package & Callsites & Calls & Calls \%\\
%%       \hline
%%       \TopTenPackageNameA & \TopTenPackageCallsiteCountA & \TopTenPackageCallCountA &\TopTenPackageCallPercA & \TopTenPackageNameF & \TopTenPackageCallsiteCountF & \TopTenPackageCallCountF & \TopTenPackageCallPercF \\
%%       \TopTenPackageNameB & \TopTenPackageCallsiteCountB & \TopTenPackageCallCountB &\TopTenPackageCallPercB & \TopTenPackageNameG & \TopTenPackageCallsiteCountG & \TopTenPackageCallCountG & \TopTenPackageCallPercG \\
%%       \TopTenPackageNameC & \TopTenPackageCallsiteCountC & \TopTenPackageCallCountC &\TopTenPackageCallPercC & \TopTenPackageNameH & \TopTenPackageCallsiteCountH & \TopTenPackageCallCountH & \TopTenPackageCallPercH \\
%%       \TopTenPackageNameD & \TopTenPackageCallsiteCountD & \TopTenPackageCallCountD &\TopTenPackageCallPercD & \TopTenPackageNameI & \TopTenPackageCallsiteCountI & \TopTenPackageCallCountI & \TopTenPackageCallPercI \\
%%       \TopTenPackageNameE & \TopTenPackageCallsiteCountE & \TopTenPackageCallCountE &\TopTenPackageCallPercE & \TopTenPackageNameJ & \TopTenPackageCallsiteCountJ & \TopTenPackageCallCountJ & \TopTenPackageCallPercJ \\
%%       \hline
%%     \end{tabular}
%%   }
%%   \label{table:top-ten-package-summary}
%%   \caption{Distribution of eval calls and callsites in the top ten packages}
%% \end{table}


Table \ref{table:site-package-summary} shows the results of static analysis
for the CRAN packages in our corpus. 89 packages have a single call site,
and one package has \miss{252} call sites of \eval. \miss{XXX} packages have
no calls to \eval. The core packages have \miss{41} callsites of eval and
Kaggle has none. These static numbers are under approximations as \eval may
be called reflectively, through aliases, or passed to a higher-order function.

\begin{table}[ht]\resizebox{\columnwidth}{!}{%
\begin{tabular}{rr|rr|rr}\hline
Callsites & Packages & Callsites & Packages & Callsites & Packages \\ \hline
\SiteSummarySiteCountA & \SiteSummaryPackageCountA & \SiteSummarySiteCountF & \SiteSummaryPackageCountF & \SiteSummarySiteCountK & \SiteSummaryPackageCountK \\
\SiteSummarySiteCountB & \SiteSummaryPackageCountB & \SiteSummarySiteCountG & \SiteSummaryPackageCountG & \SiteSummarySiteCountL & \SiteSummaryPackageCountL \\
\SiteSummarySiteCountC & \SiteSummaryPackageCountC & \SiteSummarySiteCountH & \SiteSummaryPackageCountH & \SiteSummarySiteCountM & \SiteSummaryPackageCountM \\
\SiteSummarySiteCountD & \SiteSummaryPackageCountD & \SiteSummarySiteCountI & \SiteSummaryPackageCountI & \SiteSummarySiteCountN & \SiteSummaryPackageCountN \\
      \SiteSummarySiteCountE & \SiteSummaryPackageCountE & \SiteSummarySiteCountJ & \SiteSummaryPackageCountJ & \SiteSummarySiteCountO & \SiteSummaryPackageCountO \\    \hline
\end{tabular}} \label{table:site-package-summary}
\caption{Distribution of callsites in CRAN}
\end{table}


To roughly estimate the complexity of the code being evaluated,
Fig.~\ref{fig:ast-size} shows the distribution of the abstract syntax tree
size of the \c{expr} argument to \eval. The majority of inputs across core
and packages (\miss{XX}\%) is 1. That is to say, the code being evaluated
consists of a single variable name or constant. A function call needs at
least three nodes. Inputs smaller than \c{X} nodes account for 95\% of all
calls. Sizes are roughly similar between core and packages.

\begin{figure}[htbp]
\includegraphics[width=\columnwidth]{ast_sizes}
\caption{Input sizes} \label{fig:ast-size}
\end{figure}

\medskip\noindent\emph{Discussion:} Comparing the use of \eval in R and
JavaScript is instructive. The use of \eval is more widespread in data
science code than it was in JavaScript. Any run of an R program seems to
trigger \eval, more over package programmers use \eval liberally. On the
other hand, the authors of Kaggle packages do not use the feature at all.
This point to a bifurcated user community, experts who write package know
how to use \eval while end-users do not. An alternative explanation is that
end-users do not need \eval for simple data analysis tasks. This bifurcation
was not observed in the JavaScript community, but this may have changed with
the advent of Node.js ecosystem.  The distribution of input sizes is similar
to that found in JavaScript, the majority of inputs being rather small.


{\bf Missing from this section is the measure of the number of events
  happening in an eval}

\section{A taxonomy of eval}

We first classify \c{eval} depending on the resolved expression, \emph{i.e.}
the first argument of \c{eval} after all possible function calls and symbol
resolution in it have been executed.

\subsection{Operation mix}

Here we should show some information about what happens during an eval. What kinds of operations...



\subsection{Scope}

Here we talk about scopes and envs

One of the main differences of the R \eval with the javascript one is how it
interacts with \emph{scope}, or environments in R. In Javascript, \c{eval}
can access the local scope and the global scope. In R, \c{eval} has an
argument \c{envir} to indicate in which environment its first argument,
after being resolved, must be evaluated. By default, it is equal to \c{parent.frame()}, which is the parent environment of the \c{eval} call.

 This argument can also be \c{NULL}, a list or a dataframe. In that case, \c{eval} copies \c{envir} and encloses it the environment defined by \c{enclos}. Finally, the argument can also be a number $n$. It means that the environment in which the expressions is evaluated will be the results of \c{sys.call(n)} where $n$ refers to the $n$-th call frame after the global environment.

\textbf{Do we speak more about what an environment is, \c{.GlobalEnv}, \c{emptyenv()}, environment navigation here, or in the Background section?}

We classify the environment from two points of view:
\begin{itemize}
	\item in which environment \c{eval} writes new bindings 
	\item in which \emph{existing} environment \c{eval} reads. It is the first non new environment. \c{eval} will also be able to read in all parent environment of that environment. 
\end{itemize}

% Variability: how many different envir class, and different envir expr, for one call site?

\subsection{Patterns}

Here we categorize evals in some common groups

\subsection{Provenance}

Here we talk about where the input of an eval comes from.

Although \c{eval} takes an R expression, it can evaluate a string if this string is first parsed with \c{parse}, \c{str2expression} or \c{str2lang}. This happens for \PercentParsedEvals of the eval calls, and \PercentParsedCallSites of the call sites.

This corresponds to strings loaded from a file or to metaprogramming, \ie building new variables or functions by concatenating strings together, as in Listing~\ref{code:parsepaste}. To detect this kind of metaprogramming, we look at the usage of the main concatenating functions in R, \c{paste}, \c{paste0}, \c{glue} and \c{str_c}, and \PasteParsedEvals of the evals and \PasteParsedCallSite of the call sites using a parse function actually call a concatenating function.


\begin{lstlisting}[caption={Using \c{eval} and \c{parse} to do meta-programming in package \emph{data.table}, to build the options at load time (in \emph{onLoad.R}).}, label=code:parsepaste]
eval(parse(text=paste0("options(",i,"=",opts[i],")")))
\end{lstlisting}

\textbf{What about a table of packages ranked by usage of parse (and/or paste...)?}


\subsection{Consistency}

Here we talk about whether the same call site sees the same pattern

{\bf Polymorphic may not be the right word.}

We look at how \emph{polymorphic} the \c{expr} argument of \eval can be, \ie
how many different types of the resolved \c{expr} there are per call sites,
in Figure~\ref{fig:polymorphism}. \PercentMonomorphic of the call sites are
\emph{monomorphic}.

\begin{figure}[!h]
    \includegraphics[width=\columnwidth]{polymorphism}
    \caption{Degree of polymorphism of \c{expr} per call sites.} \label{fig:polymorphism}
\end{figure}


\section{Case studies}

\begin{itemize}
\item Escape Hatch: \c{eval(unlock_binding ... )} to prevent R CMD check warnings
\item Accessing data frame columns like local variables: \c{eval(x + y + z, data.frame(x, y z))}
\item Poor man's sandboxing: local function
\item Assembling call strings from components passed by callers: eval(substitute(f(x, y, z)))
\item Convenience: 
\item Dynamic code loading: \c{eval(parse(...))}. The textual input in parse can come from three places: a string, a file or in interactive R sessions directly from user input. Next to \c{parse}, there are also \c{str2expression} and \c{str2lang} which are a specialized versions of parse that do not keep source references and thus are a bit faster.
\item bypassing R CMD check in the case one accesses dangerous functions such as unlockBinding or private symbols.
\item evaling recorded calls in model-fitting functions or passing most of the current call to another function. Both ``patterns'' use the \c{match.call} to capture current call. The latter is just for laziness as it is the same as calling the new function with explicitly selecting the arguments.
\end{itemize}

\subsection{base}

There are 30 functions in \emph{base} that use \eval. Only five of them use \c{parse}, for legitimate reasons, such \c{source} and \c{sys.source} which load a file and then evaluate it, or \c{invokeRestartInteractively} which prompts the user for arguments to restart execution after an error.

We endeavored  to write these functions without \eval. \c{match.arg} appears in \MatchArgPercent of the \eval calls. It is used to get the arguments, possibly abbreviated, of a function. With one argument, as \c{match.arg(arg)}, it uses \c{eval} to infer them from a parent function's formal arguments. We can rewrite it using its version with two arguments, \c{match.arg(arg, choices)} where \c{choices} are the possible formal arguments of the function, as shown in Listing~\ref{code:matcharg}.

\begin{lstlisting}[caption={Rewriting \c{match.arg} without \eval}, label=code:matcharg]
f <- function(type=c("examples", "tests", "vignettes")) {
actual_type = match.arg(type)
...
}

f <- function(type=c("examples", "tests", "vignettes")) {
actual_type = match.arg(type, choices=c("examples", "tests", "vignettes"))
...
}
\end{lstlisting}

\section{Threats to Validity}

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{bib/bibliography,bib/jv}

\end{document}
