\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{hyperref}
\newcommand{\missingTag}[1]{\textcolor{red}{#1}\xspace}
\newcommand{\missingNumber}{\textcolor{red}{XX}\xspace}
\newcommand{\missingPercentage}{\textcolor{red}{XX\%}\xspace}
\newcommand{\missingTable}{\textcolor{red}{XXTable}\xspace}
\newcommand{\missingGraph}{\textcolor{red}{XXGraph}\xspace}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{paralist}
\usepackage{xspace}
\usepackage{url}

\input{tag/counts.tex}
\input{tag/corpus.tex}
\input{tag/side-effects.tex}
\input{tag/operations.tex}
\input{tag/events.tex}
\input{tag/analysisStable.tex}
\input{tag/enrichDataset.tex}

\graphicspath{{img/}}

\lstset{ 
 language=R,              % the language of the code
 basicstyle=\small\sffamily,    % the size of the fonts
% numbers=left,            % where to put the line-numbers
 numberstyle=\color{Blue},% the style that is used for the line-numbers
 stepnumber=1,            % the step between two line-numbers. If it is 1,
                          % each line will be numbered
 numbersep=2pt,           % how far the line-numbers are from the code
 backgroundcolor=\color{white},  % choose the background color.
 showspaces=false,        % show spaces adding particular underscores
 showstringspaces=false,  % underline spaces within strings
 showtabs=false,          % show tabs within strings adding particular 
%frame=single,            % adds a frame around the code
 rulecolor=\color{black}, % if not set, the frame-color may be changed on
                          % line-breaks within not-black text (e.g. commens
                          % (green here))
 tabsize=2,               % sets default tabsize to 2 spaces
 captionpos=b,            % sets the caption-position to bottom
 breaklines=true,         % sets automatic line breaking
 breakatwhitespace=false, % sets if automatic breaks should only happen at 
 keywordstyle=\color{Blue},    % keyword style
 commentstyle=\color{YellowGreen},  % comment style
 stringstyle=\color{ForestGreen}    % string literal style
}

% https://tex.stackexchange.com/questions/351435/table-name-placment-in-tabular-documentclass-ieeetran-conference
\makeatletter
\def\@IEEEtablestring{figure}
\makeatother

\begin{document}

\title{A Large-Scale Study of the Use of Eval in R}
\author{\vspace{-.8cm}\IEEEauthorblockN{~}} %\IEEEauthorblockA{ dept} \\ \and
\maketitle

\newcommand{\eg}{\emph{e.g.},\xspace}
\newcommand{\ie}{\emph{i.e.},\xspace}
\newcommand{\cf}{\emph{cf.}\xspace}
\newcommand{\summary}[1]{{\csname #1\endcsname} ({\csname #1Min\endcsname} / {\csname #1Mean\endcsname} / {\csname #1Max\endcsname})}
\newcommand{\summaryrnd}[1]{{\csname #1Rnd\endcsname} ({\csname #1MinRnd\endcsname} / {\csname #1MeanRnd\endcsname} / {\csname #1MaxRnd\endcsname})}

\newcommand{\eval}{\c{eval}\xspace}
\newcommand{\parse}{\c{parse}\xspace}
\newcommand{\source}{\c{source}\xspace}
\newcommand{\local}{\c{local}\xspace}
\newcommand{\unlockBinding}{\c{unlockBinding}\xspace}
\newcommand{\substitute}{\c{substitute}\xspace}
\newcommand{\datatable}{\c{data.table}\xspace}
\newcommand{\mlogit}{\c{mlogit}\xspace}
\newcommand{\mboost}{\c{mboost}\xspace}
\newcommand{\metafor}{\c{metafor}\xspace}
\newcommand{\lavaan}{\c{lavaan}\xspace}
\newcommand{\mclust}{\c{mclust}\xspace}
\newcommand{\gamlss}{\c{gamlss}\xspace}
\newcommand{\ggproto}{\c{ggproto}\xspace}
\newcommand{\ggplot}{\c{ggplot2}\xspace}

\newcommand{\base}{\c{base}\xspace}
\renewcommand{\c}[1]{\lstinline{#1}\xspace}
\newcommand{\miss}[1]{{\textcolor{red}{#1}}\xspace}
\newcommand{\evil}{\emph{evil}\xspace}
\newcommand{\instrumentr}{\emph{instrumentr}\xspace}
\newcommand{\rdyntrace}{\emph{R-dyntrace}\xspace}
\newcommand{\covr}{\emph{covr}\xspace}
\newcommand{\runr}{\emph{runr}\xspace}

\begin{abstract}
  The \eval function turns text into code. From the point of view of
  reasoning about software system, uses of \eval turn the program's code
  from a statically known quantity that can be reasoned about and analyzed
  into one of the inputs to the program. Thus, widespread use of \eval
  hinders our ability to provide safety and security guarantees for
  software. Understanding how \eval is used in practice is key to finding
  ways to mitigate its damage. One question that has not been studied is
  whether the usage of \eval is dependent on the application domain or other
  features of the language. This paper is a large-scale study of a corpus of
  \CorpusAllCodeRnd lines of data science libraries and end-user written in
  the R language. We performed a sophisticated dynamic analysis to confirm
  that \eval is indeed in wide spread use, and to categorized patters of
  usage. We found that \eval in this code base is more dangerous in some
  ways, and safer in others, than what was previously reported for
  JavaScript.
\end{abstract}

\section{Introduction}

Traditional program analysis techniques are based on sound reasoning about
program behavior, a program analyzer computes an over-approximation of the
set of possible operations performed by the system being
analyzed~\cite{cc77}.  This is predicated on knowing the program's code, and
computing that code's behavior for all possible inputs. The presence of
\eval entails that the program's input, any string variable, may be
reflected into its code giving rise to \emph{any} behavior allowed by the
language semantics. In many dynamic languages, \eval can redefine most
user-defined and built-in functions resulting in a complete loss of
precision for any operation following \eval. A group of influential
resaerchers argued to give up on soundness and, instead, to
under-approximate dynamic features~\cite{soundy} In their words ``a
practical analysis, therefore, may pretend that \eval does nothing, unless
it can precisely resolve its string argument at compile time.''  Assuming
that \eval does not have side-effects may be too optimistic. We rather
propose to study what \eval does in practice, in real-world code, so as to
give researchers data about how to better under-approximate uses of that
feature.

While there has been previous work investigating the usage of \eval in web
programming and JavaScript~\cite{ecoop11}, there are no other studies that
shed light on the usage of \eval in other programming languages and
application domains.  It is thus reasonable to wonder if the results from
previous work can be carried over to other contexts. For this paper, we have
picked the data science language R~\cite{r} as our object of study. R is
both different from JavaScript in linguistic terms and in terms of its user
base. JavaScript was designed to run untrusted code in browser, while R was
design for statistical computing on a desktop. JavaScript is used as a
general purpose language by a wide community of programmers; while R is used
for scientific computing by data scientist and domain experts with, often,
limited programming experience.

This paper sets out to highlight the differences in usage patterns of \eval
between these two languages and user communities. Our results are broadly
applicable as they increase our understanding of the range of uses of \eval,
present new use-cases, and, hopefully, provide insights into reasonable
under-approximations. More narrowly, for researchers interested in static
analysis of data science code, we provide a publicly available corpus of
programs and a toolchain that can be customized to deliver fine-grained
information about the usage and impact of \eval.

Our methodology for this work bows to the fact that static analysis of R is
difficult, instead we focus on dynamic analysis. To observe \eval we have
built a two-level monitoring infrastructure: we are able to monitor R
programs by instrumentation -- this gives us access to many user-visible
properties of R programs -- and we also can monitor the inner working of the
R interpreter -- this allows us to capture details of execution that are not
exposed at the source level.

Our corpus has been constructed to reflect the different level of
sophistication in the R community. We distinguish between the core
implementers of the language, developers with both extensive programming
experience and keen understanding of the language semantics, library
implementers, developers with some programming experience and a working
knowledge of R, and end-users, who are typically not expert programmers and
often have only a cursory knowledge of the language.\footnote{To illustrate
  the above consider that R uses lazy evaluation like Haskell. The authors
  have informally surveyed end-users, including computer scientists, and did
  not find a single one aware of this fact. Library developers, on the other
  hand, know about laziness and program defensively around it.}  Thus, our
corpus distinguished between \emph{core packages} (there are
\CorpusCorePackages such packages that are distributed together with the R
virtual machine), \emph{CRAN packages} (we selected \CorpusPackages curated
packages that pass stringent community quality checks and are equipped with
tests and sample data), \emph{Kaggle scripts} (we used \CorpusFinishedKaggle
end-user written programs that performs a particular data analysis
task). There are reasons to believe that \eval usage patterns may differ
between these datasets. The core R code is extremely stable, it has only
been maintained for the last 20 years with very few far reaching
changes. The libraries represent a more lively ecosystem with new libraries
added each day.  Finally, end user code is often one-use scripts thrown
together and sometimes never revisited.


To understand the behavior of eval we perform a dynamic analysis and manual
code inspection. The results presented here summarize our findings.

Our infrastructure is publicly available, released in open source, and our
data, and code will be submitted to artifact evaluation.


\section{Background and Related Work}

The origins of \eval go back to the first implementation of the Lisp
programming language~\cite{lisp}. Over the years, \eval has been included in
many dynamic languages, with some variability in expressive power and
nomenclature. In all of its incarnation \eval allow developpers to
dynamically extend the code of their program at run-time.  Originally, \eval
was limited to interpreted languages, but with the advent of just-in-time
compilation this restriction was gradually lifted.

Some languages have chosen to restrict \eval. The essential difference
between languages relates to scoping of variables that can be accessed from
code executed by \eval.  Java and Julia are among the more restrictive
languages. In Java, \eval must be synthesized as a combination dynamic
loading and reflection.\footnote{While Java does not have \eval built-in,
  one can take the input to \eval, wrap it in a static method of an
  anonymous class, generate bytecode for that class, invoke the Java class
  loader to dynamically install the new code, and then use reflection to
  call the new method.} Julia provides \eval more directly but limits its
scope to top level variables and methods. JavaScript support both a,
so-called, global \eval, and a local \eval. Where the latter has acces to
local variables of the function being executed (as well as variables of
lexically enclosing scopes). Finally languages such as R allow \eval to
manipulate variables defined in the caller environment of the function in
which \eval is invoked.

The difference between these design choices has an impact on the ability of
compilers to optimize, and of analysis tools to reason about code. The
impact of a top level \eval is that any global variable may be modified, but
currently running functions can continue to execute unchanged. A local
\eval, may change the values of local variables, their types, or even their
presence. This is a much more invasive change, one that may invalidate
currently executing functions as it may change the results of common
optimizations such as constant folding or copy propagation. The designers of
the Julia programming language were aware of this difference and
purposefully chose to restrict \eval to the top level in order to preserve
their ability to optimize code~\cite{oopsla18a}. They even added a
versioning mechanism to function definitions to ensure that code added
during an \eval would not invalidate optimizations such as inlining.  R's
ability to affect any one of the callers of a function is even more invasive
as the impact of \eval is non-local. Such unfettered power is difficult to
deal with.

The intuition behind the soundiness manifesto~\cite{soundy} is that while
the use of a single \eval may invalidate every single invariant computed by
a static analyzer, this worst case usually does not happen.  In many cases,
\eval is used carefully to limited effect. This hypothesis must be validated
and, for it to be useful, we need to have a way to make some educated
guesses as to which side-effects will result from any particular \eval.

Richards et al.~\cite{ecoop11} provided the first large-scale study of the
runtime behavior of \eval in JavaScript. They analyzed a corpus of the
10,000 most popular websites. They used an instrumented web browser to
gather execution traces and perform a dynamic analysis.  Their results show
that \eval is pervasive with 82\% of the most popular websites using it. The
reasons for using \eval include the desire to load code on demand (to reduce
the time it takes to display a page), the deserialization of JSON data
(about 37\% of all calls) and lightweight meta-programming to customize web
pages.  While many uses \eval were legitimate, many were unnecessary and
could be replaced with equivalent and safer code.  The reported on a
categorization of inputs to \eval that covers the vast majority of input
strings.  Restricting themselves to \eval in which all named variables refer
to the global scope, many patterns can be replaced by more disciplined
code~\cite{oopsla12b}.  The limitation of that work are that there was no
measure of code coverage, so the numbers presented can be a lower bound on
the possible behaviors.  Furthermore in 2011, JavaScript usage is quite
different from today. Their work captured very few libraries and none of the
more recent Node.js ecosystem.  More details about dynamic analysis of
JavaScript can be found in~\cite{liang}.

Wang et al.~\cite{wang} analyzed usage of dynamic features in 18 Python
programs include dynamic code generation to find if they affects file
change-proneness.  They reported that files with dynamic features are
significantly more likely to be the subject of changes, than other files.
Chen et al. looked at the correlation between code changes and dynamic
features, including \eval, in 17 Python programs~\cite{chen}. They did not
observe many uses of \eval.  Callau et al. performed an empirical study of
the usage of dynamic features in the 1,000 largest Smalltalk projects in the
Squeaksource repository, accounting for more than 4 million lines of
code~\cite{oscar}. While \eval itself is not present, Smalltalk has a rich
reflective interface that gives programmer great flexibility. Their work was
based on a static analysis.

Morandat et al.~\cite{ecoop12} had a short section on the usage of \eval in
R. They found the \eval function to be widely used in R code with 8500 call
sites in CRAN and 2.2 million dynamic calls. The 15 most frequent call sites
account for 88\% of eval. The \c{match.arg} function is the highest user
with 54\% of all calls. In the other call sites, they saw two uses
cases. The most common is the evaluation of the source code of a promise
retrieved by \c{substitute} in a new environment. This is done in the
\c{with} function. The other use case is the invocation of a function whose
name or arguments are determined dynamically. For this purpose, R provides
\c{do.call} and thus using \eval is overkill.

\section{R and The Design of Eval}

The R Project is a tool for data analysis.  At its heart is a {vectorized,
  dynamic, lazy, functional, object-oriented} programming language with an
unusual combination of features~\cite{ecoop12} designed to ease learning by
non-programmer and enable rapid development of statistical
methods~\cite{R96}.  The language was designed as a successor to
S~\cite{S88}.

\begin{figure*}[!t]\centering\includegraphics[width=.8\linewidth]
{pipeline.pdf}\caption{Analysis Pipeline}\label{fig:pipeline}
\end{figure*}


Most R data types are vectorized. Values are constructed by the \c{c(...)}
function, e.g., \c{c("hi","ho")} creates a vector of two strings.  R does
not differentiate scalars from vectors. To enable equational reasoning, R
copies values accessible through multiple variables when they are updated.
Values can be tagged by user-defined attributes. For instance, one can
attach the attribute \c{dim} to the value \c{x<-c(1,2,3,4)} with
\c{attr(x,"dim")<-c(2,2)}.  This causes arithmetic functions to treat \c x
as a 2x2 matrix. Another attribute is \c{class} which can be bound to a list
of names, e.g., \c{class(x)<-"human"}. This sets the class of \c{x} to
\c{human}, classes are used for object-oriented dispatch.  Every R
linguistic construct is desugared to a function call, even control flow
statements, assignments, and bracketing. Furthermore, all functions can be
redefined in user code. This makes R both flexible and challenging to
compile~\cite{dls19}.

Arguments to user-defined functions are bundled into thunks called
promises. Logically, a promise combines an expression's code, its
environment, and its value.  To access the value of a promise, one must
force it. Forcing a promise triggers evaluation and the computed value is
captured for future reference.


R supports reflection and meta-programming. The function
\c{substitute(e,envir)} yields the parse tree of the expression \c{e} after
performing substitutions defined by the bindings in \c{envir}.

\begin{lstlisting}
> substitute(expression(a+b), list(a=1)))  
expression(1+b)
\end{lstlisting}

\noindent
R allows programmatic manipulation of parse trees, which are themselves
first class objects. They are evaluated using the \c{eval()}
function. There are three alternative forms  of \eval in the \c{base} package:
\c{evalq}, \c{eval.parent}, \c{local}.

\begin{lstlisting}
 eval(expr,
      envir = parent.frame(),
      enclos = if(is.list(envir) || is.pairlist(envir))
               parent.frame() else baseenv())
\end{lstlisting}           

Arguments are as follows: \c{expr} is the value to be evaluated, \c{envir}
is the environment in which it is to be evaluated, and \c{enclos} is
relevant when \c{envir} is a list or a data frame, then it specifies where R
looks for objects not found in \c{envir}. This can be \c{NULL} (interpreted
as the base package environment, \c{baseenv()}) or an environment.

When called, \eval evaluates \c{expr} in the environment specified by
\c{envir} and return the computed value. If \c{envir} is missing, then the
default is \c{parent.frame()} (the environment where the call to \eval was
made).  Values to be evaluated can be of types \c{call} (AST for function
calls) or \c{expression} (AST nodes for arbitrary expressions), \c{name}
(when the symbol is looked up in the current scope and its binding is
evaluated), promise, or any of the basic types such as vectors and
environments (which are returned unchanged).

The \c{evalq} form is equivalent to \c{eval(quote(expr), ...)}.  Quoting
that argument prevents if from being evaluated in the current environment.
In the following code snippet, variable \c{r} is first looked up in
the current scope with \eval whereas it is looked up in \c{env} with
\c{evalq}.

\begin{lstlisting}
> r <- 5
> env <- new.env()
> env$r <- 9
> eval(r, envir = env)
[1] 5
> evalq(r, envir = env)
[1] 9
\end{lstlisting}

\c{eval.parent(expr, n)} is a shorthand for \c{eval(expr, parent.frame(n))}.
\c{local} evaluates an expression in a local environment. It is equivalent
to evalq except that its default argument creates a new, empty
environment. This is useful to create anonymous recursive functions and as a
kind of limited namespace feature since variables defined in the environment
are not visible from the outside.


\section{Infrastructure}


\begin{figure*}[!tb]\centering\includegraphics[width=.8\linewidth]
  {corpus.pdf}\caption{CRAN packages}\label{fig:corpus}
\end{figure*}

Our infrastructure consists of a dynamic analysis pipeline to record the
calls to \eval and RMarkdown notebooks for data analysis and visualization.
Figure~\ref{fig:pipeline} shows the main steps of the pipeline. We provide
the runtime, input and output data size for each step. All runs are done on
a single server Intel Xeon 6140, 2.30GHz with 72 cores and 256GB of RAM.

The pipeline begins with downloading and installing open source R packages
from CRAN for the corpus along with their dependencies. The Kaggle programs
are downloaded from \href{http://www.kaggle.com}{Kaggle} separately by a
crawler.  Next, code size and coverage metrics from the installed packages
are extracted using the \href{ https://github.com/r-lib/covr}{\covr}
package. This is followed by extraction of runnable programs from R
packages: tests, examples and vignettes as well as from Kaggle. The code of
each extracted program is wrapped into a call to dynamic analyzer
\emph{evil} (\emph{Ev}al \emph{I}nspection \emph{L}ibrary) that collect eval
usage. The reason for this is that we only want to record eval usage for the
extracted code. Without this, the data would include eval calls from the
unit testing frameworks as well as from bootstrapping R virtual machine
itself. To avoid any interference, each program is run in its own R
instance. The \emph{evil} framework implemented as R package in 2K lines of
R and 400 lines of C++ code.

The dynamic analyzer builds upon the dynamic analysis framework, \instrumentr
that we have implemented to enable us to write dynamic analysis logic in R. It
is an R package implemented in 2.5K lines of R and 6K lines of C++ code. It
internally uses a modified R interpreter, \rdyntrace~\cite{oopsla19a}, that
exposes hooks from within the interpreter implementation for events of interest.
\instrumentr serves as an intermediary between \rdyntrace and \evil, it
intercepts the hooks exposed by \rdyntrace and attaches R functions exported by
\evil as callbacks. The \evil callbacks execute on corresponding interpreter
events.

All steps of this pipeline are parallelized using GNU
parallel~\cite{GNUparallel} and orchestrated by GNU make. To schedule and
parallelize extraction and analysis of programs, we use the \runr
package. Furthermore, \runr gracefully handles and reports failures across
large-scale program runs which greatly aids debugging of the analysis
pipeline.  The data extracted by \evil from each program is concatenated,
cleaned and summarized in the post-processing phase by custom R
scripts. Finally, the summarized data is analyzed in RMarkdown notebooks to
gather insights. Apart from the figures, the data points included in the
paper are also generated by RMarkdown notebooks as latex macros.

\section{Corpus}

In this study, we report on a corpus of \CorpusCorePackages R core and
\CorpusPackages CRAN packages. We run \CorpusAllProgramsRnd programs
extracted from those packages as well as from the user-written data-analyses
shared on the Kaggle platform. In this section we describe this corpus and
methodology used to assemble it.

\subsection{CRAN Packages}

We have selected the TOP \CorpusPackages packages based on the number of
reverse dependencies, \ie how many other packages depend on
them\footnote{Extracted from package metadata using a builtin function}.
This criterion helps us find packages which should be better written and
have better code coverage.  The resulting set contains \CorpusRCodeRnd lines
of R code and \CorpusNativeCodeRnd lines of native code
(C/C++/Fortran)\footnote{Excluding comments and blank lines using
  \url{github.com/AlDanial/cloc}}. Figure~\ref{fig:corpus} shows these
packages, the size of the dots reflects the project's size in lines of code.
The x-axis indicates code coverage in percents and the y-axis gives the
number of call sites to \eval in log scale. Dotted lines indicate
means. Packages with over \CorpusEvalsPackageTreshold eval call sites are
named.

All included packages come from the Comprehensive R Archive Network
(CRAN\footnote{\url{http://cran.r-project.org}}), the largest repository of R
code with over \CorpusAllCranRnd packages\footnote{CRAN receives about 6 new
  package submissions a day~\cite{Ligges2017}}. Unlike other open code
repositories such as GitHub, CRAN is a curated repository. Each submitted
package must abide to a number of well-formedness rules that are automatically
checked asserting certain quality. Most relevant for this work is that all of
the \emph{runnable code} is tested and only a successfully running package is
admitted in the archive. This is important as we use this runnable code for the
dynamic analysis of eval calls.

There are three sources of runnable code in a R package. Next to the package
\emph{tests}, there are also \emph{examples} and \emph{vignettes}. Examples
are R code snippets extracted from package documentation into scripts files.
Vignettes are long-form description of package functionality written using
literate-programming style in \LaTeX\xspace or Rmarkdown with chunks of R
code that can be similarly extracted into R scripts. R provides builtin
functions for tangling the embedded code into files.

The selected packages contain \CorpusPackagePrograms programs with
\CorpusPackageProgramsCodeRnd lines of R code:
%
\begin{compactitem}[$-$]
  \item \CorpusExamplesProgramsRnd examples with
  \summaryrnd{CorpusExamplesCode}\footnote{This notation represents $sum$ ($min$
    / $mean$ / $max$)} lines of code,
  \item \CorpusVignettesProgramsRnd vignettes with
  \summaryrnd{CorpusVignettesCode} lines of code.
\end{compactitem}

For each package we use its runnable code to compute the package code
coverage\footnote{Computed using \url{ https://github.com/r-lib/covr}}. On
average, across the \CorpusPackages packages, it is \CorpusMeanExprCoverage.

\subsection{Kaggle Scripts}

To represent user-written code in the corpus, we turned to
Kaggle\footnote{\url{https://www.kaggle.com}}, an online platform for
data-science and machine-learning practitioner. Among others, the website allow
people to share data-science competitions, a data-analysis problem together with
data, for which users try to find the best solution. The solutions, called
\emph{kernels}, are then shared back to the platform in the form of either plain
scripts or as Rmarkdown / Jupyter notebooks written in either Julia, Python, R
or SQLite.

One of the most popular competition is about predicting passenger survival on
Titanic\footnote{\url{https://www.kaggle.com/c/titanic}} with \CorpusKaggle
kernels in R (over 1/4 of all available R kernels) which we used for our corpus.

Unlike CRAN, Kaggle is not a curated repository and therefore there are no
guarantees about the quality of the code. After downloading all of the
\CorpusKaggle kernels and extracting the R code from the various
formats\footnote{We use \c{rmarkdown} package to convert from Jupyer
  notebook to Rmarkdown as well as to convert from Rmarkdown to plain R}, we
found that \CorpusDuplicatedKaggle were whole-file duplicates. From
\CorpusRunnableKaggle kernels, \CorpusFailedKaggle failed to execute. Next to
various runtime exceptions, common problems were parse errors and misspelled
package names.

The final set contains \CorpusFinishedKaggle kernels with
\summaryrnd{CorpusFinishedKaggleCode} lines of R code.

\subsection{Summary}

Together, we have assembled a corpus of \CorpusPackages R packages and
\CorpusFinishedKaggle user-written data-analysis scripts. Together it contains
\CorpusAllCodeRnd lines of source code (both and R and native code). Out of
this, \CorpusAllRunnbaleCode lines R code spread across \CorpusAllPrograms R
scripts is the code that we run during the dynamic analysis.

\section{Usage Metrics}

This section presents an overview of the usage of \eval in our corpus.
Previous work reported that 82\% of JavaScript-enabled web pages used
\eval~\cite{ecoop11}. The case for R is more striking with 100\% of the
programs using that \eval.\footnote{In this section, we combine calls to
  \c{eval}, \c{eval.parent}, \c{evalq} and \c{local}.}  This high number is
explained by the fact that the R implementation, its core packages, use
\eval extensively. In our analysis, we differentiate uses of \eval based on
where they originate from. A {\bf core} \eval is one that occurs in the code
of a core package.  A {\bf dependent} \eval is one that occurs in a package
that was included by the target package. An {\bf own} \eval is one that
occurs in the source code of the target package.

Over all, we observed \AllAllCallCountRnd calls to \eval in the
\CorpusAllProgramsRnd programs in our corpus.  Table~\ref{A} details this
run-time information. The format is T (m/M/x) where T is the total number of
calls observed for all runs of the corpus, m and x are the minimum and
maximum number of calls for any given run, and M is the mean number of calls
per run. We can observe that an average run of CRAN package will trigger 581
{\eval}s in core package, 532 in dependent packages and 292 in the package
itself.  The Kaggle data set has small programs, and, these programs only
trigger \eval in core and dependent packages. Thus, an average Kaggle run
has 412 core calls, 96 dependent calls, and no own calls.

 \begin{table*}[tb]
   \centering
   \begin{tabular}{r|c|c|c@{}} \hline
     Corpus / Source & Core                              & Dependent                              & Own \\\hline
     CRAN   & \summaryrnd{CranCoreEvalCalls}    & \summaryrnd{CranDependentEvalCalls}    & \summaryrnd{CranOwnEvalCalls} \\
     Kaggle & \summaryrnd{KaggleCoreEvalCalls}  & \summaryrnd{KaggleDependentEvalCalls}  & \summaryrnd{KaggleOwnEvalCalls} \\\hline
   \end{tabular}
   \caption{Eval calls in CRAN and Kaggle}
   \label{A}
 \end{table*}



%% The following table will be replaced. {\bf <At this point separating between
%%   the different kinds of evals is distracting. We have not argued why it is
%%   important.>}

%% \begin{table}[ht]\resizebox{\columnwidth}{!}{\begin{tabular}{l|rr|r||rr|r}\hline
%% \multicolumn{1}{c}{Function}& \multicolumn{3}{c}{Calls} & \multicolumn{3}{c}{Callsites}\\
%% \cline{1-1} \cline{2-4} \cline{5-7}
%% & Core & Packages & Total & Core & Packages & Total\\\hline
%% \c{eval} & \CoreEvalCallCountRnd & \PackageEvalCallCountRnd  & \AllEvalCallCountRnd & \CoreEvalSiteCountRnd & \PackageEvalSiteCountRnd &  \AllEvalSiteCountRnd\\
%% \c{eval.parent} & \CoreEvalParentCallCountRnd & \PackageEvalParentCallCountRnd  & \AllEvalParentCallCountRnd & \CoreEvalParentSiteCountRnd & \PackageEvalParentSiteCountRnd & \AllEvalParentSiteCountRnd\\
%% \c{evalq} & \CoreEvalqCallCountRnd & \PackageEvalqCallCountRnd  & \AllEvalqCallCountRnd & \CoreEvalqSiteCountRnd & \PackageEvalqSiteCountRnd & \AllEvalqSiteCountRnd \\
%% \c{local} & \CoreLocalCallCountRnd & \PackageLocalCallCountRnd  & \AllLocalCallCountRnd & \CoreLocalSiteCountRnd & \PackageLocalSiteCountRnd & \AllLocalSiteCountRnd \\\hline
%% Total & \CoreAllCallCountRnd & \PackageAllCallCountRnd  & \AllAllCallCountRnd & \CoreAllSiteCountRnd & \PackageAllSiteCountRnd & \AllAllSiteCountRnd \\  \hline
%% \end{tabular} } \label{table:eval-count-summary}
%%  \caption{Distribution of eval calls and callsites in the corpus}
%% \end{table}

%% \ref{table:top-ten-package-summary} shows the number of calls to eval
%% made by the ten most frequent callers to eval. These packages together have
%% \missingNumber callsites and account for \missingPercentage calls to eval.
%% Furthermore, the top ten callsites account for \missingPercentage of package
%% eval calls. A single callsite of \TopTenPackageNameA accounts for over
%% \missingPercentage calls to eval.

%% \begin{table}[ht]
%%   \resizebox{\columnwidth}{!}{%
%%     \begin{tabular}{l|rrr||l|rrr}
%%       \hline
%%       \multicolumn{4}{c}{}& \multicolumn{4}{c}{} \\
%%       \cline{1-4} \cline{5-8}
%%       Package & Callsites & Calls & Calls \% & Package & Callsites & Calls & Calls \%\\
%%       \hline
%%       \TopTenPackageNameA & \TopTenPackageCallsiteCountA & \TopTenPackageCallCountA &\TopTenPackageCallPercA & \TopTenPackageNameF & \TopTenPackageCallsiteCountF & \TopTenPackageCallCountF & \TopTenPackageCallPercF \\
%%       \TopTenPackageNameB & \TopTenPackageCallsiteCountB & \TopTenPackageCallCountB &\TopTenPackageCallPercB & \TopTenPackageNameG & \TopTenPackageCallsiteCountG & \TopTenPackageCallCountG & \TopTenPackageCallPercG \\
%%       \TopTenPackageNameC & \TopTenPackageCallsiteCountC & \TopTenPackageCallCountC &\TopTenPackageCallPercC & \TopTenPackageNameH & \TopTenPackageCallsiteCountH & \TopTenPackageCallCountH & \TopTenPackageCallPercH \\
%%       \TopTenPackageNameD & \TopTenPackageCallsiteCountD & \TopTenPackageCallCountD &\TopTenPackageCallPercD & \TopTenPackageNameI & \TopTenPackageCallsiteCountI & \TopTenPackageCallCountI & \TopTenPackageCallPercI \\
%%       \TopTenPackageNameE & \TopTenPackageCallsiteCountE & \TopTenPackageCallCountE &\TopTenPackageCallPercE & \TopTenPackageNameJ & \TopTenPackageCallsiteCountJ & \TopTenPackageCallCountJ & \TopTenPackageCallPercJ \\
%%       \hline
%%     \end{tabular}
%%   }
%%   \label{table:top-ten-package-summary}
%%   \caption{Distribution of eval calls and callsites in the top ten packages}
%% \end{table}


Table \ref{table:site-package-summary} shows the results of static analysis
for the CRAN packages in our corpus. 89 packages have a single call site,
and one package has \miss{252} call sites of \eval. \miss{XXX} packages have
no calls to \eval. The core packages have \miss{41} callsites of eval and
Kaggle has none. These static numbers are under approximations as \eval may
be called reflectively, through aliases, or passed to a higher-order function.

\begin{table}[ht]\resizebox{\columnwidth}{!}{%
\begin{tabular}{rr|rr|rr}\hline
 Callsites & Packages & Callsites & Packages & Callsites & Packages \\\hline
\input{tag/table-site-package-summary.tex}
\end{tabular}}
\caption{Distribution of callsites in CRAN}
\label{table:site-package-summary}
\end{table}

To roughly estimate the complexity of the code being evaluated,
Fig.~\ref{fig:ast-size} shows the distribution of the abstract syntax tree
size of the \c{expr} argument to \eval. The majority of inputs across core
and packages (\NbAstOnePercent of \eval calls and \NbAstOneCallSitePercent of call sites) have size 1. That is to say, the code being evaluated
consists of a single variable name or constant. A function call needs at
least three nodes and a list such as \c{c(1, 2, 3)} counts for 4 nodes. Inputs smaller than \AstSizeNineFive nodes account for 95\% of all
calls. Sizes in core are larger than in packages. Most large AST originate from library \emph{VGAM}, which is used for statistical modelling. It uses \eval to evaluate large matrices with values in a given environment.  

\begin{figure}[htbp]
	\centering
\includegraphics[width=\columnwidth]{ast_sizes}
\caption{Input sizes} \label{fig:ast-size}
\end{figure}

\medskip\noindent\emph{Discussion:} Comparing the use of \eval in R and
JavaScript is instructive. The use of \eval is more widespread in data
science code than it was in JavaScript. Any run of an R program seems to
trigger \eval, more over package programmers use \eval liberally. On the
other hand, the authors of Kaggle packages do not use the feature at all.
This point to a bifurcated user community, experts who write package know
how to use \eval while end-users do not. An alternative explanation is that
end-users do not need \eval for simple data analysis tasks. This bifurcation
was not observed in the JavaScript community, but this may have changed with
the advent of Node.js ecosystem.  The distribution of input sizes is similar
to that found in JavaScript, the majority of inputs being rather small.

We compare the number of events intercepted during execution of \eval across the
whole corpus. For this, we use the number of interpreter's evaluator loops as a
proxy for events. In the corpus, we observe \AllEventCountRnd events. Only
\EvalEventCountRnd events occur inside eval. This is just \EvalEventAllPerc of
all events. Table~\ref{table:event-distribution} shows the distribution of events
caused by \eval calls from R Core and CRAN packages.

\begin{table}[ht]\resizebox{\columnwidth}{!}{%
\begin{tabular}{lll}\hline
Events & Core Eval Call \% & Package Eval Call \% \\ \hline
\EventsMinRangeA--\EventsMaxRangeA & \EventsCoreEvalPercA & \EventsPackageEvalPercA \\
\EventsMinRangeB--\EventsMaxRangeB & \EventsCoreEvalPercB & \EventsPackageEvalPercB \\
\EventsMinRangeC--\EventsMaxRangeC & \EventsCoreEvalPercC & \EventsPackageEvalPercC \\ \hline
\end{tabular}} \label{table:event-distribution}
\caption{Distribution of events in \eval calls across Core and Package}
\end{table}

We observe a very wide spread in the number of events generated by \eval
calls. The largest number of events generated by an \eval call is \EventsMaxCount.
However majority of \eval calls perform up to \EventsMaxRangeA events suggesting
that most expressions passed to \eval are small.
It is interesting to note that a higher proportion of Core R \eval calls perform
side effects in the \EventsMinRangeB--\EventsMaxRangeB range.
Only \EventsCoreEvalCountC Core R \eval calls and \EventsPackageEvalCountC
package \eval call generate \EventsMinRangeC to \EventsMaxRangeC events.
These \eval calls originate from statistical modeling packages such as \mlogit,
\mboost, \metafor, \lavaan, \mclust and \gamlss.

\section{A taxonomy of eval}

In this section we turn our attention to the expression passed for evaluation to
\eval. The \eval function accepts all types of R objects as inputs, but in the
majority of cases (\AllExpressionInputEvalCallPerc), it is called with
expressions. The remaining \AllValueInputEvalCallPerc are values, \ie, they
evaluate to themselves.
%
%Furthermore, the proportion of values passed to core
%\eval call is \missingTag{CoreEvalValueInputPerc} compared to a significantly higher
%\missingTag{PackageEvalValueInputPerc} for package \eval calls.
%
Passing a value into an eval is effectively an no-op so we look closely at the
expression types the call sites pass to eval across all the runs.
\AllValueInputEvalSitePerc of all call sites always pass values,
\AllExpressionInputEvalSitePerc pass expressions and a very large proportion,
\AllPolymorphicInputEvalSitePerc, pass both expressions and values as inputs
across all program runs. We inspected the eval callsites that accept only values
or both values and expression as inputs. The top three of these call sites
contribute to over ~80\% of all calls to evals. The first one is inside
\c{match.arg} from core. Given a vector of names as strings, this function finds
the default values for parameters with those names in the caller function. Since
the default value for a function parameter can be a complex expression or a
benign value such as \c{NULL}, \c{match.arg} evaluates it through a call to
\eval. This makes the \eval inside \c{match.arg} ``polymorhpic''. The second
such \eval call originates from the popular \ggplot package of R which is a DSL
for plotting. \ggplot has it's own object system, \ggproto, which uses \eval
inside the \ggproto function to determine the super-class from which it
inherits. The default value of the superclass is \c{NULL} but in general could
be an arbitrarily complex expression. The third \eval call is in core R \c{str}
function. This function is used to print a summary of R objects. It internally
maps \eval function on the bindings of an environment which results in inputs of
multiples types being passed to \eval.

We further investigate the type of input passed to \eval in
Figure~\ref{fig:eval-expression-kind} by the type of input and
the source of \eval. In this figure, \emph{Expression} is the type of program text,
\emph{Environmment} is the type of scopes or environments used for binding,
\emph{Null} is the type of the unique \c{NULL} object, \emph{Vector} is the type
assigned to vectors of strings, booleans, integers, reals and bytes;
\emph{Symbol} is the type of unevaluated symbols, and \emph{Function} is the
type of functions.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\columnwidth]{eval-expression-kind}
  \caption{Type of Expression passed to \eval} \label{fig:eval-expression-kind}
\end{figure}

From the figure, we observe that majority of inputs to Core \eval functions are
expressions in contrast to package \eval calls which receive mostly environments
as inputs. In fact, \PackageEnvironmentInputEvalCallPerc of \eval calls in
packages receive environments as input expressions. There are only four
callsites in package evals that receive environments as expressions.
\begin{compactitem}[$-$]
\item \c{ggplot2::ggproto} contributes to 99.9\% of package \eval calls that
receive environments as input.
\item \c{R6::generator_funs} function contributes to only 936 \eval calls and
implements the same functionality as \c{ggplot2::ggproto} but for the \emph{R6}
OOP system.
\item \c{future::backtrace} function applies \eval to a future object which is
implemented as an environment. This is called only once.
\item \c{RModel::str.RMmodel} function overloads the core \c{str} method for its
\c{RModel} objects and maps \eval exactly like \c{str}. This is called only
once.
\end{compactitem}


We looked at the top ten expressions passed to core and package \eval calls. The
most frequent ten expressions to eval calls from core R contribute to 85.2\% of
all \eval calls.
\begin{table*}[tb]
  \centering
  \begin{tabular}{l|rr} \hline
    Expression & Eval Call & Eval Call \% \\\hline
    \c{c("auto", "shell", "radix")} & 1987105 & 29.3\%\\
    \c{c("auto", "shell", "quick", "radix")} & 1593169  & 23.5\%\\
    \c{\{info <- loadingNamespaceInfo(...} & 1008632 &       14.9\%\\
    \c{c("onLoad", "attach", "detach", "onUnload")}   & 470566 &      6.9\%\\
    \c{c("append", "prepend", "replace")} &              261587&       3.9\% \\
    \c{c("left", "right", "centre", "none")} & 162086     & 2.4\%\\
    \c{c("no", "ifany", "always")}   &                71580 &       1.1\%\\
    \c{c("pearson", "kendall", "spearman")}  & 72962 &      1.1\%\\
    \c{NULL}& 75330  &      1.1\% \\
    \c{Symbol}&                 66279&       1\%\\\hline
  \end{tabular}
  \caption{Top ten eval calls in Core}
  \label{A}
\end{table*}

The expression \c{\{info <- loadingNamespaceInfo(...} is added by core R
to a package directory during installation. To load the package, this code is
executed. It creates a namespace for the package, injects the package bindings,
and attaches the namespace to the program search path. The \c{NULL} comes from a
call to \c{substitute(subset)} in \c{stats::model.frame.default} function which
has a default value of \c{subset} as \c{NULL}. The \c{Symbol} arises from a
call to \c{as.name} in \c{base::str} function that returns a symbol that is
looked up by evaluating it in a specific environment. The remaining cases arise
from calls to \c{match.arg} which is used to look up the default choices for a
variable and match against the choice passed by the caller.

The most frequent ten expressions to eval calls from CRAN packages contribute to
77.1\% of all \eval calls.
\begin{table*}[tb]
  \centering
  \begin{tabular}{l|rr} \hline
    Expression & Eval Call & Eval Call \% \\\hline
    \c{Environment} &                                                              989302   & 61.8\%\\
    \c{column[rows] <<- what} &                                                    55677    & 3.5\%\\
    \c{function(value) freduce(value, `_function_list`)} &                         37251    & 2.3\%\\
    \c{NULL} &                         32005    & 2\%\\
    \c{List} &                         22293    & 1.4\%\\
    \c{c("default", "default2012", "default2011" ...}&                             20610    & 1.3\%\\
    \c{force(..1)}                                        &                        20461    & 1.3\%\\
    \c{alist(`_spec`)}                                   &                         18532    & 1.2\%\\
    \c{inner}                                           &                          18530    & 1.2\%\\
    \c{String Vector}                           &                           17487     & 1.1\%\\
  \end{tabular}
  \caption{Top ten eval calls in CRAN}
  \label{A}
\end{table*}

The expression \c{Environment} occurs because of the four callsites explained
above, \c{ggplot2::ggproto}, \c{R6::generator_funs}, \c{future::backtrace} and
\c{RModel::str.RMmodel}. The next expression, \c{column[rows] <<- what}, is used
inside the \c{plyr::rbind.fill} function to merge data frames by assigning
concatenated vectors to rows. The \c{<<-} operator is interesting in that it
skips the current scope and assigns in a parent scope in which the variable is
already present. In our corpus, all these \eval calls contribute to a single
side-effect. The expression \c{function(value) freduce(value, `_function_list`)}
arises from the \c{magrittr::\%>\%} function which is a pipe operator that pipes
the output of previous command to the next one. The expression is evaluated in a
custom environment to create a function binding for evaluating the components of
the pipe.\c{String Vector} and \c{List} also arise from the same function when a
string or a list is piped using the \c{\%>\%} function into the next expression.
The \c{NULL} arises from \c{R6::generator_funs} function when the \eval is
passed a \c{NULL} argument by the \c{DataMask_generator} package. The
\c{c("default", "default2012", "default2011" ...} pattern arises from
\c{copula::polyG} where it reflectively access the default expression for its
formal parameter and evaluates it. The \c{force(..1)} and \c{alist(`_spec`)}
patterns occur in \c{glue::glue_data} function which concatenates and
interpolates strings. The two patterns occur because the function captures
unevaluated unnamed arguments and maps the evaluation of \c{force(..1)} on them.
The \c{force} function forces promises~\cite{oopsla19a} and returns the result
of evaluation. The \c{inner} pattern arises from \c{glue::identity_transformer}
which enables the creation of custom transformation functions for affecting the
interpolation and concatenation of input by the \c{glue} package.

\subsection{Operation Mix}
In this section, we look at the operations executed by the expressions and
symbols on evaluation. R is a lazy language, so symbols can be bound to promises
that can evaluate arbitrary code. All operations in R are function calls, even
primitive operations such as addition and assignment are treated as calls to
functions implemented in the interpreter. In our corpus, we observe 2.9G calls
to R functions of which 125.5M happen inside \eval. Furthermore, unlike
Javascript, R code also calls into peformant native implementations of numerical
and statistical routines. In the corpus there are 463.8M calls to native code of
which only 31.2M happens inside \eval.
Qualitatively, we observe all kinds of operations being executed inside \eval
calls, from numerical computing for statistical modeling to reflection and
metaprogramming for implementing DSLs. Even testing frameworks such \c{genthat}
heavily rely upon \eval to execute test code snippets. The core R implementation
is rife with uses of \eval for mundane things like accessing default values of
formal parameters and in statistical packages for computation.

\subsubsection{Provenance}

In this section we turn our attention to the \AllExpressionInputEvalCallPerc
expressions passed as input to \eval. There are two ways to obtain
expressions in R.
\begin{itemize}
  \item \c{substitute} function can be used to synthesize ASTs from
  expressions by replacing symbols with their bindings in the specified
  environment.
  \item \c{expression} function can be used to create a vector of expression
    objects from text.
  \item \c{parse} function and its variants \c{str2expression} and \c{str2lang}
    can be used to turn strings or text from file into R expressions.
\end{itemize}

In our corpus, we observe 5.2\% cases where \eval directly evaluates output of
\c{substitute}, 0.7\% cases where output of \c{parse} and its variants is read
directly and only 704 cases for \c{expression}. Majority of expression objects
consumed by \eval are generated by their transitive uses by other functions.

The pattern \c{eval(parse(...))} can be used for dynamic code loading. This
forms the core of \c{source} and \c{sys.source} functions in R that are commonly
used for loading code in R files in interactive settings. We investigated the
number of cases in which the output of \c{parse} and its variants is passed to
\eval, directly or transitively by tainting their output. This corresponds to \PercentParsedCallSites of the total \eval call sites and
\PercentParsedEvals of the eval calls. We observed that
very few of the eval call(\NbParseFilesRnd in total) consume the result of calling \c{parse} on
a file. Most of the eval calls consume the result of calling
\c{parse} on a string.  We also identified one
function in core R, \c{invokeRestartInteractively} that prompts the user for
input, parses it, and passes it to \eval.

%%Unlike Javascript, R packages also call native code for access to efficient
%%numerical and statistical routines. We intercept all calls to R functions and
%%native functions from the programs and \eval calls. In the whole corpus there
%%are \AllClosureCallCount calls to R functions and \AllNativeCallCount to native
%%code. Of those, \eval contribute to only \EvalClosureCallCount calls to R
%%functions and \EvalNativeCallCount to native code.
%%
%%\begin{table}[ht]\resizebox{\columnwidth}{!}{%
%%    \begin{tabular}{lll}\hline
%%      Source & Primitive Functions & R Functions & Native Functions \\ \hline
%%      Core & \CorePrimitiveFunctionCount & \CoreRFunctionCount & \CoreNativeFunctionCount \\
%%      CRAN & \PackagePrimitiveFunctionCount & \PackageRFunctionCount & \PackageNativeFunctionCount \\
%%      All  & \AllPrimitiveFunctionCount & \AllRFunctionCount & \AllNativeFunctionCount \\
%%    \end{tabular}} \label{table:operation-distribution}
%%  \caption{Distribution of operation calls to \eval calls across Core and Package}
%%\end{table}

\subsection{Side-Effects}

In the corpus we observe \AllWritesRnd writes to variables of which
\EvalWritesRnd writes happen inside \eval. However, all writes are not
dangerous. Only writes to environments not local to the computation spawned
by \eval are side-effecting. These writes outlive the computation and hence are
visible outside the computation. The remaining writes are local to the
computation. We observe that \EvalSideEffectingWritesRnd writes inside evals are
side-effecting. This is only \EvalSideEffectingWritesEvalPerc of all variable
writes inside \eval and \EvalSideEffectingWritesAllPerc of all variable writes
in the corpus.

An \eval is considered side-effecting if it performs a side-effecting write to a
variable, directly or indirectly. Only \SideEffectingCoreCallPerc \eval calls in
Core R are side-effecting and \SideEffectingPackageCallPerc \eval calls in CRAN
packages are side-effecting.

\subsection{Scope}

One of the main differences of the R \eval with the javascript one is how it
interacts with \emph{scope}, or environments in R. In Javascript, \c{eval} can
access the local scope and the global scope. In R, \c{eval} has an argument
\c{envir} to indicate in which environment its first argument must be evaluated.
By default, it is equal to \c{parent.frame()}, 
which is the parent environment of \c{eval} call. This default argument
happens in \DefaultEnvirExprPercent of the eval calls.

This argument can also be \c{NULL}, a \c{List} or a data frame. This happens in
3.5\% of the cases.
\c{eval} copies the fields of the list or data frame and creates bindings for
them in a new environment. This pattern is used to evaluate formulas which can
directly refer to the fields of the data.
The \c{envir} argument can also be a number $n$. It means that the environment in
which the expressions is evaluated will be the result of \c{sys.call(n)} where
$n$ refers to the $n$-th stack frame.

The top-level environment in R is called the global environment. New
environments can be created using \c{new.env}. They can be provided a parent
environment which becomes the enclosing scope of the new environment.

We looked at the environments passed to all the \eval calls in our corpus.
Table~\ref{tab:environments} summarizes the results. A numeric environment class
\c{n} denotes the environment of the $n$-th call stack frame from the current
function. \c{global} denotes the top-level environment and \c{list} denotes a list
passed for evaluation of formulas. Environment classes of the form $n+$ denote
the $n$-th environment extended with a new environment. The new environment
provides a limited form of sandboxing. All assignments using the \c{\<-}
function occur inside it and prevent the extended environment from
mutation. However, it is still possible to mutate the extended environment
using the \c{\<\<-} or \c{assign} functions; but, that happens rarely.

\begin{table}[htbp]
    \centering
    \begin{tabular}{c|c|c|c}
        \hline
        \multicolumn{2}{c|}{Core} & \multicolumn{2}{|c}{Packages}  \\
        \hline
        Environment Class & Eval Calls \% & Environment Class & Eval Calls \% \\
        \hline
        \CoreEnvClassA & \CoreProportionA & \PackagesEnvClassA &  \PackagesProportionA\\
        \CoreEnvClassB & \CoreProportionB & \PackagesEnvClassB &  \PackagesProportionB\\
        \CoreEnvClassC & \CoreProportionC & \PackagesEnvClassC &  \PackagesProportionC\\
        \CoreEnvClassD & \CoreProportionD & \PackagesEnvClassD &  \PackagesProportionD\\
        \CoreEnvClassE & \CoreProportionE & \PackagesEnvClassE &  \PackagesProportionE\\
        \hline
    \end{tabular} \label{tab:environments}
\caption{Environments in terms of eval calls.}
\end{table}

We observe that a disproportionately high core R \eval calls access the
caller's caller's environment. This is because many core R functions call
functions that pass the result of \c{parent.frame()} to an \eval.
A disproportionately high calls to \eval happen in an extended
top-level environment. This can be explained by the fact that many packages
evaluate code passed from the user's workspace in the top-level environment to
access the bindings.

%%Below, we provide some code snippets to show how some of these environment
%%classes manifest.

%%The \c{match.arg} (see Listing~\ref{code:matchargenvir}) function has an
%%environment class 2. \c{sys.parent()} refers to the parent environment of
%%\c{match.arg} and so to the grand-parent environment of \c{eval} which we denote
%%as $2$.
%%
%%\begin{lstlisting}[label=code:matchargenvir, caption={\c{match.arg} using \eval with a custom environment.}]
%%formal.args <- formals(sys.function(sysP <- sys.parent()))
%%choices <- eval(formal.args[[as.character(substitute(arg))]], envir = sys.frame(sysP))
%%\end{lstlisting}

%%In package \emph{gamlss::get.K} function \c{get.K} evaluates its expression in its
%%parent environment, the one of the body of \c{get.K} returned from
%%\c{environment()}.
%%
%%\begin{lstlisting}
%%eval(parse(text = paste(\"fam$dld\", ii, sep = \"\")), envir = environment())
%%\end{lstlisting}
%%
%%
%%The \emph{glue} library is used to interpolate string litterals with variables, such as in \c{glue('My name is \{name\}.')} where \c{name} will be subtituted by its value. \emph{glue} makes it possible to define transformers that process those interpolated variables. They are built upon the \c{identity_transformer}, which uses \eval. The interpretation environment for \eval is often global here, as the string litterals are often used to print variabels at the top level.
%%
%%\begin{lstlisting}
%%identity_transformer <- function(text, envir) 
%%{
%%    eval(parse(text = text, keep.source = FALSE), envir)
%%}
%%\end{lstlisting}
%%
%%
%%The \c{imchange} function of package \c{imager} makes it possible to modify images using a dedicated formula syntax using \c{\~}.\footnote{Inspired by \c{map} in package \emph{purr}.}  Here, \eval is evaluated in \c{newenv}, which creates a new environment that inherits from \c{parent.frame()} by default. We classify it as 1+.
%%
%%
%%\begin{lstlisting}
%%newenv <- new.env()
%%...
%%fo <- parse(text=as.character(fo)[2])
%%im[where] <- eval(fo,envir=newenv,enclos=env)
%%\end{lstlisting}
%%
%%\c{adjCoef} in package \emph{actuar} find the root of an equation defined by a function \c{h} whose arguments must be named \c{x} and \c{y}.
%%\c{h} is transformed into an auxiliary function \c{h2} that can be optimized. Here, the list used for \c{envir} ensures the correspondance between the textual arguments of \c{h} and the arguments of \c{h2}. 
%%
%%
%%\begin{lstlisting}
%%sh <- substitute(h)
%%fcall <- paste(sh, "(x, y)")
%%...
%%h2 <- function(x, y)
%%    eval(parse(text = fcall),
%%    envir = list(x = x, y = y),
%%    enclos = parent.frame(2))
%%\end{lstlisting}

\subsection{Patterns}

This section presents the common patterns of \eval usage in our corpus. We
randomly selected 20 packages from the corpus and studied how they use eval.
We have grouped those uses in the following categories.

  \subsubsection{Evaluation using Data Fields} A common use of \eval is to
  evaluate expressions that access data frame columns or list (heterogeneous
  vectors) elements by name. Core R provides two functions, \c{with} and
  \c{within} that implement this pattern. These functions construct a temporary
  environment with data frame columns or list elements as the bindings.
  Side-effects performed by the evaluated expression take place in this temporary
  environment and are ignored by the \c{with} function. The \c{within} function
  copies these modifications to a local copy of the input data and returns it.
  This pattern is used quite often for statistical modeling. The modeling
  expressions is constructed from variable names whose values are supplied by a
  data frame or list.

  \subsubsection{Sandboxing} The combination of \eval and first class
  environments can be used to construct a sandbox for evaluating side-effecting
  expressions. The newly constructed environment can use the current environment
  for its lexical scope. This provides the evaluated expression access to all the
  bindings in the current scope but restricts the side-effect to the newly
  constructed throwaway environment. However, this sandboxing is limited because
  the expression can reflectively access any environment in the call stack or use
  the super assignment operator(\c{<<-}) to write to parent scope. The \c{local}
  function which is part of Core R is an example of this usage pattern.

  \subsubsection{Dynamic Code Loading} The combination of \parse and \eval is
  used to parse text from string or file and evaluate in a custom environment.
  This pattern is used by the \source Core R function to load R code from a file
  in the current workspace. This is quite useful for loading functions for
  interactive data analysis.

  \subsubsection{Metaprogramming} The combination of \eval and \substitute is
  used to do metaprogramming in R. \substitute can be used to synthesize ASTs by
  stitching together user inputs. \substitute walks over its argument AST and
  replaces the symbols with their bindings in the specified environment and
  returns this new AST which can then be evaluated in custom environments using
  \eval.

  \subsubsection{Convenience}
  evaling recorded calls in model-fitting functions or passing most of the
  current call to another function. Both ``patterns'' use the \c{match.call} to
  capture current call. The latter is just for laziness as it is the same as
  calling the new function with explicitly selecting the arguments.

  \subsubsection{Implementation}
  The package installation process adds an extra file to the package directory
  that contains the code to load the package code inside a \local block. It
  creates a namespace object and injects the package bindings.

  \subsubsection{Miscellaneous} \eval is also used in R packages to get around
  various limitations. For example, in \datatable package, \eval is used to modify
  a binding in the \base package of R. Bindings in the \base package are locked
  for modification and require explicit unlocking using \unlockBinding function.
  However, directly using this function triggers a warning during package building
  process which statically analyzes the package for use of such restricted
  functions. The \datatable gets around this limitation by enclosing the call to
  \unlockBinding inside \eval.

\subsection{Consistency}



We look at how \emph{consistent} the \c{expr} argument of \eval can be, \ie
how many different types of the resolved \c{expr} there are per call sites. Most of the call sites, \ie \PercentMonomorphic,  are
\emph{consistent}, and this is similar to javascript. However, a few ones are highly \emph{polymorphic} (10 different types). They are the  pipe  operators \c{\%>\%}, \c{\%<>\%} and \c{\%$\%} in package \emph{magrittr}. It is effectively used to compose functions on their first argument, which can be of any type.

%\begin{figure}[!h]
%	\centering
%    \includegraphics[width=\columnwidth]{polymorphism}
%    \caption{Number of different types  of \c{expr} per call sites.} \label{fig:polymorphism}
%\end{figure}
%
%\subsection{base}
%
%%There are 30 functions in \emph{base} that use \eval. Only five of them use \c{parse}, for legitimate reasons, such \c{source} and \c{sys.source} which load a file and then evaluate it, or \c{invokeRestartInteractively} which prompts the user for arguments to restart execution after an error.
%%
%%We endeavored  to write these functions without \eval. \c{match.arg} appears in \MatchArgPercent of the \eval calls. It is used to get the arguments, possibly abbreviated, of a function. With one argument, as \c{match.arg(arg)}, it uses \c{eval} to infer them from a parent function's formal arguments. We can rewrite it using its version with two arguments, \c{match.arg(arg, choices)} where \c{choices} are the possible formal arguments of the function, as shown in Listing~\ref{code:matcharg}.
%%
%%\begin{lstlisting}[caption={Rewriting \c{match.arg} without \eval}, label=code:matcharg]
%%f <- function(type=c("examples", "tests", "vignettes")) {
%%actual_type = match.arg(type)
%%...
%%}
%%
%%f <- function(type=c("examples", "tests", "vignettes")) {
%%actual_type = match.arg(type, choices=c("examples", "tests", "vignettes"))
%%...
%%}
%%\end{lstlisting}

\section{Threats to Validity} The primary threat to validity is the issue of
code coverage, a cause of concern for any dynamic approach. We mitigate this by
including only those R packages in our corpus that have many reverse
dependencies and thus high code coverage. We turn off the bytecode compiler for
this study. The bytecode compiler can also call eval. We do not get source
locations for \UndefinedEvalsRnd eval calls. In these cases \eval is either
passed as an argument to a higher-order functions or is defined in a function
returned by a higher-order function and the R parser does not retain location
information for \eval. However, this is a meager \PercentUndefinedEval of all
eval calls and is unlikely to affect our analysis. We ignore calls to the native
\eval function exposed by R. We also ignore the \c{rlang::tidy_eval} function
which uses native \eval internally because \c{rlang} is used to implement a DSL
for data analysis in R. It introduces a new first-class promise object called
\c{quosure} for which it implements special evaluation support in \c{tidy_eval}.

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{bib/bibliography,bib/jv}

\end{document}
