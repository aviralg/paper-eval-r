---
title: "Usage metrics"
author: "Pierre Donat-Bouillud"
output:
    html_document:
        gallery: false
        toc: true
        toc_depth: 3
        toc_float: true
        df_print: paged
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
params:
  base_dir: /var/lib/R/project-evalR
  calls_packages_path: /run/package-evals-traced.9/summarized-packages.fst
  corpus_path: /run/package-evals-traced.9/corpus.fst
---

The following is a set up chunk. Currently there is an variable "jan" 
used to run the script from docker on my machine. This should be
removed. (TODO)

```{r setup, include=FALSE}
library(tidyverse)
library(fst)
library(fs)
library(DT)

jan <- TRUE

knitr::knit_hooks$set(time_it = local({
  now <- NULL
  function(before, options) {
    if (before) { # record the current time before each chunk
      now <<- Sys.time()
    } else { # calculate the time difference after a chunk
      res <- difftime(Sys.time(), now)
      # return a character string to show the time
      paste("Time for this code chunk to run:", res, units(res))
}}}))
now <- Sys.time()
knitr::opts_chunk$set(
  echo = TRUE,
  fig.retina = 2,
  fig.width = 10,
  cache.lazy = FALSE,
  time_it = TRUE
)

if (jan == FALSE) {
  source("inc/paths.R")
}
source("inc/latextags.R")
source("inc/setup.R")
theme_set(theme_minimal())
if (jan) {
  TAGS_DIR <- PLOT_DIR <- "."
}
create_tags(path(TAGS_DIR, "usage_metrics.tex"), prefix = "", default = TRUE)

base_dir <- params$base_dir
# Set the paths, and customize for Jan.
if (jan) {
  base_dir <- "/mnt/code/eval"
}
eval_calls_path <- paste0(base_dir, params$calls_packages_path)
corpus_path <- paste0(base_dir, params$corpus_path)
```

Read in the summarized calls data and the file that describes
which package belong to our corpus.

```{r read}
read_fst(eval_calls_path) %>% as_tibble() -> E_raw
read_fst(corpus_path) %>% as_tibble() -> C_raw
```

The two data sets are E and C for conciseness.  C is only used here to compute
the corpus size. E is used throughout. Recall that E_raw has the source data
with all columns.

```{r}
E_raw %>% select(
  run_package = package,        # The package which was run to trigger the eval
  ev_variant = eval_function,   # Which eval variant was called
  duplicates = nb_ev_calls,     # Number of identical eval calls (weight)
  src_ref = eval_call_srcref,   # Source ref for the eval call site
  file,                         # Script file name (name of this run)
  ev_package =eval_source,      # Package of the call site
  arg_size = expr_expression_length, # Size in bytes of eval argument string
  opcodes = direct_interpreter_eval  # Non-recursive AST nodes traversed
) -> E                               # by the interpreter

C_raw -> C
corpus_size <- nrow(C)
```

Compute the number of times eval and it variants were called in the entire 
corpus. This is a dynamic count that aggregates all call sites within
the corpus.

```{r eval-calls-overview}
nb_calls <- (E %>% count(wt = duplicates))$n
eval_calls <- E %>%
  filter(ev_variant == "eval") %>%
  count(wt = duplicates)
evalq_calls <- E %>%
  filter(ev_variant == "evalq") %>%
  count(wt = duplicates)
eval.parent_calls <- E %>%
  filter(ev_variant == "eval.parent") %>%
  count(wt = duplicates)
local_calls <- E %>%
  filter(ev_variant == "local") %>%
  count(wt = duplicates)

r("nb eval and variant calls", nb_calls)
r("nb eval calls", eval_calls$n)
r("nb evalq calls", evalq_calls$n)
r("nb eval parent calls", eval.parent_calls$n)
r("nb local calls", local_calls$n)
``` 

There so mane calls: `r nb_calls` 


# Number of eval call sites

```{r nb-eval-call-sites}
nb_ev_call_sites <- E %>%
  select(src_ref) %>%
  n_distinct(na.rm = TRUE)

call_sites_per_package <- E %>%
  group_by(ev_package) %>%
  summarize(nb_call_sites = n_distinct(src_ref, na.rm = TRUE))

nb_packages_one_call_site <- call_sites_per_package %>%
  filter(nb_call_sites == 1) %>%
  nrow()
packages_at_least_one_call_site <- call_sites_per_package %>%
  filter(nb_call_sites >= 1)
median_call_sites <- packages_at_least_one_call_site %>%
  summarise(med = median(nb_call_sites, na.rm = TRUE))
max_packages <- packages_at_least_one_call_site %>%
  filter(nb_call_sites == max(nb_call_sites, na.rm = TRUE)) %>%
  select(ev_package, nb_call_sites)
nb_no_calls_to_eval <- corpus_size -
  n_distinct(select(packages_at_least_one_call_site, ev_package))

# sites triggered over the whole corpus
r("nb eval call sites", nb_ev_call_sites) 
# How many packages in the corpus have a single triggered call site
r("nb one call site", nb_packages_one_call_site)
r("at least one call site", nrow(packages_at_least_one_call_site))
r("packages no calls to eval", nb_no_calls_to_eval)
r("nb median call sites", median_call_sites$med)
r("max call sites", max_packages$nb_call_sites)
r("max call sites package", max_packages$ev_package)
```


```{r call-sites-violin-plot}
E %>%
  group_by(ev_package) %>%
  mutate(nb_call_sites = n_distinct(src_ref, na.rm = TRUE)) %>%
  ggplot(aes(x = ev_variant, y = nb_call_sites, fill = ev_variant), na.rm = TRUE) +
  geom_violin(trim = FALSE, width = 1.7) +
  # geom_jitter(shape=16, position=position_jitter(0.2)) +
  labs(y = "Call sites per package") +
  theme(
    legend.position = c(0.5, 0.85),
    legend.box.background = element_rect(fill = "white", size = 0.1),
    legend.box = "horizontal",
    axis.title.x = element_blank()
  )

ggsave(path(PLOT_DIR, "call_sites_per_packages.pdf"))
```


# Numbers of calls to eval

This is the number of eval calls which have a call site in each package, divided by the number of runs for the package.


We first compute an approximation of the number of runs per package.
```{r runs-per-package}
runs_package <- E %>%
  group_by(ev_package) %>%
  summarise(nb_runs = n_distinct(file)) %>%
  arrange(desc(nb_runs))
```


What are the packages with the most runs?

```{r show_run-per-package}
runs_package %>% datatable()
```


```{r nb-eval-calls}
# run = package?
eval_calls_per_package <- E %>%
  count(ev_package, wt = duplicates) %>%
  left_join(runs_package, by = "ev_package") %>%
  mutate(n = n / nb_runs)

average_eval_calls <- eval_calls_per_package %>%
  filter(n > 0) %>%
  summarise(average = mean(n))
max_eval_calls <- eval_calls_per_package %>% filter(n == max(n))

r("average eval calls per package", average_eval_calls$average) # average on packages with at least one eval call
r("max eval calls", max_eval_calls$n)
r("max eval call package", max_eval_calls$ev_package)
```


```{r eval-calls-violin-plot}
eval_calls_per_package %>%
  ggplot(aes(x = "", y = n, fill = ""), na.rm = TRUE) +
  geom_violin(trim = FALSE, width = 1.7) +
  labs(y = "Eval calls per package") +
  theme(legend.position = "none")

ggsave(path(PLOT_DIR, "eval_calls_per_packages.pdf"))
```


<!-- How is the normalized distribution of eval calls?  -->

<!-- ```{r distribution-call-sites} -->
<!-- eval_calls_per_package %>% -->
<!--   ggplot(aes(x = n), na.rm = TRUE) + -->
<!--   geom_density(adjust = 10) + -->
<!--   scale_y_log10()  -->
<!-- ``` -->

<!-- (Like the violin plot, but smoothed) -->
(th)

# Amount of code loaded by eval

```{r amout-of-code}

weighted.median <- function(x, w, na.rm = FALSE) {
  if (na.rm) {
    na.omit(x)
  }
  w <- w[order(x)]
  x <- x[order(x)]

  prob <- cumsum(w) / sum(w)
  ps <- which(abs(prob - .5) == min(abs(prob - .5)))
  return(x[ps])
}

med_size <- weighted.median(E$arg_size, w = E$duplicates, na.rm = TRUE)
max_size <- max(E$arg_size, na.rm = TRUE)
average_size <- weighted.mean(E$arg_size, na.rm = TRUE, w = E$duplicates)

sizes_prop <- E %>%
  count(arg_size, wt = duplicates) %>%
  mutate(proportion = n / sum(n)) %>%
  arrange(arg_size) %>%
  mutate(cumulative = cumsum(proportion))


nb_nodes_many <- sizes_prop %>%
  filter(cumulative >= 0.95) %>%
  .[1, ]

r("median size eval", med_size)
r("max size eval", max_size)
r("average size eval", average_size)
r("size nine five", nb_nodes_many$arg_size)
r("size nine five exact percent", percent(nb_nodes_many$cumulative))
```



# Amount of computations via eval


```{r amount_computations}
nb_events_in_eval <- E %>% summarise(nb_events = sum(opcodes))

max_events_in_eval <- E %>% summarize(maxi = max(opcodes))

less_100_events <- E %>% filter(opcodes <= 100)
nb_less_100_events <- count(less_100_events, wt = duplicates)$n

r("nb events eval", nb_events_in_eval$nb_events)
r("maxi events eval", max_events_in_eval$maxi)
r("small events eval", ratio(nb_less_100_events, nb_calls))
```

```{r nb-events-violin-plot}
E %>%
  filter(opcodes > 0) %>%
  ggplot(aes(x = "", y = opcodes, fill = ""), na.rm = TRUE) +
  geom_violin(trim = FALSE, width = 1.7) +
  scale_y_log10(labels = scales::comma) +
  labs(y = "Number of events in eval per package") +
  theme(legend.position = "none")

ggsave(path(PLOT_DIR, "nb_events_per_packages.pdf"))
```

# Loops

Here, we try to have an estimation of how often a call site is exercised per run. If it is often, maybe it is a loop!

```{r loops}
eval_calls_exercised <- E %>%
  group_by(file) %>%
  count(src_ref, wt = duplicates)

eval_calls_exercised %>% ggplot(aes(n)) +
  geom_histogram() +
  scale_x_log10(labels = scales::comma) +
  labs(x = "Number of eval calls per srcref per run")
```


# The case of core libraries


```{r}
duration <- difftime(Sys.time(), now)
```

Notebook execution was `r duration` `r units(duration)`.
