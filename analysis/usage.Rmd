---
title: "Usage metrics"
author: "Pierre Donat-Bouillud"
output:
    html_document:
        gallery: false
        toc: true
        toc_depth: 3
        toc_float: true
        df_print: paged
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
params:
  base_dir: Data/
  calls_packages_path: summarized-packages.fst
  corpus_path: corpus.fst
  static_evals_path: package-evals-static.csv
---


```{r setup, include=FALSE}
library(tidyverse)
library(fst)
library(fs)
library(DT)

knitr::knit_hooks$set(time_it = local({
  now <- NULL
  function(before, options) {
    if (before) { # record the current time before each chunk
      now <<- Sys.time()
    } else { # calculate the time difference after a chunk
      res <- difftime(Sys.time(), now)
      # return a character string to show the time
      paste("Time for this code chunk to run:", round(res, 2), units(res))
    }
  }
}))
now <- Sys.time()
knitr::opts_chunk$set(
  echo = TRUE,
  fig.retina = 2,
  fig.width = 10,
  cache.lazy = FALSE,
  time_it = TRUE
)

source("inc/paths.R")
source("inc/latextags.R")
source("inc/setup.R")
theme_set(theme_minimal())
create_tags(path(TAGS_DIR, "usage_metrics.tex"), prefix = "", default = TRUE)

base_dir <- params$base_dir
eval_calls_path <- paste0(base_dir, params$calls_packages_path)
corpus_path <- paste0(base_dir, params$corpus_path)
eval_static_path <- paste0(base_dir, params$static_evals_path)
```

# Read

Read in the summarized calls data (`E_raw`) and the file that describes
which package belong to our corpus (`C_raw`). For conciseness, `E` is
the input data set with only the columns that we use.

```{r read, message=TRUE, include=F}
# This test should allow to re-run the whole script without reading the
# data again from disk. It assumes the data doesn't change.
if (exists(eval_calls_path) == FALSE) {
  read_fst(eval_calls_path) %>% as_tibble() -> E_raw
  read_fst(corpus_path) %>% as_tibble() -> C_raw
  read_csv(eval_static_path) %>% semi_join(C_raw, by = "package") -> P
}
```

The two data sets are `E` and `C` for conciseness.  `C` is only used here to 
compute the corpus size. `E` is used throughout. Recall that `E_raw` has the 
source data with all columns. `P` corresponds to static information about the
call sites.

```{r select-columns, message=TRUE, warning=FALSE, include=FALSE}
E_raw %>% select(
  run_package = package, #      The package which was run to trigger the eval
  ev_variant = eval_function, # Which eval variant was called
  duplicates = nb_ev_calls, #   Number of identical eval calls (weight)
  src_ref = eval_call_srcref, # Source ref for the eval call site
  file, #                       Script file name (name of this run)
  ev_package = eval_source, #   Package of the call site
  arg_size = expr_expression_length, # Size in bytes of eval argument string
  opcodes = direct_interpreter_eval #  Count of the AST nodes traversed by the
) -> E #                               interpreter during eval (non-recursive).

C_raw -> C
corpus_size <- nrow(C)

nb_runs <- E %>%
  pull(file) %>%
  n_distinct()
overview_table(
  r("Corpus", corpus_size),
  r("NbRuns", nb_runs)
) 
```

The size of our corpus is `r corpus_size` packages.

#  Variants

The data set has `r nrow(E)` distinct calls to the `eval` and its variants. 
There are `r length(unique(E$ev_package))` corpus packages with `eval` calls
(note that we may not trigger all `eval`s in a package due to spotty coverage).

```{r variants,echo=F}
nb_calls <- E %>%
  count(wt = duplicates) %>%
  .$n
eval_calls <- E %>%
  filter(ev_variant == "eval") %>%
  count(wt = duplicates) %>%
  .$n
evalq_calls <- E %>%
  filter(ev_variant == "evalq") %>%
  count(wt = duplicates) %>%
  .$n
eval.parent_calls <- E %>%
  filter(ev_variant == "eval.parent") %>%
  count(wt = duplicates) %>%
  .$n
local_calls <- E %>%
  filter(ev_variant == "local") %>%
  count(wt = duplicates) %>%
  .$n

overview_table(
  r("Allcalls", nb_calls),
  r("Evals", eval_calls),
  r("Evalqs", evalq_calls),
  r("Eparents", eval.parent_calls),
  r("Locals", local_calls),
  r("Triggeredpkgs", length(unique(E$ev_package)))
) -> TBL
``` 

We recorded `r nb_calls` to eval, out of these, there were `r evalq_calls`
(`r fmt(ratio(evalq_calls,nb_calls))`) calls to `evalq`, `r eval.parent_calls` 
(`r fmt(ratio(evalq_calls,nb_calls))`) calls to `eval.parent`,  and
`r local_calls` (`r fmt(ratio(evalq_calls,nb_calls))`) calls to `local`. 

`r TBL`

```{r exercised-call-site-per-variant}
static_eval <- P %>%
  filter(call_fun_name == "eval") %>%
  nrow()
static_evalq <- P %>%
  filter(call_fun_name == "evalq") %>%
  nrow()
static_eval.parent <- P %>%
  filter(call_fun_name == "eval.parent") %>%
  nrow()
static_local <- P %>%
  filter(call_fun_name == "local") %>%
  nrow()

sites_per_variant <- E %>% # src_ref indicates where the call originates from.
  group_by(ev_variant) %>%
  summarize(n = n_distinct(src_ref)) # This the number of distinct sites

trig_eval <- sites_per_variant %>% filter(ev_variant == "eval")
trig_eval <- if (nrow(trig_eval) == 0) {
  0
} else {
  trig_eval$n
}

trig_evalq <- sites_per_variant %>% filter(ev_variant == "evalq")
trig_evalq <- if (nrow(trig_evalq) == 0) {
  0
} else {
  trig_evalq$n
}
trig_eval.parent <- sites_per_variant %>% filter(ev_variant == "eval.parent")
trig_eval.parent <- if (nrow(trig_eval.parent) == 0) {
  0
} else {
  trig_eval.parent$n
}
trig_local <- sites_per_variant %>% filter(ev_variant == "local")
trig_local <- if (nrow(trig_local) == 0) {
  0
} else {
  trig_local$n
}

overview_table(
  r("StaticEval", static_eval),
  r("StaticEvalq", static_evalq),
  r("StaticEvalParent", static_eval.parent),
  r("StaticLocal", static_local),
  r("TriggeredEval", trig_eval),
  r("TriggeredEvalq", trig_evalq),
  r("Triggeredevalparent", trig_eval.parent),
  r("Triggeredlocal", trig_local),
  r("Triggeredevalpercent", ratio(trig_eval, static_eval)),
  r("Triggeredevalqpercent", ratio(trig_evalq, static_evalq)),
  r("Triggeredevalparentpercent", ratio(trig_eval.parent, static_eval.parent)),
  r("Triggeredlocalpercent", ratio(trig_local, static_local))
) -> TBL
```


`r TBL`

# Ev-dist 

We compute the frequency of evals by package. This shows that some packages
hardly use eval while other use it intensly. The numbers are "for all runs",
this means that a popular package may be triggered by more runs and thus
have higher frequencies. We will later control for this by looking at individual 
call sites.

```{r ev-dist, include=FALSE}
# The calls per package
pack_calls <- E %>% 
  count(ev_package, wt=duplicates, sort = TRUE)
# Create bins at every power of ten
bins <- cut(pack_calls$n, breaks = c(0, 10^seq(1,8)), include.lowest = TRUE)
sum <- summary(bins)
# Packages with more than 1K calls
Kcalls <- pack_calls %>% filter(n > 1000) %>% nrow()

# The number of runs per package
runs <- E %>%
  group_by(ev_package) %>%
  summarise(nb_runs = n_distinct(file)) 

# Number of eval calls per package
ecpp <- pack_calls %>% left_join(runs, by = "ev_package") 
stopifnot( nrow(filter(ecpp, n==0)) == 0)
# The package with the most calls
max_calls <- ecpp %>% filter(n == max(n))

overview_table(
r("bina", sum[[1]]),
r("binb", sum[[2]]),
r("binc", sum[[3]]),
r("bind", sum[[4]]),
r("bine", sum[[5]]),
r("binf", sum[[6]]),
r("bing", sum[[7]]),
r("binh", sum[[8]]),
r("Fewcalls", sum[[1]] + sum[[2]]),
r("Manycalls", Kcalls),
r("Maxcalls", max_calls$n),
r("Maxcallspack", max_calls$ev_package)
) -> TBL
```

The distribution of eval calls, each bin represents a power of 10, the number 
in the bin is the count of packages.

`r TBL`

# Triggered sites

```{r triggered-sites, include=F}
stopifnot(sum(is.na(E$src_ref)) == 0) # No NAs in scr_ref!

sites <- E %>% # src_ref indicates where the call originates from.
  pull(src_ref) %>%
  n_distinct() # This the number of distinct sites

sites_per_package <- E %>%
  count(ev_package, src_ref) %>%
  count(ev_package) # All packages have at least one site!

max_packages <- sites_per_package %>%
  filter(n == max(n)) %>%
  .$n

TBL <- overview_table(
  # Sites triggered over the whole corpus:
  r("nb eval call sites", sites),
  # How many packages in the corpus have a single triggered call site:
  r("nb one call site", sites_per_package %>% filter(n == 1)),
  r("at least one call site", nrow(sites_per_package)),
  r("packages no calls to eval", corpus_size - nrow(sites_per_package)),
  r("nb median call sites", median(sites_per_package$n)),
  r("max call sites", max_packages),
)
```

`r TBL`

```{r call-sites-violin-plot, include=F}
E %>%
  group_by(ev_package) %>%
  mutate(sites = n_distinct(src_ref)) %>%
  ggplot(aes(x = ev_variant, y = sites, fill = ev_variant), na.rm = TRUE) +
  geom_violin(trim = FALSE, width = 1.7) +
  # geom_jitter(shape=16, position=position_jitter(0.2)) +
  labs(y = "Call sites per package") +
  theme(
    legend.position = c(0.5, 0.85),
    legend.box.background = element_rect(fill = "white", size = 0.1),
    legend.box = "horizontal",
    axis.title.x = element_blank()
  ) -> PLOT

ggsave(path(PLOT_DIR, "call_sites_per_packages.pdf"))
```



How many call sites and how many packages did we exercise out of the total number of call sites of the corpus?

```{r exercised-call-sites, include=F}

# TODO :: remove?
static_sites <- nrow(P)

static_call_sites_per_package <- P %>%
  count(package, sort = TRUE)

static_at_least_one_call_site <- static_call_sites_per_package %>%
  filter(n >= 1)

r("nb static call sites", static_sites)
r("call sites percent", ratio(sites, static_sites))
r("nb static at least one call site", nrow(static_at_least_one_call_site))
r("static at least one call site percent", ratio(nrow(sites_per_package %>% filter(n >= 1)), nrow(static_at_least_one_call_site)))
```




# Numbers of calls to eval

This is the number of eval calls which have a call site in each package, divided by the number of runs for the package.


We first compute an approximation of the number of runs per package.
```{r runs-per-package}
runs_package <- E %>%
  group_by(ev_package) %>%
  summarise(nb_runs = n_distinct(file)) %>%
  arrange(desc(nb_runs))
```


What are the packages with the most runs?

```{r show_run-per-package}
runs_package %>% datatable()
```


```{r nb-eval-calls}
# run = package?

### TODO REMOVE
eval_calls_per_package <- E %>%
  count(ev_package, wt = duplicates) %>%
  left_join(runs_package, by = "ev_package")


average_eval_calls <- eval_calls_per_package %>%
  filter(n > 0) %>%
  summarise(average = mean(n))
max_eval_calls <- eval_calls_per_package %>% filter(n == max(n))

r("average eval calls per package", average_eval_calls$average) # average on packages with at least one eval call
r("max eval calls", max_eval_calls$n)
r("max eval call package", max_eval_calls$ev_package)
```


```{r eval-calls-violin-plot}
eval_calls_per_package %>%
  ggplot(aes(x = "", y = n, fill = ""), na.rm = TRUE) +
  geom_violin(trim = FALSE, width = 1.7) +
  labs(y = "Eval calls per package") +
  theme(legend.position = "none")

ggsave(path(PLOT_DIR, "eval_calls_per_packages.pdf"))
```




# Amount of code loaded by eval

```{r amout-of-code, include=F}

weighted.median <- function(x, w, na.rm = FALSE) {
  if (na.rm) {
    na.omit(x)
  }
  w <- w[order(x)]
  x <- x[order(x)]

  prob <- cumsum(w) / sum(w)
  ps <- which(abs(prob - .5) == min(abs(prob - .5)))
  return(x[ps])
}

med_size <- weighted.median(E$arg_size, w = E$duplicates, na.rm = TRUE)
max_size <- max(E$arg_size, na.rm = TRUE)
average_size <- weighted.mean(E$arg_size, na.rm = TRUE, w = E$duplicates)

sizes_prop <- E %>%
  count(arg_size, wt = duplicates) %>%
  mutate(proportion = n / sum(n)) %>%
  arrange(arg_size) %>%
  mutate(cumulative = cumsum(proportion))


nb_nodes_many <- sizes_prop %>%
  filter(cumulative >= 0.95) %>%
  .[1, ]

r("median size eval", med_size)
r("max size eval", max_size)
r("average size eval", average_size)
r("size nine five", nb_nodes_many$arg_size)
r("size nine five exact percent", percent(nb_nodes_many$cumulative))
```



# Amount of computations via eval


```{r amount_computations, include=F}
nb_events_in_eval <- E %>% summarise(nb_events = sum(opcodes))

max_events_in_eval <- E %>% summarize(maxi = max(opcodes))

less_100_events <- E %>% filter(opcodes <= 100)
nb_less_100_events <- count(less_100_events, wt = duplicates)$n

r("nb events eval", nb_events_in_eval$nb_events)
r("maxi events eval", max_events_in_eval$maxi)
r("small events eval", ratio(nb_less_100_events, nb_calls))
```

```{r nb-events-violin-plot,include=F}
E %>%
  filter(opcodes > 0) %>%
  ggplot(aes(x = "", y = opcodes, fill = ""), na.rm = TRUE) +
  geom_violin(trim = FALSE, width = 1.7) +
  scale_y_log10(labels = scales::comma) +
  labs(y = "Number of events in eval per package") +
  theme(legend.position = "none")

ggsave(path(PLOT_DIR, "nb_events_per_packages.pdf"))
```

# Loops

Here, we try to have an estimation of how often a call site is exercised per run. If it is often, maybe it is a loop!

```{r loops}
eval_calls_exercised <- E %>%
  group_by(file) %>%
  count(src_ref, wt = duplicates)

eval_calls_exercised %>% ggplot(aes(n)) +
  geom_histogram() +
  scale_x_log10(labels = scales::comma) +
  labs(x = "Number of eval calls per srcref per run")
```


```{r}
eval_call_triggered <- E %>%
  group_by(src_ref) %>%
  summarize(n_runs = n_distinct(file), n = sum(duplicates), average = n / n_runs) %>%
  arrange(desc(average))
```

```{r}
eval_call_triggered %>% 
  filter(average < 50) %>% 
  ggplot(aes(x = average)) +
  geom_histogram(binwidth =  1) 

ggsave(path(PLOT_DIR, "calls_per_run_per_call_site.pdf"))


bins <- cut(eval_call_triggered$average, breaks = c(0, 50, 100, 250,500, 1000, 1500,  2000, 3000), include.lowest = TRUE)
sum <- summary(bins)

overview_table(
  r("Runbina", sum[[1]]),
  r("Runbinb", sum[[2]]),
  r("Runbinc", sum[[3]]),
  r("Runbind", sum[[4]]),
  r("Runbine", sum[[5]]),
  r("Runbinf", sum[[6]]),
  r("Runbing", sum[[7]]),
  r("Runbinh", sum[[8]])
)
```


# The case of core libraries


```{r}
duration <- difftime(Sys.time(), now)
```

Notebook execution was `r duration` `r units(duration)`.
